title,abstract,pages,DOI,link,year
Challenges posed by hijacked journals in Scopus,"This study presents and explains the phenomenon of indexjacking, which involves the systematic infiltration of hijacked journals into international indexing databases, with Scopus being one of the most infiltrated among these databases. Through an analysis of known lists of hijacked journals, the study identified at least 67 hijacked journals that have penetrated Scopus since 2013. Of these, 33 journals indexed unauthorized content in Scopus and 23 compromised the homepage link in the journal's profile, while 11 did both. As of September 2023, 41 hijacked journals are still compromising the data of legitimate journals in Scopus. The presence of hijacked journals in Scopus is a challenge for scientific integrity due to the legitimization of unreliable papers that have not undergone peer review and compromises the quality of the Scopus database. The presence of hijacked journals in Scopus has far-reaching effects. Papers published in these journals may be cited, and unauthorized content from these journals in Scopus is thus imported into other databases, including ORCID and the WHO COVID-19 Research Database. This poses a particular challenge for research evaluation in those countries, where cloned versions of approved journals may be used to acquire publications and verifying their authenticity can be difficult. © 2023 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals LLC on behalf of Association for Information Science and Technology.",395-422,10.1002/asi.24855,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177730960&doi=10.1002%2fasi.24855&partnerID=40&md5=4b2cdcf54b854ebba15671567d95a991,2023
Normalizing Large Scale Sensor-Based MWD Data: An Automated Method toward A Unified Database,"In the context of geo-infrastructures and specifically tunneling projects, analyzing the large-scale sensor-based measurement-while-drilling (MWD) data plays a pivotal role in assessing rock engineering conditions. However, handling the big MWD data due to multiform stacking is a time-consuming and challenging task. Extracting valuable insights and improving the accuracy of geoengineering interpretations from MWD data necessitates a combination of domain expertise and data science skills in an iterative process. To address these challenges and efficiently normalize and filter out noisy data, an automated processing approach integrating the stepwise technique, mode, and percentile gate bands for both single and peer group-based holes was developed. Subsequently, the mathematical concept of a novel normalizing index for classifying such big datasets was also presented. The visualized results from different geo-infrastructure datasets in Sweden indicated that outliers and noisy data can more efficiently be eliminated using single hole-based normalizing. Additionally, a relational unified PostgreSQL database was created to store and automatically transfer the processed and raw MWD as well as real time grouting data that offers a cost effective and efficient data extraction tool. The generated database is expected to facilitate in-depth investigations and enable application of the artificial intelligence (AI) techniques to predict rock quality conditions and design appropriate support systems based on MWD data. © 2024 by the authors.",-,10.3390/s24041209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185561482&doi=10.3390%2fs24041209&partnerID=40&md5=5cb27b9fe25b5c2a65a178b4b28694e7,2022
Instant learning based on deep neural network with linear discriminant analysis features extraction for accurate iris recognition system,"Biometric-based identity verification systems have gained substantial attention due to their ability to provide high-level security. Among these systems, iris recognition systems have emerged as one of the most accurate and complex verification approaches. However, an ideal recognition system with a short processing time has not yet been reported in the literature because of the trade-offs involved. In this article, a novel framework for an iris recognition system is proposed based on hybrid deep neural network (DNN) classification-based Linear Discriminant Analysis (LDA) for feature extraction. The developed system includes unique pre-processing steps for both training and testing datasets, which are modulated by greyscale conversation, Gaussian blurring, binary imaging, contour segmentation and resizing. The proposed LDA-DNN provides high accuracy and stability for human identity verification with a short processing time. The proposed model accomplishes this task perfectly without any loss, which is unique among this type of approach. The results are validated via five computed measurement parameters. Experimental results are obtained by applying the model to four typical existing databases for powerful validation. Moreover, the proposed LDA-DNN framework results are compared with outcome measures obtained for state-of-the-art iris recognition approaches. The experimental results illustrate the success and power of the proposed LDA-DNN model, which attains an accuracy of 100% within a time of 70 ms, corresponding to an ideal recognition result that validated using several databases. Furthermore, this work provides a model within a unique property, in which it does not require a specific database or measurement parameters for evaluation. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",32099-32122,10.1007/s11042-023-16751-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171424677&doi=10.1007%2fs11042-023-16751-6&partnerID=40&md5=1b9a48a4de7c702a768852ced592ace3,2024
Continuous Speech Database Acquisition of Various Dialects from Maharashtra Region,"The development of a speech dataset in the Marathwada dialect of the Marathi language, which is spoken across the Maharashtra state is described in this paper. The study describes the creation of a continuous sentences speech dataset in Marathi for different dialects, as well as a continuous speech database for various dialects in various Maharashtra areas. The newly developed voice database will be useful to all Marathi language scholars. Any different variety of a language spoken by a group of people is referred to as a dialect. Dialect refers to variances in a language's speaking patterns. Due to the speakers' regional and cultural disparities, individuals' exposure to native languages impacts their accents and choice of words through conversation. Marathi seems to be an Indo- Aryan linguistic pronounced primarily by the Marathi community throughout Maharashtra, India. It is the state language of Maharashtra. The spoken form of Marathi varies by area. These changes have a significant impact on the utterance architecture of phonology. Such changes have a significant impact on the utterance architecture of phonology. Four separate Marathi dialects, including Marathwada area (Marathi), Khandesh (Ahirani), Vidarbha region (Varhadi), and Kokan Region (malvani), were studied in this paper for their effect on phonetic duration through spoken phrases. © 2023 IEEE.",-,10.1109/ICICIS56802.2023.10430275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186514086&doi=10.1109%2fICICIS56802.2023.10430275&partnerID=40&md5=9b961e87e288d90223f7200b0d4be10c,2024
Optimal Push and Pull-Based Edge Caching For Dynamic Content,"We introduce a framework and optimal &#x2018;fresh&#x2019; caching for a content distribution network (CDN) comprising a front-end local cache and a back-end database. The data content is dynamically updated at a back-end database and end-users are interested in the most-recent version of that content. We formulate the average cost minimization problem that captures the system&#x2019;s cost due to the service of aging content as well as the regular cache update cost. We consider the cost minimization problem from two individual perspectives based on the available information to either side of the CDN: the back-end database perspective and the front-end local cache perspective. For the back-end database, the instantaneous version of content is observable but the exact demand is not. Caching decisions made by the back-end database are termed &#x2018;push-based caching.&#x2019; For the front-end local cache, the age of content version in the cache is not observable, yet the instantaneous demand is. Caching decisions made by the front-end local cache are termed &#x2018;pull-based caching.&#x2019; Our investigations reveal which type of information, updates, or demand dynamic, is of higher value towards achieving the minimum cost based on other network parameters including content popularity, update rate, and demand intensity. IEEE",1-13,10.1109/TNET.2024.3352029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187011877&doi=10.1109%2fTNET.2024.3352029&partnerID=40&md5=20580c57f68c1e44ef8665c01c7560d7,2024
Regression-based Path Loss Model Correction to Construct Fingerprint Database for Indoor Localization,"The fingerprint-based indoor localization has been widely used due to its simple hardware setup and high positioning accuracy, especially using Received Signal Strength Indicator (RSSI). However, the fingerprint database has main drawbacks in database construction, requiring a lot of effort and time. This paper presents an approach for reducing the effort of manual fingerprint database construction for indoor localization using path loss model enhancement via simple regression, i.e., Linear and Polynomial Regression for RSSI-based fingerprint technique. We used the public dataset to evaluate our proposal, which was collected in a small room with low interference using three wireless technologies (Wi-Fi, ZigBee, and Bluetooth Low Energy). The K-nearest neighbors (KNN) is applied to locate the target. We compared the results from the original path loss model (O-PLM), the linear regression-path loss model (LR-PLM), and the polynomial regression path loss model (PR-PLM) with the actual RSSI values to validate our approach. The results showed that the Original Path Loss Model database and the Polynomial Regression Path Loss Model database improved the localization accuracy for Wi-Fi devices. The Linear Regression Path Loss Model can perform well in the ZigBee device case.  © 2023 ACM.",181-186,10.1145/3592307.3592335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169784578&doi=10.1145%2f3592307.3592335&partnerID=40&md5=c4f353ccb5212cdf8b7dbf68999b8da2,2023
An Ensemble Classification Model for Medical Databases Using Hybrid Weights,"Large databases are now frequently utilized to identify and diagnose medical disorders using extreme learning procedures. Due to its promising implementation and processing speed, the fundamental model of the study utilized for real-time applications is the ensemble classifier. Standard approaches associated with extreme learning practices project the inability for error prediction based on output layer hidden's selection under static weight selection. In this study, a unique weighted extreme learning machine (WELM) for predicting medical conditions is introduced. This research will help to solve the classification problems in the field of healthcare and medicine holds significant promise for improving the accuracy, validation, accountability, and reliability of medical data classification tasks. Moreover, the key objective for the weighted extreme learner's approach is to predict the illness by outlining the high-dimensional data applied for the case study. Typically, the practice proposed an ensemble model which functions the enhances the accurate predictions of high-dimensional cancer showing significant impact of the diagnosis and treatment planning. Furthermore, the WELM model effectiveness is validated through other ensemble learning simulations that include neural networks, random forest, PSO + NN, and ACO + NN techniques. Evaluation for the outcomes is verified through various medical datasets with attributes of the liver, ovarian, lung, diabetes, and DLBCL-Stanford. The results show that the WELM described is very computationally efficient which is related to true positive rate, accuracy, and error rate. © The Institution of Engineers (India) 2024.",-,10.1007/s40031-024-01006-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186444686&doi=10.1007%2fs40031-024-01006-1&partnerID=40&md5=e588936513c8bd8cf5ab493699791fd9,2024
A lightweight hybrid CNN-LSTM explainable model for ECG-based arrhythmia detection,"Objective: Electrocardiogram (ECG) is the most frequent and routine diagnostic tool used for monitoring heart electrical signals and evaluating its functionality. The human heart can suffer from a variety of diseases, including cardiac arrhythmias. Arrhythmia is an irregular heart rhythm that in severe cases can lead to stroke and can be diagnosed via ECG recordings. Since early detection of cardiac arrhythmias is of great importance, computerized and automated classification and identification of these abnormal heart signals have received much attention for the past decades. Methods: This paper introduces a light Deep Learning (DL) approach for high accuracy detection of 8 different cardiac arrhythmias and normal rhythms. To employ DL techniques, the ECG signals were preprocessed using resampling and baseline wander removal techniques. The classification was performed using an 11-layer network employing a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM). Results: In order to evaluate the proposed technique, ECG signals are chosen from the two physionet databases, the MIT-BIH arrhythmia database and the long-term AF database. The proposed DL framework based on the combination of CNN and LSTM showed promising results than most of the state-of-the-art methods. The proposed method reaches the mean diagnostic accuracy of 98.24%. Conclusion: A trained model for arrhythmia classification using diverse ECG signals were successfully developed and tested. Significance: This study presents a lightweight classification technique with high diagnostic accuracy compared to other notable methods, making it a potential candidate for implementation in Holter monitor devices for arrhythmia detection. Finally, we used SHapley Additive exPlanations (SHAP), the most popular Explainable Artificial Intelligence (XAI) method to understand how our model make predictions. The results indicate that those features (ECG samples) that have contributed the most to a prediction are consonant with clinicians’ decisions. Therefore, the use of interpretable models increases the trust of clinicians in AI and thus leads to decreasing the number of misdiagnoses of cardiovascular diseases. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181051857&doi=10.1016%2fj.bspc.2023.105884&partnerID=40&md5=6edddee38f00527fa0e77b5719bb58bc,2022
AVID: A speech database for machine learning studies on vocal intensity,"Vocal intensity, which is quantified typically with the sound pressure level (SPL), is a key feature of speech. To measure SPL from speech recordings, a standard calibration tone (with a reference SPL of 94 dB or 114 dB) needs to be recorded together with speech. However, most of the popular databases that are used in areas such as speech and speaker recognition have been recorded without calibration information by expressing speech on arbitrary amplitude scales. Therefore, information about vocal intensity of the recorded speech, including SPL, is lost. In the current study, we introduce a new open and calibrated speech/electroglottography (EGG) database named Aalto Vocal Intensity Database (AVID). AVID includes speech and EGG produced by 50 speakers (25 males, 25 females) who varied their vocal intensity in four categories (soft, normal, loud and very loud). Recordings were conducted using a constant mouth-to-microphone distance and by recording a calibration tone. The speech data was labelled sentence-wise using a total of 19 labels that support the utilisation of the data in machine learning (ML) -based studies of vocal intensity based on supervised learning. In order to demonstrate how the AVID data can be used to study vocal intensity, we investigated one multi-class classification task (classification of speech into soft, normal, loud and very loud intensity classes) and one regression task (prediction of SPL of speech). In both tasks, we deliberately warped the level of the input speech by normalising the signal to have its maximum amplitude equal to 1.0, that is, we simulated a scenario that is prevalent in current speech databases. The results show that using the spectrogram feature with the support vector machine classifier gave an accuracy of 82% in the multi-class classification of the vocal intensity category. In the prediction of SPL, using the spectrogram feature with the support vector regressor gave an mean absolute error of about 2 dB and a coefficient of determination of 92%. We welcome researchers interested in classification and regression problems to utilise AVID in the study of vocal intensity, and we hope that the current results could serve as baselines for future ML studies on the topic. © 2024 The Author(s)",-,10.1016/j.specom.2024.103039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184995100&doi=10.1016%2fj.specom.2024.103039&partnerID=40&md5=3d30be8332d9a964e355f070d4a9a3a1,2020
Analysis and Characterization of Cyber Threats Leveraging the MITRE ATT&CK Database,"MITRE ATT&CK is a comprehensive knowledge-base of adversary tactics, techniques, and procedures (TTP) based on real-world attack scenarios. It has been used in different sectors, such as government, academia, and industry, as a foundation for threat modeling, risk assessment, and defensive strategies. There are valuable insights within MITRE ATT&CK knowledge-base that can be applied to various fields and applications, such as risk assessment, threat characterization, and attack modeling. No previous work has been devoted to the comprehensive collection and investigation of statistical insights of the MITRE ATT&CK dataset. Hence, this work aims to extract, analyze, and represent MITRE ATT&CK statistical insights providing valuable recommendations to improve the security aspects of Enterprise, Industrial Control Systems (ICS), and mobile digital infrastructures. For this purpose, we conduct a hierarchical analysis starting from MITRE ATT&CK threat profiles toward the list of techniques in the MITRE ATT&CK database. Finally, we summarize our key findings while providing recommendations that will pave the way for future research in the area.  © 2013 IEEE.",1217-1234,10.1109/ACCESS.2023.3344680,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181580017&doi=10.1109%2fACCESS.2023.3344680&partnerID=40&md5=f7693f2c4a6275a0d22beab2c83f88eb,2024
The Extent of AI Applications in EFL Learning and Teaching,"Foreign language teaching, like almost all other aspects of human existence, has been substantially influenced by recent advances in modern information and communication technologies, such as augmented reality, virtual reality, and artificial intelligence (AI). Although AI has been in use for almost 30 years, educators remain skeptical toward the use of AI-technology in the education field more broadly, and how its use might meaningfully affect English language skills. Through a systematic review, this work seeks to provide a summary of the available literature with regard to the applications of AI in English as a foreign language (EFL) education. This review considers a wide range of AI technologies and methodologies, with a specific focus on the integration of AI into the realm of EFL education. The review then delineates the possible effects of AI in terms of developing students' language skills, students' and teachers' perceptions of using AI applications, and the difficulties and challenges inherent to implementing AI applications. The discussion culminates by identifying research gaps.  © 2008-2011 IEEE.",653-663,10.1109/TLT.2023.3322128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174856569&doi=10.1109%2fTLT.2023.3322128&partnerID=40&md5=5f64c3668a1e2f25551eebcbff77d473,2020
Graph Based Frequent Item Set Generation in Association Rule Mining,"This research proposes a new technique that uses a reconstruction strategy for categorical datasets to preserve the privacy of the association rules. In order to identify all frequent item association sets and determine sets comprising frequent items based on the association sets, the suggested technique takes advantage of the two-item permutation set to create an association part. The enhanced method scans the database only once, addressing a flaw in the conventional approach that scans the database twice and so increasing scanning costs. The database and association sets can be mined using the two-item permutation set, which has the advantages of operating more quickly, requiring less memory, and being simple to maintain. © 2023 IEEE.",-,10.1109/NEleX59773.2023.10421720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186521089&doi=10.1109%2fNEleX59773.2023.10421720&partnerID=40&md5=7a9dbdd5ddfbc652cf77f6ee045b1454,2021
Uniform Reliability for Unbounded Homomorphism-Closed Graph Queries,"We study the uniform query reliability problem, which asks, for a fixed Boolean query Q, given an instance I, how many subinstances of I satisfy Q. Equivalently, this is a restricted case of Boolean query evaluation on tuple-independent probabilistic databases where all facts must have probability 1/2. We focus on graph signatures, and on queries closed under homomorphisms. We show that for any such query that is unbounded, i.e., not equivalent to a union of conjunctive queries, the uniform reliability problem is #P-hard. This recaptures the hardness, e.g., of s-t connectedness, which counts how many subgraphs of an input graph have a path between a source and a sink. This new hardness result on uniform reliability strengthens our earlier hardness result on probabilistic query evaluation for unbounded homomorphism-closed queries [2]. Indeed, our earlier proof crucially used facts with probability 1, so it did not apply to the unweighted case. The new proof presented in this paper avoids this; it uses our recent hardness result on uniform reliability for non-hierarchical conjunctive queries without self-joins [3], along with new techniques. © Antoine Amarilli; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150689465&doi=10.4230%2fLIPIcs.ICDT.2023.14&partnerID=40&md5=81b2e0da0ba9e0bb66fa0c4922b4ea3c,2019
Constant-Delay Enumeration for SLP-Compressed Documents,"We study the problem of enumerating results from a query over a compressed document. The model we use for compression are straight-line programs (SLPs), which are defined by a context-free grammar that produces a single string. For our queries we use a model called Annotated Automata, an extension of regular automata that allows annotations on letters. This model extends the notion of Regular Spanners as it allows arbitrarily long outputs. Our main result is an algorithm which evaluates such a query by enumerating all results with output-linear delay after a preprocessing phase which takes linear time on the size of the SLP, and cubic time over the size of the automaton. This is an improvement over Schmid and Schweikardt's result [25], which, with the same preprocessing time, enumerates with a delay which is logarithmic on the size of the uncompressed document. We achieve this through a persistent data structure named Enumerable Compact Sets with Shifts which guarantees output-linear delay under certain restrictions. These results imply constant-delay enumeration algorithms in the context of regular spanners. Further, we use an extension of annotated automata which utilizes succinctly encoded annotations to save an exponential factor from previous results that dealt with constant-delay enumeration over vset automata. Lastly, we extend our results in the same fashion Schmid and Schweikardt did [26] to allow complex document editing while maintaining the constant-delay guarantee. © Martín Muñoz and Cristian Riveros; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150741210&doi=10.4230%2fLIPIcs.ICDT.2023.7&partnerID=40&md5=f86643bece6c0811e1c46bc42a73a223,2020
An Optimal Algorithm for Sliding Window Order Statistics,"Assume there is a data stream of elements and a window of size m. Sliding window algorithms compute various statistic functions over the last m elements of the data stream seen so far. The time complexity of a sliding window algorithm is measured as the time required to output an updated statistic function value every time a new element is read. For example, it is well known that computing the sliding window maximum/minimum has time complexity O(1) while computing the sliding window median has time complexity O(log m). In this paper we close the gap between these two cases by (1) presenting an algorithm for computing the sliding window k-th smallest element in O(log k) time and (2) prove that this time complexity is optimal. © Pavel Raykov; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150697484&doi=10.4230%2fLIPIcs.ICDT.2023.5&partnerID=40&md5=7194d8e7664eaf88bc328eaa84793cbf,2021
Degree Sequence Bound for Join Cardinality Estimation,"Recent work has demonstrated the catastrophic effects of poor cardinality estimates on query processing time. In particular, underestimating query cardinality can result in overly optimistic query plans which take orders of magnitude longer to complete than one generated with the true cardinality. Cardinality bounding avoids this pitfall by computing an upper bound on the query's output size using statistics about the database such as table sizes and degrees, i.e. value frequencies. In this paper, we extend this line of work by proving a novel bound called the Degree Sequence Bound which takes into account the full degree sequences and the max tuple multiplicity. This work focuses on the important class of Berge-Acyclic queries for which the Degree Sequence Bound is tight. Further, we describe how to practically compute this bound using a functional approximation of the true degree sequences and prove that even this functional form improves upon previous bounds. © Kyle Deeds, Dan Suciu, Magda Balazinska, and Walter Cai; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150714940&doi=10.4230%2fLIPIcs.ICDT.2023.8&partnerID=40&md5=872abd1c45c29e1b3feb98a79a641102,2022
The Consistency of Probabilistic Databases with Independent Cells,"A probabilistic database with attribute-level uncertainty consists of relations where cells of some attributes may hold probability distributions rather than deterministic content. Such databases arise, implicitly or explicitly, in the context of noisy operations such as missing data imputation, where we automatically fill in missing values, column prediction, where we predict unknown attributes, and database cleaning (and repairing), where we replace the original values due to detected errors or violation of integrity constraints. We study the computational complexity of problems that regard the selection of cell values in the presence of integrity constraints. More precisely, we focus on functional dependencies and study three problems: (1) deciding whether the constraints can be satisfied by any choice of values, (2) finding a most probable such choice, and (3) calculating the probability of satisfying the constraints. The data complexity of these problems is determined by the combination of the set of functional dependencies and the collection of uncertain attributes. We give full classifications into tractable and intractable complexities for several classes of constraints, including a single dependency, matching constraints, and unary functional dependencies. © Amir Gilad, Aviram Imber, and Benny Kimelfeld; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150732684&doi=10.4230%2fLIPIcs.ICDT.2023.22&partnerID=40&md5=98f997b6e3eb76b73b6e3834b8533726,2023
Conjunctive Queries on Probabilistic Graphs: The Limits of Approximability,"Query evaluation over probabilistic databases is a notoriously intractable problem - not only in combined complexity, but for many natural queries in data complexity as well [7, 14]. This motivates the study of probabilistic query evaluation through the lens of approximation algorithms, and particularly of combined FPRASes, whose runtime is polynomial in both the query and instance size. In this paper, we focus on tuple-independent probabilistic databases over binary signatures, which can be equivalently viewed as probabilistic graphs. We study in which cases we can devise combined FPRASes for probabilistic query evaluation in this setting. We settle the complexity of this problem for a variety of query and instance classes, by proving both approximability and (conditional) inapproximability results. This allows us to deduce many corollaries of possible independent interest. For example, we show how the results of [8] on counting fixed-length strings accepted by an NFA imply the existence of an FPRAS for the two-terminal network reliability problem on directed acyclic graphs: this was an open problem until now [37]. We also show that one cannot extend a recent result [34] that gives a combined FPRAS for self-join-free conjunctive queries of bounded hypertree width on probabilistic databases: neither the bounded-hypertree-width condition nor the self-join-freeness hypothesis can be relaxed. Finally, we complement all our inapproximability results with unconditional lower bounds, showing that DNNF provenance circuits must have at least moderately exponential size in combined complexity. © Antoine Amarilli, Timothy van Bremen, and Kuldeep S. Meel.",-,10.4230/LIPIcs.ICDT.2024.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188638620&doi=10.4230%2fLIPIcs.ICDT.2024.15&partnerID=40&md5=e5dff7400b4953bf7d95ffeda07f095f,2021
Parallel construction of RNA databases for extensive lncRNA-RNA interaction prediction,"Long non-coding RNA sequences (lncRNAs) have completely changed how scientists approach genetics. While some believe that many lncRNAs are results of spurious transcriptions, recent evidence suggests that there exist thousands of them and that they have functions and regulate key biological processes. For the experimental characterization of lncRNAs, many tools that try to predict their interactions with other RNAs have been developed. Some of the fastest and more accurate tools, however, require a slow database construction step prior to the identification of interaction partners for each lncRNA. This paper presents a novel and efficient parallel database construction procedure. Benchmarking results on a 16-node multicore cluster show that our parallel algorithm can build databases up to 318 times faster than other tools in the market using just 256 CPU cores. All the code developed in this work is available to download at GitHub under the MIT License (https://github.com/UDC-GAC/pRIblast).  © 2023 Owner/Author(s).",555-558,10.1145/3555776.3577772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162887452&doi=10.1145%2f3555776.3577772&partnerID=40&md5=79a0d9ffbb22a69cb8a846dff7065db3,2020
Cloud Based Online Bus Ticket System,"The current system for booking bus tickets is reliant on humans and can be cumbersome. This project aims to develop a web application that simplifies the ticket booking process, allowing students, employees, and anyone else to easily book bus tickets. Unlike existing applications that only provide information on travel destinations and fares, the application not only facilitates ticket booking but also stores the tickets in a secure cloud database. Users can purchase tickets using their smartphones or laptops through the web application, eliminating the need to carry physical railway tickets. The ticketing information is securely stored in the cloud database, and a ticket checker application is provided to authenticate user tickets. The ticket checker can search for a user's ticket in the cloud database using the ticket number or relevant information for verification purposes. In case a user's display is damaged or there are issues like battery failure preventing the display of the ticket, there was an alternative safeguard option. The ticket can be validated by searching the ticket database using the ticket number or other relevant user information. © 2023 IEEE.",-,10.1109/ICRASET59632.2023.10420257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186098675&doi=10.1109%2fICRASET59632.2023.10420257&partnerID=40&md5=17e7d4fd64d9d1fd96b0e78f39651321,2022
An enhanced ResNet-50 deep learning model for arrhythmia detection using electrocardiogram biomedical indicators,"Electrocardiogram (ECG) is one among the most common detecting techniques in the analysis and detection of cardiac arrhythmia adopted due to its cost efficiency and simplicity. In a clinical routine, ECG database is collected on daily basis and these databases are reviewed manually. Along with other conventional methods, various approaches using machine learning has been proposed in the past few years. But these would require in-depth knowledge on several parameters and pre-processing techniques in the specific domain. This study is aimed at implementing a more reliable deep learning model that has the capacity to diagnose arrhythmia from a database with 109,446 samples in 5 different categories. In our proposed work, we have used deep learning methodologies for the diagnosis and detection of cardiac arrhythmia automatically. Balancing the biasedness in the waveforms from MIT-BIH arrhythmia database, model is developed. MIT-BIH arrhythmia database with the ECG waveforms promises good accuracy. This automated prediction of the disease using CNN and ResNet-18 architectures are compared in terms of accuracy. CNN has accuracy approximately 97.86% and 98.14% for improved ResNet-18. Also, a comparative analysis is done with the proposed model and already existing techniques. Several limitations and future opportunities are also reviewed. We believe it can be used considerably for cardiac arrhythmia prediction worldwide. Based on the results obtained, ResNet-18 architecture can be used as an efficient procedure, that reduces the burden of training a deep convolutional neural network from start, resulting in a technique that is simple to use. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",83-97,10.1007/s12530-023-09559-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180687186&doi=10.1007%2fs12530-023-09559-0&partnerID=40&md5=c4ac860135f24b6d2b4b34b6b9b53fde,2021
A Voice-Assisted Approach for Vehicular Data Querying from Automotive IoT-Based Databases,"The automotive industry's transformative conver-gence of cutting-edge technologies, such as the Internet of Things (loT), electronic voice assistants, and custom APls, have paved the way for a remarkable array of opportunities to enhance vehicle security while revolutionizing user-car interactions. Actually, when leveraging loT sensors of multiple types and configurations, their produced, collected, and stored real-time data may offer in-depth and comprehensive insights into vehicle information, opening many possibilities when loT-generated databases are created. In this context, the primary goal of this paper is to create a new approach based on the popular Amazon Alexa voice assistant, which would allow facilitated queries of vehicular data from cloud-based databases. For that, a new Alexa service (skill) is created, as well as a custom API, allowing easy access to different types of data previously retrieved from vehicular sensors and properly stored. Doing so, the implemented skill indirectly processes data from loT sensors through the custom API, enabling users to access vital vehicular information using intuitive voice commands, remotely. A case study in a real scenario is conducted to validate and confirm the feasibility of real-time vehicle information access, showcasing the benefits of the proposed approach when combined with other solutions in a macro vehicular-centric loT ecosystem.  © 2023 IEEE.",-,10.1109/SIoT60039.2023.10389856,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184816143&doi=10.1109%2fSIoT60039.2023.10389856&partnerID=40&md5=9b5e6dc75a8b82662a35a56212621c06,2021
Simpler is Much Faster: Fair and Independent Inner Product Search,"The problem of inner product search (IPS) is important in many fields. Although maximum inner product search (MIPS) is often considered, its result is usually skewed and static. Users are hence hard to obtain diverse and/or new items by using the MIPS problem. Motivated by this, we formulate a new problem, namely the fair and independent IPS problem. Given a query, a threshold, and an output size k, this problem randomly samples k items from a set of items such that the inner product of the query and item is not less than the threshold. For each item that satisfies the threshold, this problem is fair, because the probability that such an item is outputted is equal to that for each other item. This fairness can yield diversity and novelty, but this problem faces a computational challenge. Some existing (M)IPS techniques can be employed in this problem, but they require O(n) or o(n) time, where n is the dataset size. To scale well to large datasets, we propose a simple yet efficient algorithm that runs in O(logn + k) expected time. We conduct experiments using real datasets, and the results demonstrate that our algorithm is up to 330 times faster than baselines. © 2023 Copyright held by the owner/author(s).",2379-2383,10.1145/3539618.3592061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168650384&doi=10.1145%2f3539618.3592061&partnerID=40&md5=80cd7cae838f80d4b4820983043f6506,2022
Advancing Early Detection of Sepsis With Temporal Convolutional Networks Using ECG Signals,"In the hours leading up to the onset time of sepsis, the autonomic nervous system presents sub-clinical indicators of disease that may not be observable to providers. The objective of this research is to create an interpretable sepsis prediction algorithm utilizing continuous electrocardiography (ECG) signals, with the aim of implementing it in patient monitoring systems for individuals in intensive care units (ICU). We develop an early sepsis detection algorithm utilizing two datasets; in particular, the Medical Information Mart for Intensive Care (MIMIC-III) Clinical Dataset and MIMIC-Waveform Database. We carry out a systematic approach to selecting ECG segments of superior quality that are recorded in highly dynamic intensive care unit environments. Later, we use the single-lead ECG waveform to investigate the potential of heart rate variability (HRV) for continuous monitoring. In this study, 715 patients were included, of whom 65 are sepsis patients labeled with recent sepsis definition on an hourly basis. The predictive potential of the critical features is visualized to assist the interpretation of the model in a clinical practice. Moreover, since we are framing the early sepsis prediction as a supervised time series classification task, we evaluate the model performance by implementing Temporal Convolutional Networks (TCN). Performance analysis reported with varying prediction windows preceding sepsis onset time using area under the receiver-operating-characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). The deep learning model delivers promising results by leveraging time series with the use of temporal convolutions. Our findings reveal that the HRV characteristics of adults can be a valuable indicator for continuous sepsis monitoring in an ICU. Finally, this research work adds to the field of early sepsis detection by providing an annotated continuous waveform dataset from the MIMIC-Waveform Database, which is made accessible to the public.  © 2013 IEEE.",3417-3427,10.1109/ACCESS.2023.3349242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181568370&doi=10.1109%2fACCESS.2023.3349242&partnerID=40&md5=181a4c75b50b2a560544536cf04cf318,2021
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting,"Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to be performed one-time per new database with minimal human intervention. Our approach demonstrates superior performance on the KaggleDBQA dataset, designed to evaluate generalizability for the Text-to-SQL task. We further showcase consistent performance improvement of LTMP-DA-GP over GP, across LLMs and databases of KaggleDBQA, highlighting the efficacy and model agnostic benefits of our prompt based adapt and decompose approach. © 2023 Association for Computational Linguistics.",143-151,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184519542&partnerID=40&md5=06c8c96bf76349c67f5a2b5581eb65fa,2024
Database Design for Pre-Eclampsia Case Administration,"The Ministry of Health of Indonesia has referred to pre-eclampsia as one of the most severe diseases affecting women. As an urgency, it is crucial to administrate pre-eclampsia cases for disease prevention as a long-term national healthcare strategy. Regarding health science, case data was significant in developing research and innovation. However, the main problem regarding pre-eclampsia case administration is data handling, recording, and management incompetence. Hence, this research proposed a conceptual design of a database for pre-eclampsia case administration. The proposed design covered conceptual, logical, and physical design. We elaborate the concept into three concepts of pre-eclampsia disease: pre-treatment, treatment, and post-treatment. This study proposed a solution to gain more data and study pre-eclampsia disease in Indonesia.  © 2023 IEEE.",-,10.1109/ICITDA60835.2023.10426919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186496688&doi=10.1109%2fICITDA60835.2023.10426919&partnerID=40&md5=86e9c0323f5273c503e69ba39c700fce,2023
DBMS-Assisted Live Migration of Virtual Machines,"Live migration of virtual machines (VMs) is a technique that moves active VMs between different physical hosts without losing any running states. Although it is desirable for administrators that the live migration is completed as quickly as possible, the pre-copy-based live migration, widely used in modern hypervisors, does not satisfy this demand on the current trend that VMs on which running applications are performance-critical such as database management systems (DBMSes) have quite large memory. DMigrate, presented in this paper, shortens the time for live-migrating VMs with even large memory DBMSes. To quickly produce the running state of the migrating VMs on the destination, DMigrate performs regular memory transfers while simultaneously constructing the DBMS's buffer-pool by fetching the data items from the shared storage. We prototyped DMigrate on MySQL 5.7.30, QEMU 5.1.0, and Linux 4.18.20. The experimental results show that the migration time of the prototype is up to 1.71 × × and 1.71 × × shorter under workloads, including sysbench and TPC-C, than the default pre-copy and post-copy schemes, respectively.  © 1968-2012 IEEE.",380-393,10.1109/TC.2023.3329943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177028453&doi=10.1109%2fTC.2023.3329943&partnerID=40&md5=8e03dbc6565c092c68cd07e216ce8e81,2022
NLSQL: Generating and Executing SQL Queries via Natural Language Using Large Language Models,"The goal of data management has long been to make data more approachable for non-technical users. Natural language interfaces (NLIs), which let users interact with data by asking questions in natural language, have become more popular in recent years. Natural language is often vague, and user queries often don't give enough information. This makes it hard to use NLIs for database querying. In order to solve this problem, this study suggests using large language models (LLMs) to turn free-form natural language queries into SQL statements that can then be run on databases. The proposed system, NLSQL, makes use of LLMs like GPT-3 and shows how efficiently prompt engineering can be done in order to extract from LLMs the desired code for SQL queries. NLSQL shows that using pre-trained LLMs along with the suggested priming prompts is an accurate and reliable way to create and run SQL queries in natural language, even if the queries aren't very well written or are missing important information. Unlike traditional methods, which involve making grammar rules by hand, this method improves query inference and makes the development of NLIs faster and cheaper. The study shows that the suggested method is safe, protects privacy, and can be used with many different databases. If NLSQL is used to make databases easier for people who aren't tech-savvy to use, they may not need as much training in SQL and related technologies, which would save a company both time and money. © 2023 IEEE.",-,10.1109/ICACTA58201.2023.10392861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184801742&doi=10.1109%2fICACTA58201.2023.10392861&partnerID=40&md5=8c3a7a105259314e5b25030477e50694,2023
Hermite Based Parametric Representation of Magnetohydrodynamic Effect for the Generation of Synthetic ECG Signals During Magnetic Resonance Imaging,"Aim. ECG signals during Magnetic Resonance Imaging (MRI) are distorted by a magnetohydrodynamic (MHD) artefact. We proposed a model to generate synthetic MHD artefacts to augment a dataset of standard ECG and to train deep learning models more robust to this distortion. Methods. An open database of ECG in MRI was used to extract a median MHD template over a small subject population. These were decomposed on a basis of Hermite functions to represent the MHD effect by a set of 29 parameters. A Gaussian mixture model was fitted on these coefficients, which allows MHD artefacts to be generated by sampling this probability distribution. The model was assessed on a heartbeat classification task on an in-house database of ECG signals acquired in a 1.5T MRI scanner. A convolutional neural network (CNN) trained on the MIT-BIH arrhythmia (MITAR) database without pretraining was compared with models pretrained on the CinC 2021 database using the proposed MHD specific data augmentation. Results. The randomly initialized CNN, and the proposed augmentation obtained average F1 scores of 0.21, and 0.44 respectively on the in-house MRI database. Conclusion. The proposed MHD artefact generator can be used to effectively augment ECG data and learn a representation more robust to MRI environment distortions. © 2023 CinC.",-,10.22489/CinC.2023.116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182325417&doi=10.22489%2fCinC.2023.116&partnerID=40&md5=6118d913afc7d08b1fa64e8557abb9fa,2021
ProSA: A provenance system for reproducing query results,"Good scientific work requires comprehensible, transparent and reproducible research. One way to ensure this is to include all data relevant to a study or evaluation when publishing an article. This data should be at least aggregated or anonymized, at best compact and complete, but always resilient. In this paper we present ProSA, a system for calculating the minimal necessary data set, called sub-database. For this, we combine the Chase - a set of algorithms for transforming databases - with additional provenance information. We display the implementation of provenance guided by the ProSA pipeline and show its use to generate an optimized sub-database. Furhter, we demonstrate how the ProSA GUI looks like and present some applications and extensions.  © 2023 Owner/Author.",1555-1558,10.1145/3543873.3587563,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159625019&doi=10.1145%2f3543873.3587563&partnerID=40&md5=8842858c1387d130974fe0c054bd1e8f,2023
Chemical Multiverse and Diversity of Food Chemicals,"Food chemicals have a fundamental role in our lives, with an extended impact on nutrition, disease prevention, and marked economic implications in the food industry. The number of food chemical compounds in public databases has substantially increased in the past few years, which can be characterized using chemoinformatics approaches. We and other groups explored public food chemical libraries containing up to 26,500 compounds. This study aimed to analyze the chemical contents, diversity, and coverage in the chemical space of food chemicals and additives and, from here on, food components. The approach to food components addressed in this study is a public database with more than 70,000 compounds, including those predicted via omics techniques. It was concluded that food components have distinctive physicochemical properties and constitutional descriptors despite sharing many chemical structures with natural products. Food components, on average, have large molecular weights and several apolar structures with saturated hydrocarbons. Compared to reference databases, food component structures have low scaffold and fingerprint-based diversity and high structural complexity, as measured by the fraction of sp3 carbons. These structural features are associated with a large fraction of macronutrients as lipids. Lipids in food components were decompiled by an analysis of the maximum common substructures. The chemical multiverse representation of food chemicals showed a larger coverage of chemical space than natural products and FDA-approved drugs by using different sets of representations. © 2024 The Authors. Published by American Chemical Society.",1229-1244,10.1021/acs.jcim.3c01617,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185597680&doi=10.1021%2facs.jcim.3c01617&partnerID=40&md5=3deff50a910f9de0a6c52da5d47b6817,2022
Classification of drought severity in contiguous USA during the past 21 years using fractal geometry,"Drought is characterized by a moisture deficit that can adversely impact the environment, economy, and society. In North America, like many regions worldwide, predicting the timing of drought events is challenging. However, our novel study in climate research explores whether the Drought Monitor database exhibits fractal characteristics, represented by a single scaling exponent. This database categorizes drought areas by intensity, ranging from D0 (abnormally dry) to D4 (exceptional drought). Through vibration analysis using power spectral densities (PSD), we investigate the presence of power-law scaling in various statistical moments across different scales within the database. Our multi-fractal analysis estimates the multi-fractal spectrum for each category, and the Higuchi algorithm assesses the fractal complexity, revealing that D4 follows a multi-fractal pattern with a wide range of exponents, while D0 to D3 exhibit a mono-fractal nature with a narrower range of exponents. © 2023, The Author(s).",-,10.1186/s13634-023-01094-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181241576&doi=10.1186%2fs13634-023-01094-z&partnerID=40&md5=93238fbdb09d44127cce1293448431a3,2022
FruityHub: A diverse collection of fruits created for edibility estimation,"The export of fresh fruits not only exerts a substantial influence on both national and global economies but also serves as a crucial source of livelihood for a vast number of farmers. Automated estimation of these fruits' edibility (i.e. freshness level) is very essential to maintain the quality of exported fruits and to reduce the financial loss. Owing to the over-whelming performances achieved by deep neural networks in performing image classification tasks in recent years, many researchers have designed automated methods by exploiting the deep neural architectures to perform non-invasive quality identification of several plant parts (leaves, fruits, vegetables, etc.) in more time-efficient and cost-effective manner. The most notable hindrance which is faced by researchers while designing the automated methods is the lack of proper databases which are required to train them. To mitigate this research gap, in this work we have created a novel database namely, FruityHub comprising of a total of 1786 image frames belonging to eight different types of fruits namely, Dragon Fruits, Net Melons, Strawberries, Star Fruit, Zucchini, Mango, Pineapple and Plum. The image frames belonging to each fruit category can be classified as Fresh, Damaged and Severely Damaged which facilitates automated estimation of edible quality of these fruits depending upon their skin color, texture, shape, etc. We have also validated the effectiveness of this database using fourteen deep neural networks namely, AlexNet, Vgg16, ResNet152, MobileNetv2, ShuffleNetv2, SqueezeNet, InceptionV3, InceptionResNetv2, DenseNet121, NasNetLarge, NasNetMobile, GhostNetv2, EfficientNetB7 and Xception. The validation is performed using quantitative metrics like Test Accuracy, Recall, Precision and F1 score.  © 2023 IEEE.",-,10.1109/IEMENTech60402.2023.10423480,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186264205&doi=10.1109%2fIEMENTech60402.2023.10423480&partnerID=40&md5=5323fcd2fda747c6583d308f22372437,2023
Empowering Database Learning Through Remote Educational Escape Rooms,"Learning about databases is indispensable for individuals studying software engineering or computer science or those involved in the IT industry. We analyzed a remote educational escape room for teaching about databases in four different higher education courses in two consecutive academic years. We employed three instruments for evaluation: a pre- and posttest to assess the escape room's effectiveness for student learning, a questionnaire to gather students' perceptions, and a Web platform that unobtrusively records students' interactions and performance. We show novel evidence that educational escape rooms conducted remotely can be engaging as well as effective for teaching about databases.  © 1997-2012 IEEE.",18-25,10.1109/MIC.2023.3333199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178026282&doi=10.1109%2fMIC.2023.3333199&partnerID=40&md5=50b2efb794019f4ed153017853c93140,2022
Processing-in-Memory for Databases: Query Processing and Data Transfer,"The Processing-in-Memory (PIM) paradigm promises to accelerate data processing by pushing down computation to memory, reducing the amount of data transfer between memory and CPU, and – in this way – relieving the CPU from processing. Particularly, in in-memory databases memory access becomes a performance bottleneck. Thus, PIM seems to offer an interesting solution for database processing. In this work, we investigate how commercially available PIM technology can be leveraged to accelerate query processing by offloading (parts of) query operators to memory. Furthermore, we show how to address the problem of limited PIM storage capacity by interleaving transfer and computation and present a cost model for the data placement problem. © 2023 Copyright held by the owner/author(s).",107-111,10.1145/3592980.3595323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163768576&doi=10.1145%2f3592980.3595323&partnerID=40&md5=b9358ae95ade3085efcc7b42558e03c1,2023
"The Past, Present, and Future of Typological Databases in NLP","Typological information has the potential to be beneficial in the development of NLP models, particularly for low-resource languages. Unfortunately, current large-scale typological databases, notably WALS and Grambank, are inconsistent both with each other and with other sources of typological information, such as linguistic grammars. Some of these inconsistencies stem from coding errors or linguistic variation, but many of the disagreements are due to the discrete categorical nature of these databases. We shed light on this issue by systematically exploring disagreements across typological databases and resources, and their uses in NLP, covering the past and present. We next investigate the future of such work, offering an argument that a continuous view of typological features is clearly beneficial, echoing recommendations from linguistics. We propose that such a view of typology has significant potential in the future, including in language modeling in low-resource scenarios. © 2023 Association for Computational Linguistics.",1163-1169,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183296689&partnerID=40&md5=dbc875adeea6305a29f8e33910b51158,2022
Automatic and manual prediction of epileptic seizures based on ECG,"This study presents a new attempt to quantify and predict changes in the ECG signal in the pre-ictal period. In the proposed approach, threshold techniques were applied to the standard deviation of two heart rate variability features (The number of heartbeats per two minutes and approximate entropy) computed to ensure prediction and quantification of the pre-ictal state. We analyzed clinical data taken from two epileptic public databases, Siena scalp EEG and post-ictal heart rate oscillations in partial epilepsy and a local database. By testing the proposed approach on the Siena scalp EEG database, we achieved a sensitivity of 100%, specificity of 95%, and an accuracy of 96.4% whereas using acquisitions from the post-ictal database, we achieved a sensitivity of 100%, specificity of 91% and an accuracy of 94% and using the local database we achieved a sensitivity of 100%, a specificity of 97% and an accuracy of 97.5%. Furthermore, the proposed approach predicted 58.7%, 57.2, and 40% of the seizures before the onset by more than 10 min for the data taken from post-ictal, local and Siena database, respectively. Using the automatic threshold technique, we were able to achieve a sensitivity, specificity, and accuracy of 85%, 81%, 82% using our local database, respectively, whereas using acquisitions take from the Siena scalp EEG database, we achieved a sensitivity of 75%, specificity of 85% and an accuracy of 82%. Besides, using the post-ictal database, we achieved a sensitivity of 90%, a specificity of 83% and an accuracy of 85%. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",-,10.1007/s11760-024-03063-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187938077&doi=10.1007%2fs11760-024-03063-x&partnerID=40&md5=5e443ff34ff6384fd37b56006bf1db27,2022
Method and Software Tool for Generating Artificial Databases of Biomedical Images Based on Deep Neural Networks,"A wide variety of biomedical image data, as well as methods for generating training images using basic deep neural networks, were analyzed. Additionally, all platforms for creating images were analyzed, considering their characteristics. The article develops a method for generating artificial biomedical images based on GAN. GAN architecture has been developed for biomedical image synthesis. The data foundation and module for generating training images were designed and implemented in a software system. A comparison of the generated image database with known databases was made. © 2023 Copyright for this paper by its authors.",15-26,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182280513&partnerID=40&md5=97d077a6694c76b0039d2d77229d7516,2023
Streamlining Temporal Formal Verification over Columnar Databases,"Recent findings demonstrate how database technology enhances the computation of formal verification tasks expressible in linear time logic for finite traces (LTLf). Human-readable declarative languages also help the common practitioner to express temporal constraints in a straightforward and accessible language. Notwithstanding the former, this technology is in its infancy, and therefore, few optimization algorithms are known for dealing with massive amounts of information audited from real systems. We, therefore, present four novel algorithms subsuming entire LTLf expressions while outperforming previous state-of-the-art implementations on top of KnoBAB, thus postulating the need for the corresponding, leading to the formulation of novel xtLTLf-derived algebraic operators. © 2024 by the author.",-,10.3390/info15010034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183080509&doi=10.3390%2finfo15010034&partnerID=40&md5=41f9d86df2c1bf8f7418c33cd6fcd265,2021
pimDB: From Main-Memory DBMS to Processing-In-Memory DBMS-Engines on Intelligent Memories,"The performance and scalability of modern data-intensive systems are limited by massive data movement of growing datasets across the whole memory hierarchy to the CPUs. Such traditional processor-centric DBMS architectures are bandwidth- and latency-bound. Processing-in-Memory (PIM) designs seek to overcome these limitations by integrating memory and processing functionality on the same chip. PIM targets near- or in-memory data processing, leveraging the greater in-situ parallelism and bandwidth. In this paper, we introduce pimDB and provide an initial comparison of processor-centric and PIM-DBMS approaches under different aspects, such as scalability and parallelism, cache-awareness, or PIM-specific compute/bandwidth tradeoffs. The evaluation is performed end-to-end on a real PIM hardware system from UPMEM. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",44-52,10.1145/3592980.3595312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163730180&doi=10.1145%2f3592980.3595312&partnerID=40&md5=a384d5e98a97df0d27bfa3f96e372cf6,2023
A design of machine learning-based adaptive signal processing strategy for ECG signal analysis,"The human heart is categorized into four sections such as left and right atrium and left and right ventricle. The development of medical trained systems has increased the necessity for efficient new signal processing approaches to detect irregularities in order to diagnose heart-related disorders. The primary goal of this research is to provide medical treatments for people and hospital management systems. Observing and taking precautions for every human heart is an extremely fundamental part. The early prediction, is crucial for saving and giving the accurate attention to people about diet plans and way of life plans. Additionally, this is utilized to enhance the medical diagnosis and treatment of every affected patient. Here, to identify the heart-based problems Electrocardiogram (ECG) is utilized to analyze the electrical signal of the heart from the human body surface. Therefore, in this article, a novel Catboost-based echo State Network (CbESN) module is proposed to predict the human heart condition. Here, the standard datasets were utilized as implementation namely the PhysioNet 2017 database. Initially, the collected datasets are trained to the system then the preprocessing and feature extraction process take place. After that, classification is performed under different classifications such as normal and abnormal heartbeats to predict heart-based disease. Additionally, the developed CbESN has provided the finest outcomes of framework efficiency. Here, the proposed techniques achieve finest accuracy measure as 99%, precision measure is 99%, sensitivity is 99.1%, specificity is 98.02%, F-1 score is 97.23% and lower error rate as 0.016% for whole ECG signal processing. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11042-024-18990-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188621278&doi=10.1007%2fs11042-024-18990-7&partnerID=40&md5=73a0b2237d86f5536e84e3a8c20e003b,2024
Cross-lingual deep learning model for gender-based emotion detection,"In real-world applications, speech recognition is becoming increasingly popular. Human emotion and automatic gender recognition, which aims to identify male and female voices from any available emotional speech database, is an exciting application. It is noticeable that the performance of the automatic speech emotion and gender recognition system diminishes when cross-corpus circumstances exist, such as when multiple languages are present or a previously unknown language is present, such as Urdu. This study focuses on automatic emotion detection and gender identification from publicly available emotional speech databases. For this work, two public western language databases, namely, RAVDESS (English) and EmoDB (German), are combined for training, and the Urdu database is used for test purposes. The research reported that the k-fold ensemble soft-voting model, deep learning model, and augmented deep learning model obtained 79%, 82%, and 97.6% accuracy, respectively. The results are considerably better than those of many existing systems. The performance evaluation results are also encouraging. Many previous studies on speech emotion recognition have focused on various languages. The proposed technique is sufficiently robust and can efficiently detect emotion and identify gender from the Urdu database. The approach can be used in a wide range of applications. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",25969-26007,10.1007/s11042-023-16304-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168943650&doi=10.1007%2fs11042-023-16304-x&partnerID=40&md5=60d8cd94edb57277c17920b35e120af1,2021
Segmentation of brain MRI using moth-flame optimization with modified cross entropy based fitness function,"The work presents a newly designed penalty function to be added with an existing Cross Entropy based fitness function [3] for optimal selection of multi-level thresholds for image segmentation. The extended fitness function so designed is tested here for Brain magnetic resonance (MR) image segmentation using nature-inspired meta-heuristics such as Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Whale Optimization Algorithm (WOA) and Moth-flame Optimization (MFO), and MFO is finally selected. The proposed method excels in Brain MR image segmentation when tested on WBA database and BrainWeb MR image database with other nature-inspired meta-heuritic based methods. It outperformed the other methods in terms of the three performance metrics Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM) and Feature Similarity Index (FSIM). The best results shown by the proposed method on WBA database in terms of these metrics - PSNR, SSIM and FSIM - are 29.77, 0.927 and 0.899 respectively. On BrainWeb database, the proposed method yields 26.91, 0.864 and 0.892 for the three respective metrics. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11042-024-18461-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185926994&doi=10.1007%2fs11042-024-18461-z&partnerID=40&md5=32e7cd9bdb3f26ba50417d954562b6e2,2023
Polymer expert – A software tool for de novo polymer design,"A versatile and user-friendly “expert system” for de novo polymer design, named Polymer Expert, has been developed and implemented. Polymer Expert can be used to rapidly generate novel candidate polymer repeat units to meet desired performance targets. It is anticipated to accelerate innovation through materials science in industries that use polymers and polymer matrix composites. It was implemented by (1) generating an initial repeat unit database, (2) expanding this initial database into a large analog repeat unit database, (3) performing calculations for all repeat units in the large analog database by using quantitative structure–property relationships (QSPR) of broad applicability, and (4) integrating the resulting searchable library of repeat units and their predicted properties (PEARL, acronym for Polymer Expert Analog Repeat-unit Library) as a new module in a materials modeling and simulation software suite. Its use is illustrated by identifying biobased alternatives for poly(ethylene terephthalate) (PET) and bisphenol-A polycarbonate (BPAPC), alternatives for highly crystalline polypropylene homopolymer (PPHP) and 10% glass fiber containing polypropylene (PP10GF), and polymers that may provide unusually high dielectric constants. Many promising candidates were unobvious and unlikely to have been identified without using a polymer informatics approach. Future work will focus on improving the quality of candidate repeat units by refining the QSPR method, enhancing the diversity of candidate repeat units by expanding PEARL, providing additional interactive search options, and converting Polymer Expert into a versatile R&D platform that users can customize for their own needs. © 2024 The Author(s)",-,10.1016/j.commatsci.2024.112810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185173625&doi=10.1016%2fj.commatsci.2024.112810&partnerID=40&md5=11c9afb0abbb73d60c2eb0df12278b3f,2020
The concept of the Internet of Things in the development of information and analytical systems based on the method of constructing a scalar assessment of the results of research activities of scientists,"The article describes the creation of tools for building intelligent systems in the field of scientometrics as the concept of the Internet of Things. The concept is presented as an information system where scientometric databases such as Scopus, WoS, as well as sites with databases of scientific projects and other databases act as sensors that generate data. Systems of data accumulation and processing are systems created using microservices technology. At the output, we get a control signal in the form of a proposal to correct the trajectory of the development of the scientific potential as an organization or a scientist. The system will also suggest how to strengthen the research team by selecting scientific partners from the global scientific community. The article describes the structural model of the concept of the Internet of Things in the form of a multi-level structure using microservice technologies. Also, the article suggests methods of constructing a scalar evaluation of the results of scientific research activities of scientists based on the analysis of citations of their publications. © 2024 Elsevier B.V.. All rights reserved.",684-690,10.1016/j.procs.2023.12.161,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183915909&doi=10.1016%2fj.procs.2023.12.161&partnerID=40&md5=b6c5f5870a9d6c1d62fdc33c797e7bfa,2024
Databases for Digital Infrastructure of Small Modular Reactors Considering Dynamic Subject Area,"The paper analyses the digital infrastructure (DIS) of small modular reactors (SMRs), their main systems (Instrumentation and Control Systems of SMR, Supervision Information and Control System, System of Exchanging and Processing Information of Crisis Center, System of Physical Security and Protection, System of Radiation Control, UAV Fleet Control System, System of Consumption Management), and requirements to databases as a part of the DIS. It describes methods of forming dynamic subject areas (DSA) considering the specific features of SMR DIS. The results of DSA analysis and suggestions for developing an integrative database for SMR are presented. © 2023 IEEE.",-,10.1109/DESSERT61349.2023.10416500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185833186&doi=10.1109%2fDESSERT61349.2023.10416500&partnerID=40&md5=d70ab0bfa8555e1452f3e277cda13284,2020
Unconstrained Face Recognition Using Infrared Images,"Recently, face recognition (FR) has become an important research topic due to increase in video surveillance. However, the surveillance images may have vague non-frontal faces, especially with the unidentifiable face pose or unconstrained environment such as bad illumination and dark environment. As a result, most FR algorithms would not show good performance when they are applied on these images. On the contrary, it is common at surveillance field that only Single Sample per Person (SSPP) is available for identification. In order to resolve such issues, visible spectrum infrared images were used which can work in entirely dark condition without having any light variations. Furthermore, to effectively improve FR for both the low-quality SSPP and unidentifiable pose problem, an approach to synthesize 3D face modeling and pose variations is proposed in this paper. A 2D frontal face image is used to generate a 3D face model. Then several virtual face test images with different poses are synthesized from this model. A well-known Surveillance Camera's Face (SCface) database is utilized to evaluate the proposed algorithm by using PCA, LDA, KPCA, KFA, RSLDA, LRPP-GRR, deep KNN and DLIB deep learning. The effectiveness of the proposed method is verified through simulations, where increase in average recognition rates up to 10%, 27.69%, 14.62%, 25.38%, 57.46%, 57.43, 37.69% and 63.28%, respectively, for SCface database as observed.  © 2025 World Scientific Publishing Company.",-,10.1142/S0219467825500561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184591302&doi=10.1142%2fS0219467825500561&partnerID=40&md5=745b3813b801a0776ef1568c49234cbe,2021
Exploring Conditions of Image Samples Formation for Person Identification Information Technology,"This paper describes the research the algorithm underlying in the basis of information technology of face recognition and person identification with an aim to improve its performance by exploring the conditions of forming image samples and effect caused by properties of the images containing in the samples on the algorithm efficiency. Researched algorithm is based on Haar features as method of localizing of face area on the image, Gabor wavelets as method of face image processing, 1-dimensional local binary patterns and histogram of oriented gradients as methods of face image feature extraction. During the experimental research several sets of experiments were conducted on face images from several different databases that contain images captured under constrained and unconstrained environmental conditions. After first set of experiments, that were conducted on image samples formed by extracting images with unrecognizable face areas and expanding the etalon samples of images captured under unconstrained conditions, the performance of the algorithm was improved on 7.5-45%. Although the influence of image format and resolution on algorithm performance was explored. As a result of experiments, it was established that format conversion might have an impact on identification accuracy rate by increasing it on 5% after converting images to JPG format. Resolution conversion improved algorithm performance on 5-20% on initial image samples from databases of images captured in constrained and unconstrained conditions and 5-35% on expanded image samples from databases of images captured in unconstrained conditions. After all, it was found that reforming of etalon image samples by expanding it and resolution conversion of the images have the biggest impact on the algorithm performance. As a result, the highest identification accuracy of 95% on the images from the SCface and FERET databases was obtained. ©2023 Copyright for this paper by its authors.",33-42,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187543879&partnerID=40&md5=b50785740e410ace6a06c603f62b1d29,2022
SplitDB: Closing the Performance Gap for LSM-Tree-Based Key-Value Stores,"—Log Structured Merge Tree (LSM tree) serves as the core data storage engine in modern key-value stores. Its adoption is rapidly accelerated with cloud computing and data center development. Acknowledging its widespread use, the LSM tree still faces severe performance issues such as write stall, write amplification, and read inefficiency. This article presents research on improving LSM-tree-based key-value store performance using emerging Non-Volatile Memory (NVM) technology. Our performance diagnosis reveals that the above-mentioned issues result primarily from intensive hot key-value data processing, which is compounded by slow storage devices. To address hotspot bottlenecks, we propose a split log-structured merge tree over hybrid storage by leveraging the intrinsic hot and cold data separation property of the LSM tree. Our approach promotes frequently accessed, small-sized high levels onto fast NVM and offloads the remaining cold, large-sized low levels into slow devices, effectively closing the performance gap for DRAM-disk-based LSM trees. Additionally, we optimize the split LSM tree read and write performance by proposing a variety of novel techniques. We build a hotspot-aware key-value database named SplitDB and perform extensive experiments. Experimental results demonstrate that SplitDB effectively prevents write stalls, achieves a 6-fold write reduction, and improves read throughputs by 3.5 times compared to state-of-the-art key-value databases. © 2023 IEEE.",206-220,10.1109/TC.2023.3326982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176375194&doi=10.1109%2fTC.2023.3326982&partnerID=40&md5=00edb9894d4601d5915131be3e38a504,2022
"Conjunctive Queries with Self-Joins, Towards a Fine-Grained Enumeration Complexity Analysis","Even though query evaluation is a fundamental task in databases, known classifications of conjunctive queries by their fine-grained complexity only apply to queries without self-joins. We study how self-joins affect enumeration complexity, with the aim of building upon the known results to achieve general classifications. We do this by examining the extension of two known dichotomies: one with respect to linear delay, and one with respect to constant delay after linear preprocessing. As this turns out to be an intricate investigation, this paper is structured as an example-driven discussion that initiates this analysis. We show enumeration algorithms that rely on self-joins to efficiently evaluate queries that otherwise (i.e., if the relation names were replaced to eliminate self-joins) cannot be answered with the same guarantees. Due to these additional tractable cases, the hardness proofs are more complex than the self-join-free case. We show how to harness a known tagging technique to prove hardness of queries with self-joins. Our study offers sufficient conditions and necessary conditions for tractability and settles the cases of queries of low arity and queries with cyclic cores. Nevertheless, many cases remain open.  © 2023 ACM.",277-289,10.1145/3584372.3588667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164262369&doi=10.1145%2f3584372.3588667&partnerID=40&md5=efb5ab2ba7c68453e2be2917ad95c7ce,2023
Databases of ligand-binding pockets and protein-ligand interactions,"Many research groups and institutions have created a variety of databases curating experimental and predicted data related to protein-ligand binding. The landscape of available databases is dynamic, with new databases emerging and established databases becoming defunct. Here, we review the current state of databases that contain binding pockets and protein-ligand binding interactions. We have compiled a list of such databases, fifty-three of which are currently available for use. We discuss variation in how binding pockets are defined and summarize pocket-finding methods. We organize the fifty-three databases into subgroups based on goals and contents, and describe standard use cases. We also illustrate that pockets within the same protein are characterized differently across different databases. Finally, we assess critical issues of sustainability, accessibility and redundancy. © 2024 The Author(s)",1320-1338,10.1016/j.csbj.2024.03.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189023746&doi=10.1016%2fj.csbj.2024.03.015&partnerID=40&md5=d37c93b60460c2b919960cb420e01eba,2022
REQUIRED: A Tool to Relax Queries through Relaxed Functional Dependencies,"Query relaxation aims to relax the query constraints in order to derive some approximate results when the answer set is small. In this demo paper, we present REQUIRED, an automatized, portable, and scalable query relaxation tool leveraging metadata learned from an input dataset. The intuition is to use relationships underlying attribute values to derive a new query whose approximate results still meet the user's expectations. In particular, REQUIRED exploits relaxed functional dependencies to modify the original query in two different ways: (i) relaxing some query conditions by replacing the equality constraints with ranges and/or collections of admissible values, and (ii) rewriting the original query by replacing some or all the attributes involved in the conditions of the query with attributes related to them. Our demonstration scenarios show that REQUIRED is effective in properly relaxing queries according to the considered strategy. © 2023 Copyright held by the owner/author(s)",823-826,10.48786/edbt.2023.74,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164995373&doi=10.48786%2fedbt.2023.74&partnerID=40&md5=68945a5ad3c52022215b64d8e7fca9a8,2023
Addressing intra-subject variability in electrocardiogram-based biometric systems through a hybrid architecture,"In an effort to address the critical challenge of intra-subject variability in ECG-based biometric systems, we propose a novel hybrid classification architecture. This architecture, leveraging both fiducial and non-fiducial features, effectively mitigates the impacts of variability and enhances system performance. Our evaluation, conducted on both private databases and the public MIT-BIH Arrhythmia database, showed promising results. In the private databases, an improvement of up to 12 % in the F1-score was achieved in classifying heartbeats from sessions not included in training. Complementary, in the public database, an accuracy of 99.98 % was reached by combining statistical and fiducial features as well as wavelets and fiducial features, with only a single heartbeat required for classification. Hence, the integration of different feature types via the hybrid approach offers a promising solution to handle high cardiac variability in biometric identification systems. © 2023 The Author(s)",-,10.1016/j.bspc.2023.105465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171899215&doi=10.1016%2fj.bspc.2023.105465&partnerID=40&md5=2c0e9ec71c3dee99ec1af7789866f310,2023
Towards Practical Oblivious Join Processing,"In cloud computing, remote accesses over the cloud data inevitably bring the issue of trust. Despite strong encryption schemes, adversaries can still learn sensitive information from encrypted data by observing data access patterns. Oblivious RAMs (ORAMs) are proposed to protect against access pattern attacks. However, directly deploying ORAM constructions in an encrypted database brings large computational overhead. In this work, we focus on oblivious joins over a cloud database. Existing studies in the literature are restricted to either primary-foreign key joins or binary equi-joins. Our major contribution is to support general band joins and multiway equi-joins. For oblivious join without ORAMs, we extend the existing binary equi-join algorithm to support general band joins obliviously. For oblivious join with ORAMs, we integrate BB-tree indices into ORAMs for each input table and retrieve blocks through the indices in join processing. The key point is to avoid retrieving tuples that make no contribution to the final join result and bound the number of accesses to each BB-tree index. The effectiveness and efficiency of our algorithms are demonstrated through extensive evaluations over real-world datasets. Our method shows orders of magnitude speedup for oblivious multiway equi-joins in comparison with baseline algorithms. © 1989-2012 IEEE.",1829-1842,10.1109/TKDE.2023.3310038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170535645&doi=10.1109%2fTKDE.2023.3310038&partnerID=40&md5=95904d0c4ceaeac35a6eedc9e370645b,2022
Сlassifiсatiоn and deteсtiоn оf Соvid-19 based оn X-Raу and СT images using deeр learning and maсhine learning teсhniques: A bibliоmetriс analуsis,"Abstraсt: During the COVID-19 pandemic, it was crucial for the healthcare sector to detect and classify the virus using X-ray and CT scans. This has underlined the need for advanced Deep Learning and Machine Learning approaches to effectively spot and manage the virus's spread. Indeed, researchers worldwide have dynamically participated in the field by publishing an important number of papers across various databases. In this context, we present a bibliometric analysis focused on the detection and classification of COVID-19 using Deep Learning and Machine Learning techniques, based on X-Ray and CT images. We analyzed published documents of the six prominent databases (IEEE Xplore, ACM, MDPI, PubMed, Springer, and ScienceDirect) during the period between 2019 and November 2023. Our results showed that rising forces in economy and technology, especially India, China, Turkey, and Pakistan, began to compete with the great powers in the field of scientific research, which could be seen from their number of publications. Moreover, researchers contributed to Deep Learning techniques more than the use of Machine Learning techniques or the use of both together and preferred to submit their works to Springer Database. An important result was that more than 57% documents were published as Journal Articles, which was an important portion compared to other publication types (conference papers and book chapters). Moreover, the PubMed journal “Multimedia Tools and Applications” tops the list of journals with a total of 29 published articles. © 2024 the Author(s), licensee AIMS Press.",71-103,10.3934/electreng.2024004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187211679&doi=10.3934%2felectreng.2024004&partnerID=40&md5=eda662b8ca90f8bd30032e9685fb301c,2024
Gap-Closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement,"There is a growing consensus in the research community that the optimization of low-light image enhancement approaches should be guided by the visual quality perceived by end users. Despite the substantial efforts invested in the design of low-light enhancement algorithms, there has been comparatively limited focus on assessing subjective and objective quality systematically. To mitigate this gap and provide a clear path towards optimizing low-light image enhancement for better visual quality, we propose a gap-closing framework. In particular, our gap-closing framework starts with the creation of a large-scale dataset for Subjective QUality Assessment of REconstructed LOw-Light Images (SQUARE-LOL). This database serves as the foundation for studying the quality of enhanced images and conducting a comprehensive subjective user study. Subsequently, we propose an objective quality assessment measure that plays a critical role in bridging the gap between visual quality and enhancement. Finally, we demonstrate that our proposed objective quality measure can be incorporated into the process of optimizing the learning of the enhancement model toward perceptual optimality. We validate the effectiveness of our proposed framework through both the accuracy of quality prediction and the perceptual quality of image enhancement. © 1999-2012 IEEE.",3430-3443,10.1109/TMM.2023.3312851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171563776&doi=10.1109%2fTMM.2023.3312851&partnerID=40&md5=aed37c5c2c954f9698504a48c271c9b1,2020
AFC: An adaptive lossless floating-point compression algorithm in time series database,"The time series database is a specialized type of database specifically designed for storing and analyzing time series data. Compression of time series data is crucial for its performance. However, efficiently compressing time series data, particularly floating-point data, remains challenging. Existing compression algorithms are efficient for only a limited range of data patterns, indicating a lack of self-adaptation. In this paper, we propose an effective and Adaptive lossless Floating-point Compression algorithm AFC for time series databases. We devise four unique compression strategies, and based on the data patterns, AFC dynamically selects the appropriate strategy. These strategies handle data compression for diverse data patterns, enhancing the compression ratio and efficiency. The most suitable strategy is employed to achieve an optimal compression ratio. We compared our AFC algorithm with four state-of-the-art compression algorithms, namely Gorilla, FPC, TSXor, and Chimp, as well as various general-purpose compression algorithms such as LZ4 and Snappy. Experimental results demonstrate that our algorithm achieves an improvement of at least 20% in compression ratio and even up to 100% on certain datasets. © 2023",-,10.1016/j.ins.2023.119847,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176266917&doi=10.1016%2fj.ins.2023.119847&partnerID=40&md5=c7aba2a3ba7c16f5dc72e39ed1215305,2021
Convolutional Features-Based Broad Learning With LSTM for Multidimensional Facial Emotion Recognition in Human-Robot Interaction,"Convolutional feature-based broad learning with long short-term memory (CBLSTM) is proposed to recognize multidimensional facial emotions in human-robot interaction. The CBLSTM model consists of convolution and pooling layers, broad learning (BL), and long- and short-term memory network. It aims to obtain the depth, width, and time scale information of facial emotion through three parts of the model, so as to realize multidimensional facial emotion recognition. CBLSTM adopts the structure of BL after processing was done at the convolution and pooling layer to replace the original random mapping method and extract features with more representation ability, which significantly reduces the computational time of the facial emotion recognition network. Moreover, we adopted incremental learning, which can quickly reconstruct the model without a complete retraining process. Experiments on three databases are developed, including CK+, MMI, and SFEW2.0 databases. The experimental results show that the proposed CBLSTM model using multidimensional information produces higher recognition accuracy than that without time scale information. It is 1.30% higher on the CK+ database and 1.06% higher on the MMI database. The computation time is 9.065 s, which is significantly shorter than the time reported for the convolutional neural network (CNN). In addition, the proposed method obtains improvement compared to the state-of-the-art methods. It improves the recognition rate by 3.97%, 1.77%, and 0.17% compared to that of CNN-SIPS, HOG-TOP, and CMACNN in the CK+ database, 5.17%, 5.14%, and 3.56% compared to TLMOS, ALAW, and DAUGN in the MMI database, and 7.08% and 2.98% compared to CNNVA and QCNN in the SFEW2.0 database.  © 2013 IEEE.",64-75,10.1109/TSMC.2023.3301001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168681758&doi=10.1109%2fTSMC.2023.3301001&partnerID=40&md5=2a06f3ff432a24487ba83759bc992b98,2021
Modeling and Performance Analysis of Multi-Server Cloud Database Over Quasi-static Rayleigh Fading Channel,"With the development of communication in the post-5G era, the combination of communication and cloud computing becomes closer. In order to promote the further development of cloud-network, this paper will study the performance of Multi-server cloud Database under the Communication quality of quasi-static Rayleigh fading channel with Multiple antennas(MC-MD). The CLIENTS with unlimited customers, a COMMUNICATION SYSTEM subject to quasi-static Rayleigh fading, and CLOUD DATABASE with two-phase locking protocol are the three components of the MC-MD model. Transactions are 1)initiated by the CLIENTS, 2)transmitted to the CLOUD DATABASE through the COMMUNICATION SYSTEM for processing, 3)then returned to the CLIENTS. The indicators of the model is mathematically derived by using queuing theory. These include client&#x2019;s indicators(average concurrent quantity of the system in steady state(CQ), average transactions stay time of the system in steady state(ST), average queue length of the Waiting Area in steady state(QL), and average transactions wait time of the Waiting Area in steady state(WT)) and server&#x2019;s indicator(average number of service desks in the busy period at steady state(DN)). Under the appropriate conditions, the results indicate that the theoretical value of service performance is basically consistent with the simulation value. Clearly, the high speed improves the service performance of the system and decreases the service pressure. On the basis of this, the optimization strategy is proposed and the simulation indicators Jitter of transactions sojourn time of the system in steady state(STJ) is added. The results show that the transaction scheduling optimization strategy effectively reduce the delay and its jitter. IEEE",1-1,10.1109/JIOT.2024.3373783,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176941353&doi=10.1109%2fJIOT.2024.3373783&partnerID=40&md5=665ba4b382dc6f0b9f38aeb691363ed5,2021
Inflammatory response signature score model for predicting immunotherapy response and pan-cancer prognosis,"Background: Inflammatory responses influence the outcome of immunotherapy and tumorigenesis by modulating host immunity. However, systematic inflammatory response assessment models for predicting cancer immunotherapy (CIT) responses and survival across human cancers remain unexplored. Here, we investigated an inflammatory response score model to predict CIT responses and patient survival in a pan-cancer analysis. Methods: We retrieved 12 CIT response gene expression datasets from the Gene Expression Omnibus database (GSE78220, GSE19423, GSE100797, GSE126044, GSE35640, GSE67501, GSE115821 and GSE168204), Tumor Immune Dysfunction and Exclusion database (PRJEB23709, PRJEB25780 and phs000452.v2.p1), European Genome-phenome Archive database (EGAD00001005738), and IMvigor210 cohort. The tumor samples from six cancers types: metastatic urothelial cancer, metastatic melanoma, gastric cancer, primary bladder cancer, renal cell carcinoma, and non-small cell lung cancer. We further established a binary classification model to predict CIT responses using the least absolute shrinkage and selection operator (LASSO) computational algorithm. Findings: The model had high predictive accuracy in both the training and validation cohorts. During sub-group analysis, area under the curve (AUC) values of 0.82, 0.80, 0.71, 0.7, 0.67, and 0.64 were obtained for the non-small cell lung cancer, gastric cancer, metastatic urothelial cancer, primary bladder cancer, metastatic melanoma, and renal cell carcinoma cohorts, respectively. CIT response rates were higher in the high-scoring training cohort subjects (51%) than the low-scoring subjects (27%). The five-year survival rates in the high- and low score groups of the training cohorts were 62% and 21%, respectively, while those of the validation cohorts were 54% and 22%, respectively (P < 0·001 in all cases). Inflammatory response signature score derived from on-treatment tumor specimens are highly predictive of response to CIT in patients with metastatic melanoma. A significant correlation was observed between the inflammatory response scores and tumor purity. Regardless of the tumor purity, patients in the low score group had a significantly poorer prognosis than those in the high score group. Immune cell infiltration analysis indicated that in the high score cohort, tumor-infiltrating lymphocytes were significantly enriched, particularly effector and natural killer cells. Inflammatory response scores were positively correlated with immune checkpoint genes, suggesting that immune checkpoint inhibitors may have benefited patients with high scores. Analysis of signature scores across different cancer types from The Cancer Genome Atlas revealed that the prognostic performance of inflammatory response scores for survival in patients who have not undergone immunotherapy can be affected by tumor purity. Interleukin 21 (IL21) had the highest weight in the inflammatory response model, suggesting its vital role in the prediction mode. Since the number of metastatic melanoma patients (n = 429) was relatively large among CIT cohorts, we further performed a co-culture experiment using a melanoma cell line and CD8 + T cell populations generated from peripheral blood monocytes. The results showed that IL21 therapy combined with anti-PD1 (programmed cell death 1) antibodies (trepril monoclonal antibodies) significantly enhanced the cytotoxic activity of CD8 + T cells against the melanoma cell line. Conclusion: In this study, we developed an inflammatory response gene signature model that predicts patient survival and immunotherapy response in multiple malignancies. We further found that the predictive performance in the non-small cell lung cancer and gastric cancer group had the highest value among the six different malignancy subgroups. When compared with existing signatures, the inflammatory response gene signature scores for on-treatment samples were more robust predictors of the response to CIT in metastatic melanoma. © 2023 The Authors",369-383,10.1016/j.csbj.2023.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180997101&doi=10.1016%2fj.csbj.2023.12.001&partnerID=40&md5=d211bdcc879bd3bba4a3f119a0186533,2023
Cross-Domain Authentication Scheme for Vehicles Based On Given Virtual Identities,"The advancement of intelligent transportation systems has enhanced both vehicle ad hoc networks (VANETs) and road safety. However, traditional cross-domain scenarios in VANETs face challenges such as the computational burden of identity and message authentication, as well as privacy breaches. In this study, to mitigate the issues surrounding communication security and the significant computational overhead within traditional cross-domain scenarios in VANETs, we propose a certificate-based cross-domain authentication scheme specifically tailored for VANETs. Moreover, considering the inter-domain vehicle authentication challenges, the scheme introduces an efficient batch verification mechanism suitable for dynamic multi-vehicle cross-domain scenarios. To mitigate the potential single point of failure, a two-way synchronization database mechanism is presented, ensuring uninterrupted operations in case of primary database failure. Moreover, privacy protection for vehicles is enhanced through the use of virtual identities. Through rigorous security proofs and detailed security analyses, we demonstrate that the scheme meets the security requirements of vehicular networks and can resist more security attacks. Moreover, performance analysis highlights its superiority over related advanced schemes in cross-domain VANET scenarios. Through performance evaluation using the JPBC library and comparison with relevant schemes, the proposed solution demonstrated superior results in terms of communication and computational overhead. IEEE",1-1,10.1109/JIOT.2024.3352016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183974898&doi=10.1109%2fJIOT.2024.3352016&partnerID=40&md5=db3f91f91bade6199955701df72b6cc3,2021
Impact of Geometry and Attribute Distortion in Subjective Quality of Point Clouds for G-PCC,"To better understand the effect of geometry and attribute distortion on the subjective quality of point clouds, we built a database with subjective quality scores for compressed point clouds. We analyzed the impact of geometry and attribute distortion to subjective quality. Our database, named the NWPU Point Cloud Quality Database (NWPU-PCQD), contains 817 point clouds with various geometry and attribute quantification distortions, which were subjectively evaluated using head-mounted displays (HMDs). Through correlation and significance analysis of the subjective quality scores, we observe that geometry and attribute quantization do not equally contribute to the overall subjective quality, where the effect of geometry quantization dominates. Moreover, there is an interaction between the geometry and attribute distortion. Our subjective database and findings are useful for point cloud processing, transmission, and compression. © 2023 IEEE.",-,10.1109/VCIP59821.2023.10402708,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184856146&doi=10.1109%2fVCIP59821.2023.10402708&partnerID=40&md5=00ad6ffb4ec12112c70a6cf9f6a3cc51,2022
Ferric citrate's efficacy for hyperphosphatemia and anemia in NDDCKD: A meta-Analysis,"This meta-Analysis aims to evaluate the impact of ferric citrate (FC) treatment on hyperphosphatemia and iron-deficiency anemia (IDA) in non-dialysis-dependent chronic kidney disease (NDD-CKD) patients. A comprehensive search was conducted across multiple databases, namely PubMed, Embase, Cochrane, EBSCO, Scopus, Web of Science, Wanfang Data, CNKI, and VIP databases. The purpose of this search was to identify randomized controlled trials (RCTs) pertaining to the management of hyperphosphatemia and anemia in NDD-CKD patients. The search encompassed the entire duration of the databases, up until June 2023. RevMan 5.4.0 and Stata 16.0 were used for data analysis. A total of 8 studies involving 1281 NDD-CKD patients were included in this meta-Analysis. In comparison to the control group, FC treatment led to a notable decrease in serum phosphorus levels. Additionally, FC treatment improved iron, hemoglobin, ferritin, and transferrin saturation (TSAT) levels. Findings of this meta-Analysis suggest that FC has positive effects in managing hyperphosphatemia and IDA in NDD-CKD patients.  © 2023 SPIE.",-,10.1117/12.3021828,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183005376&doi=10.1117%2f12.3021828&partnerID=40&md5=3d85be7877caeacfa706d91a1f80189c,2023
Communication-Efficient Inner Product Private Join and Compute with Cardinality,"Private join and compute (PJC) is a paradigm where two parties owing their private database securely join their databases and compute a function over the combined database. Inner product PJC, introduced by Lepoint et al. (Asiacrypt'21), is a class of PJC that has a wide range of applications such as secure analysis of advertising campaigns. In this computation, two parties, each of which has a set of identifier-value pairs, compute the inner product of the values after the (inner) join of their databases with respect to the identifiers. They proposed inner product PJC protocols that are specialized for the unbalanced setting where the input sizes of both parties are significantly different and not suitable for the balanced setting where the sizes of two inputs are relatively close. We propose an inner product PJC protocol that is much more efficient than that by Lepoint et al. for balanced inputs in the setting where both parties are allowed to learn the intersection size additionally. Our protocol can be seen as an extension of the private intersection-sum protocol based on the decisional Diffie-Hellman assumption by Ion et al. (EuroS&P'20) and is especially communication-efficient as the private intersection-sum protocol. In the case where both input sizes are 216, the communication cost of our inner-product PJC protocol is 46 × less than that of the inner product PJC protocol by Lepoint et al. © 2023 ACM.",678-688,10.1145/3579856.3582826,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168107472&doi=10.1145%2f3579856.3582826&partnerID=40&md5=d8018ffc22232590cd6a96e1dcaedde6,2024
Predicting condensate formation of protein and RNA under various environmental conditions,"Background: Liquid–liquid phase separation (LLPS) by biomolecules plays a central role in various biological phenomena and has garnered significant attention. The behavior of LLPS is strongly influenced by the characteristics of RNAs and environmental factors such as pH and temperature, as well as the properties of proteins. Recently, several databases recording LLPS-related biomolecules have been established, and prediction models of LLPS-related phenomena have been explored using these databases. However, a prediction model that concurrently considers proteins, RNAs, and experimental conditions has not been developed due to the limited information available from individual experiments in public databases. Results: To address this challenge, we have constructed a new dataset, RNAPSEC, which serves each experiment as a data point. This dataset was accomplished by manually collecting data from public literature. Utilizing RNAPSEC, we developed two prediction models that consider a protein, RNA, and experimental conditions. The first model can predict the LLPS behavior of a protein and RNA under given experimental conditions. The second model can predict the required conditions for a given protein and RNA to undergo LLPS. Conclusions: RNAPSEC and these prediction models are expected to accelerate our understanding of the roles of proteins, RNAs, and environmental factors in LLPS. © The Author(s) 2024.",-,10.1186/s12859-024-05764-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189146970&doi=10.1186%2fs12859-024-05764-z&partnerID=40&md5=c86cba87debce434ead622ab426f600d,2020
Towards constrained optimization of cloud applications: A hybrid approach,"A database system is one of the most common cloud applications where elasticity allows to dynamically allocate resources in response to changing workload demands. In such systems, users usually configure resources based on empirical decisions, for example by boosting computational resources with the expectation of improving database throughput. The latter also suggests that users manually scale resources and tune the database configurations offline to meet the application demands. However, this approach could rapidly increase infrastructure expenditures while this is also time consuming. In this paper, we propose SONA, a framework to support constrained performance optimization of cloud applications using a hybrid approach of artificial neural networks and genetic algorithms. The proposed framework monitors the source system to identify the optimal configurations that maximize application performance based on genuine workload executions. As a result, the experimental analysis presents a novel dataset collected from TPC-C runs on MySQL server. The optimization process is being constrained to satisfy user and application requirements including the cloud infrastructure expenditures, the cost-performance ratio, the baseline performance and the average percentage of idle CPU resources. Furthermore, SONA uses a cloned containerized environment that replicates the main application to avoid system overhead during the optimization process. Our results demonstrate the effectiveness of SONA framework to optimize the performance of OLTP applications deployed on cloud. © 2023 The Authors",100-110,10.1016/j.future.2023.09.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173614368&doi=10.1016%2fj.future.2023.09.024&partnerID=40&md5=ffd3e1b2a2003be9211914b44b355d5e,2023
Data Mining of Ambulatory Blood Pressure Monitoring,"The paper's aim is data mining of the recently published (2017) Ambulatory Blood Pressure Monitoring database, which covers 270 patients. Preprocessing, denoising, unsupervised Machine Learning (clustering), and feature selection were performed within Java-based software (WEKA 3-9-6). The Blood Pressure Load Level showed the best congruence with the obtained clusters. It also has the lowest noise among other levels of the database. The methods are designed to simplify database mining for clinicians largely unfamiliar with machine learning algorithms and benefit from simple algorithm approaches and visual aids output. © 2023 IEEE.",-,10.1109/DESSERT61349.2023.10416531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185836998&doi=10.1109%2fDESSERT61349.2023.10416531&partnerID=40&md5=4393db4172dc6f7c80a5d6f311517c50,2020
Sampling Big Ideas in Query Optimization,The use of random sampling can greatly enhance the scalability of complex data analysis tasks. Samples serve as concise representations or versatile summaries that can be applied directly or integrated as a component in the data analysis process. We survey some of the author's favorite big ideas in the design and applications of weighted and coordinated sampling schemes. We emphasize algorithmic simplicity and practicality and the context of streaming or distributed data.  © 2023 ACM.,361-371,10.1145/3584372.3589935,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164260924&doi=10.1145%2f3584372.3589935&partnerID=40&md5=791aca974f23d02ece6c54acac387751,2023
Adapting the Database to Feature Changes in Software Product Lines,"Software Product Lines (SPL) support the development of families of software products that share a set of core assets but differ in certain features. To generate a new product, the engineer selects the desired features and the SPL assembles and adapts the implementation of the core assets. In real scenarios, we may need to update a product by adding a feature not initially selected. Similarly, we may need to remove a feature that is no longer necessary. Modifying the selection of features of a product in use poses a challenge from the point of view of the product's database. If the added/removed features affect the database schema, we may need to adapt the schema and the data stored in the database. This paper addresses this scenario and proposes an evolution model to define actions to be executed in the database when features are added or removed. Our proposal allows us to model those adaptations and to automate them when modifying the selection of features of a product. The evolution model describes changes to be made in the database, each composed of different actions that adapt certain elements of the database. Changes are associated with the features that may trigger their execution, and the change's actions are associated with the data model elements they affect. In this way, the evolution model supports automatic adaptation of the database, and we keep traceability between features and the elements of the data model they affect.  © 2023 ACM.",194-200,10.1145/3579027.3608990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176013364&doi=10.1145%2f3579027.3608990&partnerID=40&md5=ca72c2b1c8f399d332807170e46a190e,2023
From Organic Fragments to Photoswitchable Catalysts: The OFF-ON Structural Repository for Transferable Kernel-Based Potentials,"Structurally and conformationally diverse databases are needed to train accurate neural networks or kernel-based potentials capable of exploring the complex free energy landscape of flexible functional organic molecules. Curating such databases for species beyond “simple” drug-like compounds or molecules composed of well-defined building blocks (e.g., peptides) is challenging as it requires thorough chemical space mapping and evaluation of both chemical and conformational diversities. Here, we introduce the OFF-ON (organic fragments from organocatalysts that are non-modular) database, a repository of 7869 equilibrium and 67,457 nonequilibrium geometries of organic compounds and dimers aimed at describing conformationally flexible functional organic molecules, with an emphasis on photoswitchable organocatalysts. The relevance of this database is then demonstrated by training a local kernel regression model on a low-cost semiempirical baseline and comparing it with a PBE0-D3 reference for several known catalysts, notably the free energy surfaces of exemplary photoswitchable organocatalysts. Our results demonstrate that the OFF-ON data set offers reliable predictions for simulating the conformational behavior of virtually any (photoswitchable) organocatalyst or organic compound composed of H, C, N, O, F, and S atoms, thereby opening a computationally feasible route to explore complex free energy surfaces in order to rationalize and predict catalytic behavior. © 2024 The Authors. Published by American Chemical Society.",1201-1212,10.1021/acs.jcim.3c01953,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186159586&doi=10.1021%2facs.jcim.3c01953&partnerID=40&md5=59573838d489f03885a95463d6ee0384,2023
Evaluating the Inclusion of Images with Artifacts in Medical Image Databases of Mammography for Machine Learning,"The use of the federated learning technique has grown in the medical field of diagnostic imaging, with a large number of publications and applications in clinical practice. However, the lack of knowledge of the training data by the central server and the clients among themselves raises concerns about the local composition of the databases, raising questions about the occurrence of image artifacts and their impact on the final performance of the global model. Using databases with a selection of areas of interest for mammograms distributed in an unbalanced way to 03 clients, simulations of mammography image artifacts were proceeded, with their insertion in the databases on different scenarios. An open access convolutional neural network model (EfficientNetB7) was adapted to classify the images regarding the presence and suspicion of microcalcifications according to the BI-RADS system. By measuring the F1-score, the performance of the neural network was verified without the presence of artifacts, after the insertion of artifacts and after the exclusion of images with artifacts. The results showed an improvement in the performance of the central model when opting to keep the images with artifact in the training databases. © 2023 IEEE.",-,10.1109/LA-CCI58595.2023.10409478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185219730&doi=10.1109%2fLA-CCI58595.2023.10409478&partnerID=40&md5=3a20956252415ea6302c154695e2940c,2023
Identification of Different Arabic Dialects Using Randomly Multimodal Deep Learning (RMDL) Approach on AOC Database,"Like other languages, the Arabic dialect has both spoken and written forms. The first form is the true mother tongue of Arabic speakers explored on a daily basis. As for the second form, it represents the dialect of Modern Standard Arabic (MSA) which mainly constitutes the content of most Arabic databases due to its predominance in written form. However, the large size of Arabic text databases requires robust and accurate machine learning methods. Random Multimodal Deep Learning (R MDL) is one such deep learning approach that is explored in this article to address the issue of finding the best deep learning architecture and structure while simultaneously maintaining improvement accuracy and robustness for the classification and identification of Arabic dialects (MSA, Gulf (GLF), Egypt (EGY) and Levantine (Lev)) via a set of deep learning architectures. Experimental tests were carried out on the Arabic Online Commentary (AOC) database and showed good results in terms of performance evaluated using the RMDL approach compared to those obtained with other deep learning algorithms. © 2023 IEEE.",360-365,10.1109/ICSC58660.2023.10449764,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187789232&doi=10.1109%2fICSC58660.2023.10449764&partnerID=40&md5=595a982f58cff5453b3fe437fe4a83bf,2022
Generalized Face Spoofing Detection Using Spatial Features,"Authenticating a person using biometric traits like fingerprint, iris, and face is widely adopted in security applications. Compared to other authentication procedures biometrics provides high accuracy. Face biometrics possess high precision and low intrusiveness. However, face recognition-based security systems are vulnerable to face spoofing attacks. Using printed photo or video replay intruders attempt to break the recognition system. Face spoof detection systems classify real and fake users by analyzing the features. Various spoof detection techniques have evolved based on texture and quality features. However, these methods suffer from generalization ability and variations in textures. So, in this research work, a face spoofing detection model is presented that utilizes spatial and frequency features to detect real and fake users. Feature descriptors like Histogram of Gradient (HOG) and Gray-Level Co-occurrence Matrix (GLCM) are used to obtain the essential features and extracted features are processed through a random forest classifier module. Extensive experiments on benchmark datasets namely the Reply-attack database and MSU-Mobile face spoof database demonstrate the better detection performance of the proposed spoof detection model compared to state of art of techniques. Cross-database evaluation of the proposed approach possesses minimum error indicates the generalization ability of the proposed face spoof detection model.  © 2023 IEEE.",1001-1007,10.1109/ICECA58529.2023.10395674,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186113758&doi=10.1109%2fICECA58529.2023.10395674&partnerID=40&md5=2d10d9bfab703f86a9ba436262bc6ba5,2023
Network Pharmacology and Molecular Docking Approach to Investigate Multitarget Mechanisms of Active Components From Tinosporacrispa Against Obesity,"Traditional plant medicines are an integral part of the discovery of modern-day medicines. Phytochemicals from leaves of Tinosporacrispa are regarded as a good source of organic anti-obesity compounds. However, neither the actual bioactive compound(s) in it nor the mechanism(s) by which they prevent obesity have been established. Consequently, network pharmacology approach was carried out to determine the mechanism(s) of action of major phyto constituents against obesity targets. Twenty-six phytochemicals reported in the plant were obtained from various databases like the Traditional Chinese Medicine Database platform (TCMSP) and Therapeutic Target Database (TTD). This study reveals the effect of these phytochemicals and illuminates their possible mechanism on various genes related to obesity. Disease-based databases were used to extract anti-obesity targets. A protein-protein interaction (PPI) network was constructed to demonstrate the interaction between the gene products. The PPI network was obtained using the string database and the edge, degree, and node details were collected. The network was visualized and formatted using Cytoscape. The molecular docking analysis was performed using PyRx software and the in-silico pharmacokinetic study was performed using various web tools. The results of the network pharmacology study indicated that EGFR was the main gene involved in the regulation of the antiobesity pathway. The molecular docking study was conducted for the selected compounds interacting with the EGFR gene. The highest docking score of -9.8 kcal/mol was exhibited by the compound N-Acetylanonaine with good pharmacokinetic properties. N-acetylanonaine can further act as lead molecules for development of potential anti-obesity agents. © 2023 IEEE.",-,10.1109/ICAIHC59020.2023.10430485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186760475&doi=10.1109%2fICAIHC59020.2023.10430485&partnerID=40&md5=1c5ecd8ca64d3c33d64fea2fcc59fbe4,2022
"BongFloralpedia: A comprehensive collection of diverse types of flowers growing in Ranaghat, West Bengal","The automated identification of diverse floral species with high inter-class and intra-class similarities is crucial for monitoring the health of flowers in the floriculture industry which largely contributes to the revenue earned by agriculture industry of any country. In order to train the deep neural networks for this purpose, creation of novel databases comprising of multiple images belonging to diverse floral species is required to enable the networks to get acquainted with the varsities in their characteristics. Keeping the significance of databases in this research area into consideration, here we have designed a novel database namely, BongFloralpedia comprising of a total of 1920 flower images belonging to nine diverse types of species namely, Tuberose, Marigold, Ixora, Hibiscus, Crape Jasmine, Datura, Bougainvillea, Four o' clock and Rose. The floral images included in this database are captured from real-fields located in Ranaghat, one of the most well-known flower cultivating regions of West Bengal which serves as the main inspiration behind naming the newly created database as 'BongFloralpedia'. The images incorporated in this database possess complex backgrounds and are captured using different camera angles, under varied illumination conditions and thus depict the true characteristics of the considered species essential for reliable network training. The effectiveness of BongFloralpedia is validated by performing classification using several deep neural networks like AlexNet, Vgg16, ResNet152, MobileNetv2, ShuffleNetv2, SqueezeNet, InceptionV3, InceptionResNetv2, DenseNet121, NasNetLarge, NasNetMobile, GhostNetv2, EfficientNetB7 and Xception. Out of all these networks, DenseNet121 has given the best results by registering an overall Test Accuracy of 99.0 % and giving impressive outputs for quantitative parameters like Recall, Precision and F1 score.  © 2023 IEEE.",-,10.1109/IEMENTech60402.2023.10423464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186267039&doi=10.1109%2fIEMENTech60402.2023.10423464&partnerID=40&md5=a80a15ff15dd30d8c7d139f5ce1042e9,2023
Integrating Heterogeneous Modalities for Comprehensive Facial Analysis: The Heteroface Database,"Heterogeneous Face Recognition (HFR) is gradually gaining importance in various domains including biometrics and cognitive studies. Classical methods are being replaced by deep learning techniques as a means to handle varied face modalities. The paper highlights the importance of HFR and introduces a face database that incorporates diversities in spectra, illumination, and formats (viz. photograph-sketch, longitudinal, 2D-3D). The role of deep learning methods has been discussed, and its advantages over the limitations of traditional methods have been emphasized. A comprehensive real-world HFR database, such as the one presented in the paper, will likely aid algorithm development, enriched by a neural network-based curation technique that enhances diversity by excluding similar instances. The paper underscores the role of deep learning techniques in tackling the challenges in the field of HFR and presents the database as a significant contribution to advances in HFR research and application.  © 2023 IEEE.",-,10.1109/ICEFEET59656.2023.10452246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188234866&doi=10.1109%2fICEFEET59656.2023.10452246&partnerID=40&md5=12f54d564e7378bae27453857c492425,2023
Dynamic Planar Embedding Is in DynFO,"Planar Embedding is a drawing of a graph on the plane such that the edges do not intersect each other except at the vertices. We know that testing the planarity of a graph and computing its embedding (if it exists), can efficiently be computed, both sequentially [24] and in parallel [33], when the entire graph is presented as input. In the dynamic setting, the input graph changes one edge at a time through insertion and deletions and planarity testing/embedding has to be updated after every change. By storing auxilliary information we can improve the complexity of dynamic planarity testing/embedding over the obvious recomputation from scratch. In the sequential dynamic setting, there has been a series of works [17, 25, 20, 22], culminating in the breakthrough result of polylog(n) sequential time (amortized) planarity testing algorithm of Holm and Rotenberg [21]. In this paper we study planar embedding through the lens of DynFO, a parallel dynamic complexity class introduced by Patnaik et al [31] (also [16]). We show that it is possible to dynamically maintain whether an edge can be inserted to a planar graph without causing non-planarity in DynFO. We extend this to show how to maintain an embedding of a planar graph under both edge insertions and deletions, while rejecting edge insertions that violate planarity. Our main idea is to maintain embeddings of only the triconnected components and a special two-colouring of separating pairs that enables us to side-step cascading flips when embedding of a biconnected planar graph changes, a major issue for sequential dynamic algorithms [22, 21]. © Samir Datta, Asif Khan, and Anish Mukherjee;",-,10.4230/LIPIcs.MFCS.2023.39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171423441&doi=10.4230%2fLIPIcs.MFCS.2023.39&partnerID=40&md5=912481cce38b5ebb363e130cff7eada2,2021
Ethnicity and Biometric Uniqueness: Iris Pattern Individuality in a West African Database,"We conducted more than 1.3 million comparisons of iris patterns encoded from images collected at two Nigerian universities, which constitute the newly available African Human Iris (AFHIRIS) database. The purpose was to discover whether ethnic differences in iris structure and appearance such as the textural feature size, as contrasted with an all-Chinese image database or an American database in which only 1.53% were of African-American heritage, made a material difference for iris discrimination. We measured a reduction in entropy for the AFHIRIS database due to the coarser iris features created by the thick anterior layer of melanocytes, and we found stochastic parameters that accurately model the relevant empirical distributions. Quantile-Quantile analysis revealed that a very small change in operational decision thresholds for the African database would compensate for the reduced entropy and generate the same performance in terms of resistance to False Matches. We conclude that despite demographic difference, individuality can be robustly discerned by comparison of iris patterns in this West African population.  © 2019 IEEE.",79-86,10.1109/TBIOM.2023.3327287,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169006586&doi=10.1109%2fTBIOM.2023.3327287&partnerID=40&md5=aae25d1bf0ae5306cb3ae26f01cf2996,2024
IME: Efficient list-based method for incremental mining of maximal erasable patterns,"Erasable pattern mining can help factories facing a financial crisis increase productivity by identifying and eliminating unprofitable products. The Flag-GenMax-EI algorithm extracts Maximal Erasable Itemsets (MEIs); however, it does not support dynamic data. In practice, many applications create databases incrementally. Using the Flag-GenMax-EI algorithm to mine maximal erasable patterns from incremental databases is clearly very costly because it must be run each time. In this paper, an efficient method called IME is proposed for incremental mining of maximal erasable patterns. IMEI-List and IMEP-List are two new data structures introduced by the proposed method. These lists allow the algorithm to update all tree nodes without rescanning the updated database (original database + new database) and recreating the nodes. This is the first study of incremental mining of maximal erasable patterns. Extensive experimental results on dense and sparse incremental data show that the proposed algorithm improves scalability. It extracts MEIs much faster than the Flag-GenMax-EI algorithm in different modes of database update. © 2023 Elsevier Ltd",-,10.1016/j.patcog.2023.110166,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179126761&doi=10.1016%2fj.patcog.2023.110166&partnerID=40&md5=e0a98a825ff68d5b3635d030c315a1a0,2021
Human Locomotion Databases: A Systematic Review,"The analysis of human locomotion is highly dependent on the quantity and quality of available data to obtain reliable evidence, due to the great variability of gait characteristics between subjects. Researchers usually have to make significant efforts to generate well-structured and trustworthy datasets. This situation is aggravated when patients are involved, due to experimental, privacy, and safety constraints. The availability of public datasets can facilitate this process. In this work, we systematically review the scientific and technical literature to identify the human locomotion databases publicly available nowadays. Within the 93 datasets identified, we observed that the most basic motor skills, e.g., flat or sloped walking, are well covered, whereas many other daily-life motor skills are poorly represented. The most common sensors used to record gait are optical motion capture systems, followed by RGB cameras and inertial sensors. We observed a lack of consistency in the data formats and limited sample size in most reviewed datasets. These issues hinder researchers from systematically standing on previous research results and represent a major barrier to using Artificial Intelligence and Big Data algorithms. With this work, we aim to provide the scientific community with a comprehensive, critical, and efficient guide to human locomotion datasets across different application domains.  © 2013 IEEE.",1716-1729,10.1109/JBHI.2023.3311677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171538996&doi=10.1109%2fJBHI.2023.3311677&partnerID=40&md5=04abc16fa33ceaf55d3ccca5ea21783f,2022
Security in database management system using machine learning,"The term ‘database security’ refers to the collection of rules, tools, and processes that have been developed to maintain and protect the databases’ confidentiality, integrity, and accessibility. The use of machine learning to improve database management security is becoming more common. The fundamental goal of employing machine learning in security is to make the process of malware detection more actionable, scalable, and successful than conventional techniques, which need the participation of humans. This may be accomplished by making the process more automated. The process entails overcoming problems posed by machine learning, which need to be managed in an effective, logical, and theoretical manner. Machine learning algorithm is applied in the critical paths of the tuner. The optimum configuration for the proposed system yields a throughput boost of between 22% and 35% and a latency reduction of around 60%. The method is robust to various attacks. Copyright © 2024 Inderscience Enterprises Ltd.",124-133,10.1504/IJESDF.2024.136024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182886595&doi=10.1504%2fIJESDF.2024.136024&partnerID=40&md5=190847532da5d667d501c0bed1777e63,2024
EvoSem: A database of polysemous cognate sets,"Polysemies, or “colexifications”, are of great interest in cognitive and historical linguistics, since meanings that are frequently expressed by the same lexeme are likely to be conceptually similar, and lie along a common pathway of semantic change. We argue that these types of inferences can be more reliably drawn from polysemies of cognate sets (which we call “dialexifications”) than from polysemies of lexemes. After giving a precise definition of dialexification, we introduce EvoSem, a cross-linguistic database of etymologies scraped from several online sources. Based on this database (publicly available at http://tiny.cc/EvoSem), we measure for each pair of senses how many cognate sets include them both-i.e. how often this pair of senses is “dialexified”. This allows us to construct a weighted dialexification graph for any set of senses, indicating the conceptual and historical closeness of each pair. We also present an online interface for browsing our database, including graphs and interactive tables. We then discuss potential applications to NLP tasks and to linguistic research. © 2023 Association for Computational Linguistics.",66-75,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184663317&partnerID=40&md5=a8cbecd7a44d7427b02d61ac4059d195,2021
Why are these publications missing? Uncovering the reasons behind the exclusion of documents in free-access scholarly databases,"This study analyses the coverage of seven free-access bibliographic databases (Crossref, Dimensions—non-subscription version, Google Scholar, Lens, Microsoft Academic, Scilit, and Semantic Scholar) to identify the potential reasons that might cause the exclusion of scholarly documents and how they could influence coverage. To do this, 116 k randomly selected bibliographic records from Crossref were used as a baseline. API endpoints and web scraping were used to query each database. The results show that coverage differences are mainly caused by the way each service builds their databases. While classic bibliographic databases ingest almost the exact same content from Crossref (Lens and Scilit miss 0.1% and 0.2% of the records, respectively), academic search engines present lower coverage (Google Scholar does not find: 9.8%, Semantic Scholar: 10%, and Microsoft Academic: 12%). Coverage differences are mainly attributed to external factors, such as web accessibility and robot exclusion policies (39.2%–46%), and internal requirements that exclude secondary content (6.5%–11.6%). In the case of Dimensions, the only classic bibliographic database with the lowest coverage (7.6%), internal selection criteria such as the indexation of full books instead of book chapters (65%) and the exclusion of secondary content (15%) are the main motives of missing publications. © 2023 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals LLC on behalf of Association for Information Science and Technology.",43-58,10.1002/asi.24839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175452108&doi=10.1002%2fasi.24839&partnerID=40&md5=dab2dd28d888dc178a3ba2265367de18,2020
A Distributed Alignment-free Pipeline for Human SNPs Genotyping,"Identification of known genetic traits and disease-related variants within an individual requires a fundamental task: genotyping a set of variants from a database. However, the efficiency of this process is challenged by the growing volume of sequencing data and variant databases. At such scale, even the fastest genotyping tool available can deliver a result in a time that is unacceptable.To address this issue, we present SparkGeno, the first known distributed alignment-free pipeline for genotyping the particular case of Single Nucleotide Polymorphisms (SNPs). Building upon a distributed reformulation of traditional alignment-free genotyping pipelines, and using the Apache Spark framework, we introduce several optimizations to further enhance the performance of our code in a distributed environment. Our pipeline comes in two versions that employ different data structures, making them suitable for processing datasets featuring different numbers of SNPs.Moreover, we present the results of an experimental analysis on widely studied datasets to assess how relying on distributed computing allows for a fast, accurate and scalable solution for large-scale genotyping. Finally, we also report the results of an additional experiment for validating the effectiveness of the signature-based approach we used to perform genotyping.Our results show that SparkGeno, when run on a distributed system, is able to genotype variants from whole-genome sequencing data orders of growth faster than existing tools, in a scalable manner in terms of the number of the available computational units. This makes SparkGeno a promising solution for large-scale genotyping applications, such as precision medicine and population-scale studies. © 2023 Owner/Author(s).",-,10.1145/3584371.3612990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175862854&doi=10.1145%2f3584371.3612990&partnerID=40&md5=20e789c0fbad00f5ef9ccc7ecb90a841,2020
Detection and Tracking of Multiple Faces in Video using Modified KLT Algorithm,"In proposed research, A system for accurate face detection and continual monitoring that makes use of face key characteristics as essential elements of the face that are incorporated in modified KLT approach. A greater degree of data is provided by video frames than by a single image. Critical attribute extractors (points) are used to automate the detection and monitoring of faces in a wide range of applications, such as human behavior identification, automated surveillance and human computer interface. By dividing the monitoring approach into three sections, one can construct a straightforward facial detection and continual monitoring system. Face detection, facial feature identification and tracking of faces are the first three steps. The Viola Jones object detector is used to achieve the face detection and modified KLT method is used to accomplish the continual monitoring. According to the findings of the investigation, our method is more reliable and effective at evaluating video frames to account for different types of occlusions and pose variations. Visual Tracker Benchmark database is used as a standard database. Further results are compared with staff database which was created to verify the outcomes. Average accuracy of 95% is obtained for the proposed method. © 2023 IEEE.",-,10.1109/IC-RVITM60032.2023.10435190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186687122&doi=10.1109%2fIC-RVITM60032.2023.10435190&partnerID=40&md5=237687bd48402333bda0e9d687911879,2023
Space-Query Tradeoffs in Range Subgraph Counting and Listing,"This paper initializes the study of range subgraph counting and range subgraph listing, both of which are motivated by the significant demands in practice to perform graph analytics on subgraphs pertinent to only selected, as opposed to all, vertices. In the first problem, there is an undirected graph G where each vertex carries a real-valued attribute. Given an interval q and a pattern Q, a query counts the number of occurrences of Q in the subgraph of G induced by the vertices whose attributes fall in q. The second problem has the same setup except that a query needs to enumerate (rather than count) those occurrences with a small delay. In both problems, our goal is to understand the tradeoff between space usage and query cost, or more specifically: (i) given a target on query efficiency, how much pre-computed information about G must we store? (ii) Or conversely, given a budget on space usage, what is the best query time we can hope for? We establish a suite of upper- and lower-bound results on such tradeoffs for various query patterns. © Shiyuan Deng, Shangqi Lu, and Yufei Tao; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150724579&doi=10.4230%2fLIPIcs.ICDT.2023.6&partnerID=40&md5=82a56023806ffb8b8ffbde3f1b21c5c0,2023
On Join Sampling and the Hardness of Combinatorial Output-Sensitive Join Algorithms,"We present a dynamic index structure for join sampling. Built for an (equi-) join Q-let IN be the total number of tuples in the input relations of Q-the structure uses ∼O(IN) space, supports a tuple update of any relation in ∼O(1) time, and returns a uniform sample from the join result in ∼O(INρ∗/ /max{1, OUT} ) time with high probability (w.h.p.), where OUT and ρ∗are the join's output size and fractional edge covering number, respectively; notation ∼O(.) hides a factor polylogarithmic to IN. We further show how our result justifies the O(INρ∗) running time of existing worst-case optimal join algorithms (for full result reporting) even when OUT łl INρ∗. Specifically, unless the combinatorial k-clique hypothesis is false, no combinatorial algorithms (i.e., algorithms not relying on fast matrix multiplication) can compute the join result in O(INρ∗-ϵ ) time w.h.p. even if OUT łe INϵ, regardless of how small the constant ϵ > 0 is.  © 2023 ACM.",99-111,10.1145/3584372.3588666,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153463532&doi=10.1145%2f3584372.3588666&partnerID=40&md5=5578df9d380bfd5175ce49de49bcbb35,2023
Towards 3D Colored Mesh Saliency: Database and Benchmarks,"While saliency detection for 3D meshes has been extensively studied in the past decades, only a little work considers color information, and most of existing 3D mesh saliency databases are collected using meshes without color information. The lack of publicly available 3D colored mesh saliency database hinders the research progress in 3D colored mesh saliency detection. In this article, we established a novel 3D colored mesh saliency database (3DCMS) based on an eye-tracking experiment and investigated subjects' visual attention behavior towards 3D colored meshes. Based on the investigations, a novel 3D colored mesh saliency detection framework is proposed which takes both color and geometric features into consideration. To evaluate the performance of the proposed algorithm, we compare it with several relevant methods and apply it to 3D mesh simplification task. The quantitative and qualitative evaluation results demonstrate the superior performance of the proposed framework. The proposed 3DCMS database will be made publicly available.1  © 1999-2012 IEEE.",3580-3591,10.1109/TMM.2023.3312924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171787246&doi=10.1109%2fTMM.2023.3312924&partnerID=40&md5=8e074f39ad3b236f27f6b6cce2eb8dd7,2021
RoBoCoP: A Comprehensive ROmance BOrrowing COgnate Package and Benchmark for Multilingual Cognate Identification,"The identification of cognates is a fundamental process in historical linguistics, on which any further research is based. Even though there are several cognate databases for Romance languages, they are rather scattered, incomplete, noisy, contain unreliable information, or have uncertain availability. In this paper we introduce a comprehensive database of Romance cognates and borrowings based on the etymological information provided by the dictionaries (the largest known database of this kind, in our best knowledge). We extract pairs of cognates between any two Romance languages by parsing electronic dictionaries of Romanian, Italian, Spanish, Portuguese and French. Based on this resource, we propose a strong benchmark for the automatic detection of cognates, by applying machine learning and deep learning based methods on any two pairs of Romance languages. We find that automatic identification of cognates is possible with accuracy averaging around 94% for the more difficult task formulations. © 2023 Association for Computational Linguistics.",7610-7629,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184809656&partnerID=40&md5=42060e0bfbf85d91b63aa42374dedbf4,2021
Investigating Trojan Attacks on Pre-trained Language Model-powered Database Middleware,"The recent success of pre-trained language models (PLMs) such as BERT has resulted in the development of various beneficial database middlewares, including natural language query interfaces and entity matching. This shift has been greatly facilitated by the extensive external knowledge of PLMs. However, as PLMs are often provided by untrusted third parties, their lack of standardization and regulation poses significant security risks that have yet to be fully explored. This paper investigates the security threats posed by malicious PLMs to these emerging database middleware. We specifically propose a novel type of Trojan attack, where a maliciously designed PLM causes unexpected behavior in the database middleware. These Trojan attacks possess the following characteristics: (1) Triggerability: The Trojan-infected database middleware will function normally with normal input, but will likely malfunction when triggered by the attacker. (2) Imperceptibility: There is no need for noticeable modification of the input to trigger the Trojan. (3) Generalizability: The Trojan is capable of targeting a variety of downstream tasks, not just one specific task. We thoroughly evaluate the impact of these Trojan attacks through experiments and analyze potential countermeasures and their limitations. Our findings could aid in the creation of stronger mechanisms for the implementation of PLMs in database middleware.  © 2023 ACM.",437-447,10.1145/3580305.3599395,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171328111&doi=10.1145%2f3580305.3599395&partnerID=40&md5=f4b38b2b886f65c23001771d5bd2e797,2021
Epileptic Seizure Detection with an End-to-End Temporal Convolutional Network and Bidirectional Long Short-Term Memory Model,"Automatic seizure detection plays a key role in assisting clinicians for rapid diagnosis and treatment of epilepsy. In view of the parallelism of temporal convolutional network (TCN) and the capability of bidirectional long short-term memory (BiLSTM) in mining the long-range dependency of multi-channel time-series, we propose an automatic seizure detection method with a novel end-to-end TCN-BiLSTM model in this work. First, raw EEG is filtered with a 0.5–45 Hz band-pass filter, and the filtered data are input into the proposed TCN-BiLSTM network for feature extraction and classification. Post-processing process including moving average filtering, thresholding and collar technique is then employed to further improve the detection performance. The method was evaluated on two EEG database. On the CHB-MIT scalp EEG database, our method achieved a segment-based sensitivity of 94.31%, specificity of 97.13%, and accuracy of 97.09%. Meanwhile, an event-based sensitivity of 96.48% and an average false detection rate (FDR) of 0.38/h were obtained. On the SH-SDU database we collected, the segment-based sensitivity of 94.99%, specificity of 93.25%, and accuracy of 93.27% were achieved. In addition, an event-based sensitivity of 99.35% and a false detection rate of 0.54/h were yielded. The total detection time consumed for 1 h EEG data was 5.65 s. These results demonstrate the superiority and promising potential of the proposed method in real-time monitoring of epileptic seizures. © World Scientific Publishing Company.",-,10.1142/S0129065724500126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183194096&doi=10.1142%2fS0129065724500126&partnerID=40&md5=86fcf4d7103d9ac6fc45fe2a56773576,2020
CSSLdb: Discovery of cancer-specific synthetic lethal interactions based on machine learning and statistic inference,"Synthetic lethality (SL) occurs when the inactivation of two genes results in cell death while the inactivation of either gene alone is non-lethal. SL-based therapy has become a promising anti-cancer treatment option with the increasing researches and applications in clinical practice, while the specific therapeutic opportunities for various cancers have not yet been comprehensively investigated. Herein, we described a computational approach based on machine learning and statistical inference to discover the cancer-specific synthetic lethal interactions. First, Random Forest and One-Class SVM were used to perform cancer unbiased prediction of synthetic lethality. Then, two strategies, including mutual exclusivity and differential expression, were used to screen cancer-specific synthetic lethal interactions, resulting in 14,582 SL gene pairs in 33 cancer types. Finally, we developed a freely available database of CSSLdb (Cancer Specific Synthetic Lethality Database, http://www.tmliang.cn/CSSL/) to present cancer-specific synthetic lethal genetic interactions, which would enrich the relevant research and contribute to underlying therapy strategies based on synthetic lethality. © 2024",-,10.1016/j.compbiomed.2024.108066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184000179&doi=10.1016%2fj.compbiomed.2024.108066&partnerID=40&md5=900a00ae895faeced46444c8f48e44f4,2021
IRRegularities in the Internet Routing Registry,"The Internet Routing Registry (IRR) is a set of distributed databases used by networks to register routing policy information and to validate messages received in the Border Gateway Protocol (BGP). First deployed in the 1990s, the IRR remains the most widely used database for routing security purposes, despite the existence of more recent and more secure alternatives. Yet, the IRR lacks a strict validation standard and the limited coordination across different database providers can lead to inaccuracies. Moreover, it has been reported that attackers have begun to register false records in the IRR to bypass operators' defenses when launching attacks on the Internet routing system, such as BGP hijacks. In this paper, we provide a longitudinal analysis of the IRR over the span of 1.5 years. We develop a workflow to identify irregular IRR records that contain conflicting information compared to different routing data sources. We identify 34,199 irregular route objects out of 1,542,724 route objects from November 2021 to May 2023 in the largest IRR database and find 6,373 to be potentially suspicious. © 2023 Owner/Author.",104-110,10.1145/3618257.3624843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177613561&doi=10.1145%2f3618257.3624843&partnerID=40&md5=0b1941c7237868e8dc15105b491ca5e2,2023
Rapid Determination of Positive–Negative Bacterial Infection Based on Micro-Hyperspectral Technology,"To meet the demand for rapid bacterial detection in clinical practice, this study proposed a joint determination model based on spectral database matching combined with a deep learning model for the determination of positive–negative bacterial infection in directly smeared urine samples. Based on a dataset of 8124 urine samples, a standard hyperspectral database of common bacteria and impurities was established. This database, combined with an automated single-target extraction, was used to perform spectral matching for single bacterial targets in directly smeared data. To address the multi-scale features and the need for the rapid analysis of directly smeared data, a multi-scale buffered convolutional neural network, MBNet, was introduced, which included three convolutional combination units and four buffer units to extract the spectral features of directly smeared data from different dimensions. The focus was on studying the differences in spectral features between positive and negative bacterial infection, as well as the temporal correlation between positive–negative determination and short-term cultivation. The experimental results demonstrate that the joint determination model achieved an accuracy of 97.29%, a Positive Predictive Value (PPV) of 97.17%, and a Negative Predictive Value (NPV) of 97.60% in the directly smeared urine dataset. This result outperformed the single MBNet model, indicating the effectiveness of the multi-scale buffered architecture for global and large-scale features of directly smeared data, as well as the high sensitivity of spectral database matching for single bacterial targets. The rapid determination solution of the whole process, which combines directly smeared sample preparation, joint determination model, and software analysis integration, can provide a preliminary report of bacterial infection within 10 min, and it is expected to become a powerful supplement to the existing technologies of rapid bacterial detection. © 2024 by the authors.",-,10.3390/s24020507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183285296&doi=10.3390%2fs24020507&partnerID=40&md5=b02f2cc64fb2c3dcd0d96844fe3981e6,2022
Design and implementation of advanced mathematics assisted learning system based on learning analysis technology,"In recent years, with the rapid development of the Internet and big data, various industries have been actively integrating information technology. In the field of education, China has also conducted relevant explorations. Currently, the level of informatization in higher education is much higher than that in secondary and early childhood education. Therefore, major universities have always been at the forefront in the construction of auxiliary learning systems. This article adopts the concept and methods of learning analytics technology to design and implement a fully functional personalized auxiliary learning system. Firstly, this system obtains the requirements and positioning data through preliminary research, and then summarizes and summarizes this data to form an overview of the overall framework and functions. Next, based on these elements, the design and implementation of data structures are carried out, including the division of entity items and the creation of database tables, with a focus on describing the process of building the database and the logical relationships between forms, forming the system's global E-R diagram. Finally, based on this data structure, corresponding system development work is carried out. This auxiliary learning system is specifically designed for higher mathematics subjects and offers online question answering and real-time feedback, demonstrating excellent application effectiveness.  © 2023 IEEE.",436-440,10.1109/ICEDCS60513.2023.00084,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182604006&doi=10.1109%2fICEDCS60513.2023.00084&partnerID=40&md5=160d562a8dac99783ae004227398c842,2021
Dual-Teacher Feature Distillation: A Transfer Learning Method for Insomniac PSG Staging,"Insomnia is the most common sleep disorder linked with adverse long-term medical and psychiatric outcomes. Automatic sleep staging plays a crucial role in aiding doctors to diagnose insomnia disorder. Only a few studies have been conducted to develop automatic sleep staging methods for insomniacs, and most of them have utilized transfer learning methods, which involve pre-training models on healthy individuals and then fine-tuning them on insomniacs. Unfortunately, significant differences in feature distribution between the two subject groups impede the transfer performance, highlighting the need to effectively integrate the features of healthy subjects and insomniacs. In this paper, we propose a dual-teacher cross-domain knowledge transfer method based on the feature-based knowledge distillation to improve the performance of sleep staging for insomniacs. Specifically, the insomnia teacher directly learns from insomniacs and feeds the corresponding domain-specific features into the student network, while the health domain teacher guide the student network to learn domain-generic features. During the training process, we adopt the OFD (Overhaul of Feature Distillation) method to build the health domain teacher. We conducted the experiments to validate the proposed method, using the Sleep-EDF database as the source domain and the CAP-Database as the target domain. The results demonstrate that our method surpasses advanced techniques, achieving an average sleep staging accuracy of 80.56% on the CAP-Database. Furthermore, our method exhibits promising performance on the private dataset.  © 2013 IEEE.",1730-1741,10.1109/JBHI.2023.3337261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187196961&doi=10.1109%2fJBHI.2023.3337261&partnerID=40&md5=a163d1a4e8e3a3c3acf46542ccc5ffc9,2022
"Next-generation study databases require FAIR, EHR-integrated, and scalable Electronic Data Capture for medical documentation and decision support","Structured patient data play a key role in all types of clinical research. They are often collected in study databases for research purposes. In order to describe characteristics of a next-generation study database and assess the feasibility of its implementation a proof-of-concept study in a German university hospital was performed. Key characteristics identified include FAIR access to electronic case report forms (eCRF), regulatory compliant Electronic Data Capture (EDC), an EDC with electronic health record (EHR) integration, scalable EDC for medical documentation, patient generated data, and clinical decision support. In a local case study, we then successfully implemented a next-generation study database for 19 EDC systems (n = 2217 patients) that linked to i.s.h.med (Oracle Cerner) with the local EDC system called OpenEDC. Desiderata of next-generation study databases for patient data were identified from ongoing local clinical study projects in 11 clinical departments at Heidelberg University Hospital, Germany, a major tertiary referral hospital. We compiled and analyzed feature and functionality requests submitted to the OpenEDC team between May 2021 and July 2023. Next-generation study databases are technically and clinically feasible. Further research is needed to evaluate if our approach is feasible in a multi-center setting as well. © 2024, The Author(s).",-,10.1038/s41746-023-00994-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182155704&doi=10.1038%2fs41746-023-00994-6&partnerID=40&md5=72981725a7fa0a1c6fad551d6272ced8,2023
"Survey on facial expressions recognition: databases, features and classification schemes","The recognition of facial expression in images is one of the motional states in the observing forms and is one of the most frequent non-verbal routes in which a person transfers his inner emotional expressions on faces. The recognition of facial expressions in a wide range of fields including psychological and legal studies, animation, robotics, lip-reading, image and video conferencing, communications, telecommunications, and security protection whilst counterterrorism is used to identify individuals as well as human-machine confrontation. The general solution to this problem includes three general steps: images preprocessing, features extraction, and expression classification algorithms. A series of pre-processing steps must be performed to process the area on the face and then detect the expression, that is, a square in the face must be localized while the rest of the image must be removed. Then, Features extraction is used to classify. Each facial expression to a specific category. We divide our data, including images from different expressions, into two parts: training and testing. Different categories have been learned to specify different features that tested thereafter. In recent years, a number of researches has been performed on a facial expression analysis. Even though much progress has been made in this field since the recognition of facial expression with a high accuracy rate is difficult to achieve due to the complexity and variability. In this research article, we noticed that most of researchers are used JAFFE and CK+ databases due the diversification and high accuracy. Nevertheless most of researchers are used PSO, PCA, and LBP features as well as HOG that presented high accuracy. We also noticed that SVM and CNN classification algorithms have been used mostly due to high accuracy and response latency with few errors. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",7457-7478,10.1007/s11042-023-15139-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161435494&doi=10.1007%2fs11042-023-15139-w&partnerID=40&md5=a4902f06464c812608abdbb6c87654ab,2022
INFLUENCE OF PRE-PROCESSING METHODS ON THE AUTOMATIC PRIORITY PREDICTION OF NATIVE-LANGUAGE END-USERS’ MAINTENANCE REQUESTS THROUGH MACHINE LEARNING METHODS,"Feedback and requests by occupants are relevant sources of data to improve building management, and building maintenance. Indeed, most predictable faults can be directly identified by occupants and communicated to facility managers through communications written in the end-users’ native language. In this sense, natural language processing methods can support the request identification and attribution process if they are robust enough to extract useful information from these unstructured textual sources. Machine learning (ML) can support assessing and managing these data, especially in the case of many simultaneous communications. In this field, the application of pre-processing and ML methods to English-written databases has been widely provided, while efforts in other native languages are still limited, impacting the real applicability. Moreover, the performance of combinations of methods for pre-processing, ML and classification classes attribution, has been limitedly investigated while comparing different languages. To fill this gap, this work hence explores the performance of automatic priority assignment of maintenance end-users’ requests depending on the combined influence of: (a) different natural language pre-processing methods, (b) several supervised ML algorithms, (c) two priority classification rules (2-class versus 4-class), (d) the database language (i.e. the original database written in Italian, the native end-users’ language; a translated database version in English, as standard reference). Analyses are performed on a database of about 12000 maintenance requests written in Italian concerning a stock of 23 buildings open to the public. A random sample of the sentences is supervised and labelled by 20 expert annotators following the best-worst method to attribute a priority score. Labelled sentences are then pre-processed using four different approaches to progressively reduce the number of unique words (potential predictors). Five different consolidated ML methods are applied, and comparisons involve accuracy, precision, recall and F1-score for each combination of pre-processing action, ML method and the number of priority classes. Results show that, within each ML algorithm, different pre-processing methods limitedly impact the final accuracy and average F1-score. In both Italian and English conditions, the best performance is obtained by NN, LR, SVM methods, while NB generally fails, and by considering the 2-class priority classification scale. In this sense, results confirm that facility managers can be effectively supported by ML methods for preliminary priority assessments in building maintenance processes, even when the requests database is written in end-users’ native language. © 2024 The author(s).",99-116,10.36680/j.itcon.2024.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188337396&doi=10.36680%2fj.itcon.2024.006&partnerID=40&md5=ddac16a65146c14b1d32ee67e53f639d,2021
Deep learning combined with singular value decomposition to reconstruct databases in fluid dynamics[Formula presented],"Fluid dynamics problems are characterized by being multidimensional and nonlinear. Therefore, experiments and numerical simulations are complex and time-consuming. Motivated by this, the need arises to find new techniques to obtain data in a simpler way and in less time. In this article, we present a novel methodology based on physical principles to reconstruct databases with three, four and five dimensions, from sparse databases formed by sensor measurements. The methodology consists of combining Single Value Decomposition (SVD), which can extract the main flow dynamics, with neural networks. The neural network used is characterized by a simple architecture based on combining two autoencoders that work in parallel and are joined in the last layer. This new algorithm has been proved with three databases with different dimensions and complexities: in an Atmospheric Boundary Layer (ABL) with a turbulence model and in the flow past a two- and a three-dimensional cylinder. By applying this methodology, it has been achieved to reconstruct databases of different dimensions obtaining errors of the same order as those obtained in simulations. Summarizing, this work proposes a new hybrid physics-based machine learning model with a simple, robust and generalizable architecture, which allows reconstructing databases from very few sensors and with a very low computational cost. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.121924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173360282&doi=10.1016%2fj.eswa.2023.121924&partnerID=40&md5=97d4fff12c81a8ca5a5ca26f48f8eead,2024
Cyber Risk Evaluation for Android-based Devices,"According to the National Vulnerability Database (NVD), the number of registered cyber vulnerabilities for software keeps growing. In the public vulnerability database at cve.mitre.org, as of June 13, 2023, there are 8,314 software vulnerabilities related to Android™ operating system or mobile applications for Android™ widely used in cyber-physical systems. In this paper, we propose a solution to evaluate cyber risks of Android™-based software systems. Our solution determines the software packages installed on a device with Android™, assesses their associated cyber risks, and recommends alternative software products to make the system less vulnerable against cyber attacks. Also, it allows to pre-Assess cyber risks for software and choose the least vulnerable configuration of the computing system at the design stage. We calculated the cyber risk scores using Common Vulnerability Scoring System (CVSS) scores for 2,423 Common Vulnerabilities and Exposures (CVEs) extracted from the NVD database and associated with different versions of Android™ operating system; and for 303 CVEs associated with web browsers. Our results indicate that the least vulnerable configuration for the Android™-based system includes Android™ 13 with Opera® web browser.  © 2023 IEEE.",-,10.1109/DSC61021.2023.10354217,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182283708&doi=10.1109%2fDSC61021.2023.10354217&partnerID=40&md5=962c3d870da45f02fcf550610e2e31c5,2023
Direct Access for Answers to Conjunctive Queries with Aggregation,"We study the fine-grained complexity of conjunctive queries with grouping and aggregation. For some common aggregate functions (e.g., min, max, count, sum), such a query can be phrased as an ordinary conjunctive query over a database annotated with a suitable commutative semiring. Specifically, we investigate the ability to evaluate such queries by constructing in log-linear time a data structure that provides logarithmic-time direct access to the answers ordered by a given lexicographic order. This task is nontrivial since the number of answers might be larger than log-linear in the size of the input, and so, the data structure needs to provide a compact representation of the space of answers. In the absence of aggregation and annotation, past research provides a sufficient tractability condition on queries and orders. For queries without self-joins, this condition is not just sufficient, but also necessary (under conventional lower-bound assumptions in fine-grained complexity). We show that all past results continue to hold for annotated databases, assuming that the annotation itself is not part of the lexicographic order. On the other hand, we show infeasibility for the case of count-distinct that does not have any efficient representation as a commutative semiring. We then investigate the ability to include the aggregate and annotation outcome in the lexicographic order. Among the hardness results, standing out as tractable is the case of a semiring with an idempotent addition, such as those of min and max. Notably, this case captures also count-distinct over a logarithmic-size domain. © Idan Eldar, Nofar Carmeli, and Benny Kimelfeld.",-,10.4230/LIPIcs.ICDT.2024.4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188656665&doi=10.4230%2fLIPIcs.ICDT.2024.4&partnerID=40&md5=9cbac7d36e7c404764e38aa1678aa9ea,2021
Towards Merkle Trees for High-Performance Data Systems,"Merkle Trees (and its variants) are widely used for building secure outsourced data systems. The adoption of Merkle Trees for high-performance data systems, however, uncovered major performance challenges. First and unlike classical data structures, Merkle Trees involve expensive cryptographic operations and are thus CPU-bound. Second, they are not well suited for modern multi-core CPUs because they introduce a single point of contention making Merkle Trees hard to parallelize. While recent work aimed at replacing Merkle Trees to circumvent their performance problem, we suggest new techniques to speed-up this ubiquitous data structure and achieve high-performance. In this paper, we present initial results showing that in contrast to common wisdom it is indeed possible to build high-performance Merkle Trees with orders of magnitude performance improvements. © 2023 ACM.",28-33,10.1145/3595647.3595651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163690619&doi=10.1145%2f3595647.3595651&partnerID=40&md5=3de571ef0ff8839ebff549dc5cd1a509,2020
Towards Database and Serverless Runtime Co-Design,"In serverless computing, minimizing database access latency is crucial, but databases predominantly reside in cloud environments, necessitating costly network round-trips. Embedding an in-process database library such as SQLite into the serverless runtime is the holy grail for low-latency database access. However, SQLite's architecture limits concurrency and multitenancy, which is essential for serverless providers, forcing us to rethink the architecture for integrating a database library. We propose changing the SQLite virtual machine to provide asynchronous bytecode instructions to avoid blocking in the library and decoupling the query and storage engines to facilitate database and serverless runtime co-design.  © 2023 Owner/Author.",9-10,10.1145/3630202.3630225,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181807241&doi=10.1145%2f3630202.3630225&partnerID=40&md5=d1548aae6390acc8429805a3461850d6,2024
Database of 4 Million Medicinal Chemistry-Relevant Ring Systems,"Central ring systems are the most important part of bioactive molecules. They determine molecule shape, keep substituents in their proper positions, and also influence global molecular properties. In the present study, a database of 4 million medicinal chemistry-relevant ring systems has been created, not by crude random enumeration but by applying a set of rules derived by analyzing rings present in bioactive molecules. The aromatic properties and tautomer stability of generated rings have also been considered to ensure that the rings in the database are stable and chemically reasonable. 99.2% of these rings are novel and not included in molecules in the ChEMBL or PubChem databases. This large database of ring systems has been created with the goal to provide support for bioisosteric design and scaffold hopping as well as to be used in generative chemistry applications. The complete set of created rings is available for download in the SMILES format from https://peter-ertl.com/molecular/data/. © 2024 American Chemical Society.",1245-1250,10.1021/acs.jcim.3c01812,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184804700&doi=10.1021%2facs.jcim.3c01812&partnerID=40&md5=e1277b7135f61f6118780c3639df64fd,2022
"The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements","Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students' initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education. © 2023 Owner/Author.",-,10.1145/3617694.3623223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177849262&doi=10.1145%2f3617694.3623223&partnerID=40&md5=436c8623d5d09394628a6e993210749d,2023
BRUDEX Database: Binaural Room Impulse Responses with Uniformly Distributed External Microphones,"There is an emerging need for comparable data for multi-microphone processing, particularly in acoustic sensor networks. However, commonly available databases are often limited in the spatial diversity of the microphones or only allow for particular signal processing tasks. In this paper, we present a database of acoustic impulse responses and recordings for a binaural hearing aid setup, 36 spatially distributed microphones spanning a uniform grid of (5×5) m2 and 12 source positions. This database can be used for a variety of signal processing tasks, such as (multi-microphone) noise reduction, source localization, and dereverberation, as the measurements were performed using the same setup for three different reverberation conditions (T60 ≈ {310,510,1300} ms). The usability of the database is demonstrated for a noise reduction task using a minimum variance distortionless response beamformer based on relative transfer functions, exploiting the availability of spatially distributed microphones. © VDE VERLAG GMBH Berlin Offenbach.",126-130,10.30420/456164024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183420879&doi=10.30420%2f456164024&partnerID=40&md5=6d2c0d7425b140a179a037ec9a479d64,2023
MDIW-13: a New Multi-Lingual and Multi-Script Database and Benchmark for Script Identification,"Script identification plays a vital role in applications that involve handwriting and document analysis within a multi-script and multi-lingual environment. Moreover, it exhibits a profound connection with human cognition. This paper provides a new database for benchmarking script identification algorithms, which contains both printed and handwritten documents collected from a wide variety of scripts, such as Arabic, Bengali (Bangla), Gujarati, Gurmukhi, Devanagari, Japanese, Kannada, Malayalam, Oriya, Roman, Tamil, Telugu, and Thai. The dataset consists of 1,135 documents scanned from local newspaper and handwritten letters as well as notes from different native writers. Further, these documents are segmented into lines and words, comprising a total of 13,979 and 86,655 lines and words, respectively, in the dataset. Easy-to-go benchmarks are proposed with handcrafted and deep learning methods. The benchmark includes results at the document, line, and word levels with printed and handwritten documents. Results of script identification independent of the document/line/word level and independent of the printed/handwritten letters are also given. The new multi-lingual database is expected to create new script identifiers, present various challenges, including identifying handwritten and printed samples and serve as a foundation for future research in script identification based on the reported results of the three benchmarks. © 2023, The Author(s).",131-157,10.1007/s12559-023-10193-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168954975&doi=10.1007%2fs12559-023-10193-w&partnerID=40&md5=689f9ccc44a90016f20f30e736fc5299,2024
A Simple Algorithm for Consistent Query Answering Under Primary Keys,"We consider the dichotomy conjecture for consistent query answering under primary key constraints. It states that, for every fixed Boolean conjunctive query q, testing whether q is certain (i.e. whether it evaluates to true over all repairs of a given inconsistent database) is either polynomial time or coNP-complete. This conjecture has been verified for self-join-free and path queries. We propose a simple inflationary fixpoint algorithm for consistent query answering which, for a given database, naively computes a set ∆ of subsets of database repairs with at most k facts, where k is the size of the query q. The algorithm runs in polynomial time and can be formally defined as: 1. Initialize ∆ with all sets S of at most k facts such that S |= q. 2. Add any set S of at most k facts to ∆ if there exists a block B (i.e., a maximal set of facts sharing the same key) such that for every fact a ∈ B there is a set S′ ∈ ∆ contained in S ∪ {a}. The algorithm answers “q is certain” iff ∆ eventually contains the empty set. The algorithm correctly computes certainty when the query q falls in the polynomial time cases of the known dichotomies for self-join-free queries and path queries. For arbitrary Boolean conjunctive queries, the algorithm is an under-approximation: the query is guaranteed to be certain if the algorithm claims so. However, there are polynomial time certain queries (with self-joins) which are not identified as such by the algorithm. © Diego Figueira, Anantha Padmanabha, Luc Segoufin, and Cristina Sirangelo; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146829500&doi=10.4230%2fLIPIcs.ICDT.2023.24&partnerID=40&md5=a0deb2b6ea7ca6bdba6d57acc4e251a6,2023
Efficient Vertical Structure Correlation and Power Line Inference,"High-resolution three-dimensional data from sensors such as LiDAR are sufficient to find power line towers and poles but do not reliably map relatively thin power lines. In addition, repeated detections of the same object can lead to confusion while data gaps ignore known obstacles. The slow or failed detection of low-salience vertical obstacles and associated wires is one of today’s leading causes of fatal helicopter accidents. This article presents a method to efficiently correlate vertical structure observations with existing databases and infer the presence of power lines. The method uses a spatial hash key which compares an observed tower location to potential existing tower locations using nested hash tables. When an observed tower is in the vicinity of an existing entry, the method correlates or distinguishes objects based on height and position. When applied to Delaware’s Digital Obstacle File, the average horizontal uncertainty decreased from 206 to 56 ft. The power line presence is inferred by automatically comparing the proportional spacing, height, and angle of tower sets based on the more accurate database. Over 87% of electrical transmission towers were correctly identified with no false negatives. © 2024 by the authors.",-,10.3390/s24051686,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187522945&doi=10.3390%2fs24051686&partnerID=40&md5=5d82780885f51b80ec894b6b2eaaff87,2022
Anticipating 2020s: Deep Learning Methods for Generating Videos from Found Footage Toward a Database Aesthetic,"This paper presents the methodology employed for generating the video Anticipating 2020s which attempts to portray our collective imagination about how life in the 2020s would be from the perspective of decades-earlier science fiction films and books. The video was generated by rearranging the found footage of small excerpts from various science fiction movies set in the 2020s based on the output of a deep learning algorithm for object detection. Furthermore, Anticipating 2020s is subtitled with text generated by the deep learning model GPT-2 finetuned on the text of science fiction books set in the 2020s. The presented methodology explores the use of machine learning algorithms to create a database of found footage and the potential of trajectories through the database to create new narratives.  © 2023 Owner/Author.",-,10.1145/3615522.3615555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178345348&doi=10.1145%2f3615522.3615555&partnerID=40&md5=be1a286c4b3f9db3cc3118fcbd41cb24,2022
Machine learning on cardiotocography data to classify fetal outcomes: A scoping review,"Introduction: Uterine contractions during labour constrict maternal blood flow and oxygen delivery to the developing baby, causing transient hypoxia. While most babies are physiologically adapted to withstand such intrapartum hypoxia, those exposed to severe hypoxia or with poor physiological reserves may experience neurological injury or death during labour. Cardiotocography (CTG) monitoring was developed to identify babies at risk of hypoxia by detecting changes in fetal heart rate (FHR) patterns. CTG monitoring is in widespread use in intrapartum care for the detection of fetal hypoxia, but the clinical utility is limited by a relatively poor positive predictive value (PPV) of an abnormal CTG and significant inter and intra observer variability in CTG interpretation. Clinical risk and human factors may impact the quality of CTG interpretation. Misclassification of CTG traces may lead to both under-treatment (with the risk of fetal injury or death) or over-treatment (which may include unnecessary operative interventions that put both mother and baby at risk of complications). Machine learning (ML) has been applied to this problem since early 2000 and has shown potential to predict fetal hypoxia more accurately than visual interpretation of CTG alone. To consider how these tools might be translated for clinical practice, we conducted a review of ML techniques already applied to CTG classification and identified research gaps requiring investigation in order to progress towards clinical implementation. Materials and method: We used identified keywords to search databases for relevant publications on PubMed, EMBASE and IEEE Xplore. We used Preferred Reporting Items for Systematic Review and Meta-Analysis for Scoping Reviews (PRISMA-ScR). Title, abstract and full text were screened according to the inclusion criteria. Results: We included 36 studies that used signal processing and ML techniques to classify CTG. Most studies used an open-access CTG database and predominantly used fetal metabolic acidosis as the benchmark for hypoxia with varying pH levels. Various methods were used to process and extract CTG signals and several ML algorithms were used to classify CTG. We identified significant concerns over the practicality of using varying pH levels as the CTG classification benchmark. Furthermore, studies needed to be more generalised as most used the same database with a low number of subjects for an ML study. Conclusion: ML studies demonstrate potential in predicting fetal hypoxia from CTG. However, more diverse datasets, standardisation of hypoxia benchmarks and enhancement of algorithms and features are needed for future clinical implementation. © 2024",-,10.1016/j.compbiomed.2024.108220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187805412&doi=10.1016%2fj.compbiomed.2024.108220&partnerID=40&md5=9626639d852efba9a3463a9e2a242ccc,2022
Experimental data-centric prediction of penetration depth and holding capacity of dynamically installed anchors using machine learning,"The visualization of experimental research phenomena and the reliability of experimental data enable their utilization in validating theoretical and numerical methods. However, the existing literature on dynamically installed anchors (DIAs) lacks cohesion, resulting in a reliance on onshore driven pile design theory, as well as empirical or semi-empirical calculation methods for DIA design. Therefore, this study aims to establish an experimental database and propose an data-centric design method for DIAs. The established experimental database comprises 503 sets of experimental data from various sources, including 254 sets of field tests, 210 sets of centrifuge tests and 39 sets of 1g-model tests of seven representative DIAs (i.e., torpedo anchor, DPA, OMNI-Max anchor, DEPLA, L-GIPLA, DPAIII, and fish anchor). The geometric characteristics as well as in-soil installation and loading performance of these DIAs are systematically summarized and explored. Based on this comprehensive experimental database, a novel machine learning (ML) algorithm-based approach is proposed for predicting the penetration depth and holding capacity of DIAs. This research provides a new perspective centered around existing experimental data for the design methodology of DIAs. © 2024 Elsevier Ltd",-,10.1016/j.compgeo.2024.106249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189105335&doi=10.1016%2fj.compgeo.2024.106249&partnerID=40&md5=860c4d73fdee7bf874f2e601add6372b,2021
Noise estimation based on optimal smoothing and minimum controlled through recursive averaging for speech enhancement,"One of the most significant challenges in the real time speech processing applications is the elimination of noise in the corrupted speech data. This noise can significantly impact the efficacy/performance of the applications of speech processing. However, developing a robust noise reduction algorithm is crucial for improving the accuracy of automatic speech recognition (ASR) and other speech processing systems under uncontrolled conditions. In this paper, we propose an algorithm to reduce the background noise in the degraded speech data under highly non-stationary conditions. The proposed optimal smoothing and minima controlled (OSMC) technique uses recursive averaging to enhance degraded speech data. Initially, a smoothed periodogram and local minima of the degraded speech data are computed and determined the time-frequency dependent threshold factor. The ratio of smoothed periodogram to local minima is used to find the active regions of speech in the degraded speech data by adapting the Bayesian minimum cost decision rule. To calculate the estimated noise spectrum for each frequency bin, a time-frequency smoothing factors are used. The perceptual evaluation of speech quality (PESQ) and normalized covariance metric (NCM) are used to evaluate the speech quality and intelligibility of the proposed technique over competing algorithms after speech enhancement. The experimental results demonstrated that the proposed algorithm has given a significant improvement in terms of average values of PESQ by 15.03% and 16.71% and NCM by 3.45% and 5.73% for NOIZEUS and Kannada speech databases at 5 dB and 10 dB respectively, over unprocessed speech under highly non-stationary noisy environments. © 2023 The Author(s)",-,10.1016/j.iswa.2023.200310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178061945&doi=10.1016%2fj.iswa.2023.200310&partnerID=40&md5=884c44a45a0183aeb067340d86d24e92,2022
Speaker recognition using Improved Butterfly Optimization Algorithm with hybrid Long Short Term Memory network,"Speaker recognition is extensively applied in several applications, namely identity verification, electronic voice eavesdropping, surveillance, voice recognition, etc. In an effective speaker recognition system, the extraction and selection of salient and discriminative features is an essential process for accurately identifying the speakers. Therefore, a novel hybrid framework is introduced in this research manuscript. Initially, the input data are acquired from the three-benchmark databases: THUYG-20 SRE corpus, Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), and LibriSpeech. Further, the emotional features are extracted by utilizing hybrid feature extraction techniques which are, amplitude, zero cross rate, energy, Root Mean Square (RMS), statistical moments, autocorrelation, and Mel-Frequency Cepstral Coefficients (MFCC). Then, the feature optimization is carried out using Improved Butterfly Optimization Algorithm (IBOA) that decreases the computational time and complexity of the recognition model. At last, a hybrid classifier: Convolutional Neural Network (CNN) with Long Short Term Memory (LSTM) is implemented for speaker recognition, and its performance is analyzed in terms of F1-score, specificity, accuracy, Positive Predictive Value (PPV), and sensitivity. The empirical investigation demonstrated that the IBOA-based hybrid LSTM network achieved 92.65%, 96.97% and 96.98% of recognition accuracy on the LibriSpeech, RAVDESS and THUYG-20 SRE corpus databases. These results are more impressive than the comparative models, Deep Neural Network (DNN), random forest, K-Nearest Neighbor (KNN), LSTM, Multi class Support Vector Machine (MSVM), Deep Convolutional Recurrent Neural Network (DCRNN), Golden Ratio aided Neural Network (GRaNN), deep sequential LSTM, and Probabilistic Neural Network (PNN). © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11042-024-18298-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185126184&doi=10.1007%2fs11042-024-18298-6&partnerID=40&md5=53e74c4c914b8011a51793281bba95a9,2021
Jointly Events Extraction for Database Alarm based on Dynamic Matching Strategy and GCN,"Event extraction is one of the important processes in building event knowledge graphs. However, existing models cannot combine syntactic construction of text and structured knowledge of event to execute the identification and classification of trigger word and argument. In order to solve this problem, a jointly events extraction model for database fault alarm based on dynamic matching strategy and Graph Convolution Neural Networks (GCN) is proposed. Specifically, dynamic matching strategy is designed to label the event nodes which include the feature of event construction for the text, thus solving the classification of the ambiguous and unseen trigger words. Then, to get better syntactic features, GCN is used to aggregate the semantic features. The experimental results on Oracle alarm dataset show that Precision, Recall and F1 of our model respectively are 76.3%, 72.2% and 74.2%., which are better than JMEE and GREE. © 2023 IEEE.",24-29,10.1109/ICITES59818.2023.10356864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182607974&doi=10.1109%2fICITES59818.2023.10356864&partnerID=40&md5=86796404ae5b472064cd1dd0a524de14,2023
Optimization query algorithm of distance education database based on student behavior analysis,"Distance education database query refinement is a pivotal challenge in decentralized database systems. The goal of distributed query execution is to convert an advanced inquiry in a dispersed database into an efficient operational blueprint on a localized database. In this paper, an algorithm framework based on student behavior analysis is proposed on the basis of the current mainstream elective recommendation system. By introducing a large number of students' learning behavior data, students' personality characteristics are mined, students' social relations are calculated, and students' relatively complete behavior portraits are constructed. Data query is carried out according to different data categories, thus realizing database optimization query. Experimental results show that this algorithm can effectively improve the efficiency of database information query.  © 2023 IEEE.",680-684,10.1109/ICEDCS60513.2023.00131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182593578&doi=10.1109%2fICEDCS60513.2023.00131&partnerID=40&md5=8280e0ecb036536458840a7022229339,2021
EmoMatchSpanishDB: study of speech emotion recognition machine learning models in a new Spanish elicited database,"In this paper we present a new speech emotion dataset on Spanish. The database is created using an elicited approach and is composed by fifty non-actors expressing the Ekman’s six basic emotions of anger, disgust, fear, happiness, sadness, and surprise, plus neutral tone. This article describes how this database has been created from the recording step to the performed crowdsourcing perception test step. The crowdsourcing has facilitated to statistically validate the emotion of each collected audio sample and also to filter noisy data samples. Hence we obtained two datasets EmoSpanishDB and EmoMatchSpanishDB. The first includes those recorded audios that had consensus during the crowdsourcing process. The second selects from EmoSpanishDB only those audios whose emotion also matches with the originally elicited. Last, we present a baseline comparative study between different state of the art machine learning techniques in terms of accuracy, precision, and recall for both datasets. The results obtained for EmoMatchSpanishDB improves the ones obtained for EmoSpanishDB and thereof, we recommend to follow the methodology that was used for the creation of emotional databases. © 2023, The Author(s).",13093-13112,10.1007/s11042-023-15959-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163718233&doi=10.1007%2fs11042-023-15959-w&partnerID=40&md5=c39e1b7c56a071d915c735882f1195f4,2021
DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis,"In this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single 'ground truth' label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants' ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms).  © 2023 IEEE.",-,10.1109/ACII59096.2023.10388135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184661577&doi=10.1109%2fACII59096.2023.10388135&partnerID=40&md5=01b83276292b03f475e4a075f820da19,2023
Contactless Palmprint Recognition for Children,"Effective distribution of nutritional and healthcare aid for children, particularly infants and toddlers, in the world's least developed and most impoverished countries, is a major problem due to lack of reliable identification documents. We present a mobile-based contactless palmprint recognition system, Child Palm-ID, which meets the requirements of usability, cost, and accuracy for child recognition. On a contactless child palmprint database, Child-PalmDB1, with 1,020 unique palms (age range of 6 mos. to 48 mos.), Child Palm-ID achieves a TAR=94.8% at FAR=0.1 %. Child Palm-ID is also able to recognize adults, achieving a TAR=99.5% on the CASIA contactless palmprint database and a TAR=100% on the COEP contactless adult palmprint database, both at FAR=0.1 %. For child palmprint images captured at an interval of five months with differences in standoff distance, illumination and motion blur, the TAR drops to 80.5% at FAR=0.1 %. This indicates that more research opportunities remain in contactless child palmprint recognition.  © 2023 IEEE.",-,10.1109/BIOSIG58226.2023.10345994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182397332&doi=10.1109%2fBIOSIG58226.2023.10345994&partnerID=40&md5=fb9c3a44fe712e6908b338f6a727b583,2023
Compression of high-sampling-rate heart sound signals based on downsampling and pattern matching,"Today, auscultation is one of the most effective methods in monitoring heart disease. With the advancement of technology and the facilitation of telecare on the one hand, and the increasing need for high quality and long-term recording of cardiac audio signals on the other hand, the amount of data generated has increased and therefore, the storage and transmission of these signals has become a challenge. This, in turn, demonstrates the importance and necessity of using efficient methods for compression of these signals. These methods should have a high compression ratio and, at the same time, preserve important clinical information as much as possible. In this paper, a lossy compression method is proposed for phonocardiography (PCG) signals recorded at a relatively high sampling rate so that it can control the quality of the compressed signal. This method is based on two techniques: ""two-stage downsampling"" and ""pattern matching (PM)"". The proposed two-stage downsampling technique increases the amount of compression ratio and at the same time, reduces the computational complexity. The PM technique is able to reduce the inter-period redundancy and therefore, further increase the compression ratio. The simulation results of the proposed method on two databases of the University of Michigan and the University of Washington showed that the two-stage downsampling and PM techniques have a large contribution in increasing the compression ratio. The performance of the proposed method was evaluated according to the PRD and CR criteria and compared with that of some existing methods. In this evaluation, for the PRD range of ≤5%, the CR value was between 2500 and 3900 for the University of Michigan database and between 2500 and 4125 for the University of Washington database. Also, the results of applying the proposed method on the PASCAL database showed that the efficiency of the proposed method depends to a large extent, on the quality and regularity of the PCG signal. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",201-226,10.1007/s11042-023-15714-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159350722&doi=10.1007%2fs11042-023-15714-1&partnerID=40&md5=99c9d7fd62f00c6825e1a10dc2f1be2e,2022
P2D: A Transpiler Framework for Optimizing Data Science Pipelines,"In this paper, we propose a transpilation-based approach to optimize data science pipelines that comprise database management systems (DBMSes) and data science runtimes (e.g., Python). Our approach allows to identify DBMS-supported operations and translate them into SQL to leverage DBMSes for accelerating data science workloads. The optimization target is twofold: First, to improve data loading, by reducing the amount of data to be transferred between runtimes. Second, to exploit DBMS processing capabilities by ""pushing down""certain pre-processing operations. Our optimizations are based on an intermediate representation, which allows supporting different data science libraries and DBMSes as frontends and backends respectively, making it suitable for different data science pipelines. Our evaluation with real-world and synthetic datasets shows that our approach can accelerate data science workloads by up to an order of magnitude over state-of-the-art approaches. © 2023 ACM.",-,10.1145/3595360.3595853,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168344646&doi=10.1145%2f3595360.3595853&partnerID=40&md5=6b67283836b142dae1280a6f03a4d02b,2023
Integrating Machine Learning into SQL with Exasol,"This paper introduces a novel, extendable, no-code framework for integrating machine-learning algorithms into SQL using the Exasol database. The framework combines the strengths of the high-performance, parallel-processing analytical Exasol database with the flexible and sophisticated machine learning algorithms of the Python library Scikit-Learn, while providing a seamless integration into SQL. This paper explores the technical background, the concept, and the implementation of the framework. The CREATE MODEL command for creating a machine learning model and the PREDICT function for prediction using a pre-trained model are discussed in detail. The main contributions of the framework are its seamless integration into SQL, scalability, and leveraging of existing database infrastructure. An overview of related work is also given. © 2023 Copyright for this paper by its authors.",73-85,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184662746&partnerID=40&md5=c8e593d0ea9a2dee05a89eaf101d66bc,2021
A Comparison of End-to-End Decision Forest Inference Pipelines,"Decision forest, including RandomForest, XGBoost, and LightGBM, dominates the machine learning tasks over tabular data. Recently, several frameworks were developed for decision forest inference, such as ONNX, TreeLite from Amazon, TensorFlow Decision Forest from Google, HummingBird from Microsoft, Nvidia FIL, and lleaves. While these frameworks are fully optimized for inference computations, they are all decoupled with databases and general data management frameworks, which leads to cross-system performance overheads. We first provided a DICT model to understand the performance gaps between decoupled and in-database inference. We further identified that for in-database inference, in addition to the popular UDF-centric representation that encapsulates the ML into one User Defined Function (UDF), there also exists a relation-centric representation that breaks down the decision forest inference into several fine-grained SQL operations. The relation-centric representation can achieve significantly better performance for large models. We optimized both implementations and conducted a comprehensive benchmark to compare these two implementations to the aforementioned decoupled inference pipelines and existing in-database inference pipelines such as SparkSQL and PostgresML. The evaluation results validated the DICT model and demonstrated the superior performance of our in-database inference design compared to the baselines. © 2023 Copyright held by the owner/author(s).",200-215,10.1145/3620678.3624656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178511826&doi=10.1145%2f3620678.3624656&partnerID=40&md5=e8fa85ae0307491ed1afbfe39888327a,2021
Privacy preserving rare itemset mining,"In recent years, rare pattern mining has shown great vitality in some real-world fields, such as disease diagnosis, criminal behavior analysis, anomaly detection in networks, and so on. When data organizations publish or share information publicly, shared data can be at risk of leakage as data mining techniques may discover sensitive knowledge and information. To keep competitors from obtaining hidden information after processing the database, privacy-preserving data mining (PPDM) has been proposed and studied widely. However, most of the techniques in PPDM are applied to frequent pattern mining and cannot deal with the privacy protection problems in rare pattern mining, such as network vulnerability detection and abnormal medical data. To address this limitation, we introduce a privacy-preserving technique for rare pattern mining. In this paper, two novel algorithms named Longest Transaction-Minimum Item Number (LT-MIN) and Longest Transaction-Maximum Item Number (LT-MAX) are proposed to hide sensitive rare itemsets and return the sanitized database. These two algorithms succeed in hiding target itemsets while minimizing the side effects on the original database. What's more, they employ a projection mechanism to reduce the time spent scanning the database. Besides using the traditional evaluation criteria in PPDM, we also propose two additional similarity measures to evaluate the performance from the perspective of the itemsets and the structural integrity of the database. The experimental results indicate that the proposed algorithms can hide sensitive rare itemsets successfully and efficiently, and the evaluation methods used can become the evaluation criteria for privacy-preserving rare itemset mining (PPRIM). © 2024 Elsevier Inc.",-,10.1016/j.ins.2024.120262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184016399&doi=10.1016%2fj.ins.2024.120262&partnerID=40&md5=b858a58e47abb836f9b4f0b59419c0b3,2020
A Pattern Mining-Based False Data Injection Attack Detector for Industrial Cyber-Physical Systems,"The implication of cyber-physical systems into industrial processes has introduced some security breaches due to the lack of security mechanisms. This article aims to come up with a novel methodology to detect false data injection attacks on cyber-physical systems. To reach this goal, we propose an efficient anomaly-based approach for detecting false data injection attacks against industrial cyber-physical systems. Particularly, we use sequential pattern mining techniques, which are commonly used for learning most important patterns of a system. In our case, the frequent pattern learning algorithm is used to create a database corresponding to the normal operation of the system, then, this database is fed into an attack detection algorithm in order to alert the user whenever an attack is occurring. The extensive simulations prove that our attack detection approach is able to detect attacks with a great accuracy and that this methodology could work even for large scale systems.  © 2005-2012 IEEE.",2969-2978,10.1109/TII.2023.3297139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167823808&doi=10.1109%2fTII.2023.3297139&partnerID=40&md5=1b98899393d8ee65f53b43851149bb62,2021
Preserving Individual Privacy from Inference Attack in Transaction Data Publishing,"Recently, industries such as retail and e-commerce companies have collaborated in analyzing transaction databases. It is suggested that the activity can bring various benefits for the companies since hidden information can be revealed from the analysis processes. Understanding the reality that the transaction database is more likely to contain sensitive information such as personal sensitive information, a measure to protect the sensitive information before the data is shared among parties is becoming more crucial. The sensitive information can be leaked by an attacker using an inference attack technique. An approach called a data anonymization scheme that alters an original database to preserve sensitive information can be employed to hinder the attack. Several existing approaches have been suggested to prevent the attack by anonymizing databases. However, it excessively modifies the database in protecting the sensitive information. Consequently, the modified database loses its data utility and changes the database properties significantly. In this paper, we propose a data anonymization method for preserving sensitive information from inference attacks while reducing the amount of data utility loss and maintaining database properties. Experiment evidence suggests that the proposed method outperforms an existing approach which based on global suppression technique i.e., direct algorithm.  © 2023 IEEE.",-,10.1109/ICIC60109.2023.10381942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183465902&doi=10.1109%2fICIC60109.2023.10381942&partnerID=40&md5=1ba4037ba7a69df4ab0e5e4cf4a123d8,2023
RibesDB: A Time-Series Database at Edge for the Industrial Internet of Things,"Industrial IoT systems combined with edge computing play a growing role in the digital transformation of manufacturing companies. In actual implementations, the heart of the system is a time-series database located at the edge. To this end, we have designed and implemented RibesDB, which can be deployed at the edge to provide a real-Time data source for field management, while incorporating AI models that can analyse field data in real-Time to detect and predict equipment malfunctions in the field. © 2023 IEEE.",146-150,10.1109/MICC59384.2023.10419720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186124262&doi=10.1109%2fMICC59384.2023.10419720&partnerID=40&md5=6aa124fd235385008a5b4c3ef157c20a,2021
Design and Research of Intelligent Operation and Maintenance Monitoring Platform for Database System,"With the continuous enrichment and development of power business types and quantities, as well as the pilot application of domestic databases and self-developed databases, the database type of State Grid Corporation of China has changed from the traditional single database to the open source, multi-type and domestic database. It is urgent to monitor the database in a unified way, and it is becoming more and more important to realize the intelligence and automation of database monitoring. In view of the above problems, State Grid Henan electric power company has developed and deployed the database intelligent operation and maintenance monitoring platform. By introducing the machine learning model and intelligent algorithm, a database monitoring platform with in-depth IT health inspection service is provided to comprehensively ensure the safe and stable operation of the database system. © 2023 IEEE.",637-641,10.1109/ICDACAI59742.2023.00127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182402299&doi=10.1109%2fICDACAI59742.2023.00127&partnerID=40&md5=1e0bbab47fad127a8b5dd3db41c8e6b3,2021
Design and optimization of an open personalized human-computer interaction system for New Year Painting based on the learner's model,"With the rapid development of information technology such as big data and learning analytics, intelligent systems, a product of the deep integration of technology and education, have emerged. In this paper, a human-computer interaction teaching system for traditional New Year Painting is proposed based on the learner model. Firstly, the attention mechanism based long and short term memory network is used to mine the emotion from the course review text of learners, and the association rule algorithm and ID3 algorithm are used to initialize and dynamically update the text. Constructing a personalized HCI teaching system with the learner as the center. Based on the smart learning model, the functional modules of the human-computer interaction teaching system are analyzed and designed in detail, including online learning, online testing and educational information. The design of the database of the intelligent teaching system is proposed, and the design process of the database is fully demonstrated in terms of both database relationship design and database table structure design, taking into account the security of the database. Finally, the learner model and personalized human-computer interaction system that incorporate the emotions of this paper are tested for performance, and the results show that the prediction accuracy of this paper's model is about 3 % higher than the standard model DKT on the 2009 dataset, about 3 % higher than the standard model DKT on the AUC index, and about 4 % lower than the standard model DKT on the RMSE index. Students learn through the personalized human-computer interaction system, and their mastery of the traditional art of New Year's Paintings is more thorough, and the learning effect is significantly improved. © 2023",-,10.1016/j.sasc.2023.200070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183887267&doi=10.1016%2fj.sasc.2023.200070&partnerID=40&md5=54e8d86e18af15b4ae8e6aae5acd51f8,2021
Parkinson's Disease Classification from /a/ Vowel : A Two Databases Comparison Study,"Parkinson's disease (PD) is a progressive neurological syndrome characterized by the degeneration of dopaminergic neurons responsible for movements. This decrease in dopamine causes the motor symptoms, such as tremors and muscle rigidity. Other non-motor symptoms may be observed such as speech impairments. Early detection of these vocal alterations in PD plays an essential role in its diagnosis. In this study, we propose a classification of Parkinsonian voice based on a comparative analysis of the vowel /a/, from two different databases. Through data visualization, we observed different statistical and acoustic characteristics for the same vowel. The results highlight an important conclusion: classifiers efficiency does not depend on the database used. Consistent classification performance was achieved while maintaining similar levels of accuracy, regardless of the specific characteristics of each dataset.  © 2023 IEEE.",-,10.1109/AMCAI59331.2023.10431516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186712014&doi=10.1109%2fAMCAI59331.2023.10431516&partnerID=40&md5=55c9bdeaf9b826c2c788066374205c9c,2020
Conditional Independence on Semiring Relations,"Conditional independence plays a foundational role in database theory, probability theory, information theory, and graphical models. In databases, a notion similar to conditional independence, known as the (embedded) multivalued dependency, appears in database normalization. Many properties of conditional independence are shared across various domains, and to some extent these commonalities can be studied through a measure-theoretic approach. The present paper proposes an alternative approach via semiring relations, defined by extending database relations with tuple annotations from some commutative semiring. Integrating various interpretations of conditional independence in this context, we investigate how the choice of the underlying semiring impacts the corresponding axiomatic and decomposition properties. We specifically identify positivity and multiplicative cancellativity as the key semiring properties that enable extending results from the relational context to the broader semiring framework. Additionally, we explore the relationships between different conditional independence notions through model theory. © Miika Hannula.",-,10.4230/LIPIcs.ICDT.2024.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188651809&doi=10.4230%2fLIPIcs.ICDT.2024.20&partnerID=40&md5=cacf90fb94c3a8184e971085da70569e,2021
Explainable Data Imputation using Constraints,"Data values in a dataset can be missing or anomalous due to mishandling or human error. Analysing data with missing values can create bias and affect the inferences. Several analysis methods, such as principle components analysis or singular value decomposition, require complete data. Many approaches impute numeric data and some do not consider dependency of attributes on other attributes, while some require human intervention and domain knowledge. We present a new algorithm for data imputation based on different data type values and their association constraints in data, which are not handled currently by any system. We show experimental results using different metrics comparing our algorithm with state of the art imputation techniques. Our algorithm not only imputes the missing values but also generates explanations describing the significance of attributes used for every imputation.  © 2023 ACM.",128-132,10.1145/3570991.3571009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146150634&doi=10.1145%2f3570991.3571009&partnerID=40&md5=7f5310a6ac260d4d277665cf508967e6,2020
A transferable in-silico augmented ischemic model for virtual myocardial perfusion imaging and myocardial infarction detection,"This paper proposes an innovative approach to generate a generalized myocardial ischemia database by modeling the virtual electrophysiology of the heart and the 12-lead electrocardiography projected by the in-silico model can serve as a ready-to-use database for automatic myocardial infarction/ischemia (MI) localization and classification. Although the virtual heart can be created by an established technique combining the cell model with personalized heart geometry to observe the spatial propagation of depolarization and repolarization waves, we developed a strategy based on the clinical pathophysiology of MI to generate a heterogeneous database with a generic heart while maintaining clinical relevance and reduced computational complexity. First, the virtual heart is simplified into 11 regions that match the types and locations, which can be diagnosed by 12-lead ECG; the major arteries were divided into 3–5 segments from the upstream to the downstream based on the general anatomy. Second, the stenosis or infarction of the major or minor coronary artery branches can cause different perfusion drops and infarct sizes. We simulated the ischemic sites in different branches of the arteries by meandering the infarction location to elaborate on possible ECG representations, which alters the infraction's size and changes the transmembrane potential (TMP) of the myocytes associated with different levels of perfusion drop. A total of 8190 different case combinations of cardiac potentials with ischemia and MI were simulated, and the corresponding ECGs were generated by forward calculations. Finally, we trained and validated our in-silico database with a sparse representation classification (SRC) and tested the transferability of the model on the real-world Physikalisch Technische Bundesanstalt (PTB) database. The overall accuracies for localizing the MI region on the PTB data achieved 0.86, which is only 2% drop compared to that derived from the simulated database (0.88). In summary, we have shown a proof-of-concept for transferring an in-silico model to real-world database to compensate for insufficient data. © 2024",-,10.1016/j.media.2024.103087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182876840&doi=10.1016%2fj.media.2024.103087&partnerID=40&md5=84e7ee127fe7de21092349de27c71aa2,2022
"Teaching Database Security in an Undergraduate Database Administration Course Serving Computer Science, Information Technology and Cybersecurity Students","With increasing cyberattacks in today's data-driven world, the demand for database security professional is increasing. Teaching an advanced topic such as database security at undergraduate level can be challenging. This work reports our experience teaching database security topics in a recently developed database administration course. Different methods to ensure database security such as data encryption, authentication, authorization, and change tracking are covered. Relevant hands-on demonstrations are developed to help students better understand the concepts and gain real-world insights into database security. © 2022 Owner/Author.",1406-,10.1145/3545947.3576350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149746461&doi=10.1145%2f3545947.3576350&partnerID=40&md5=2afd0402a94e7db45888a480d9dc8a56,2020
"Explain Ability, Fairness and Trust in Data Systems and Analysis","From credit scoring algorithms to face recognition systems, data-driven technologies are already impacting our lives and choices. To guarantee that new technologies function for the advantage of everyone, rather than perpetuating prejudices and discrimination, we have to address the ability, fairness, and trustworthiness of data systems and analysis. In literature, ability of data systems and analysis is defines as the capability of data systems to capture, store, retrieve, and manipulate data in large databases. Fairness of data systems means making sure that the usage of data and analytics does not result in biased or discriminating results for individuals or organizations. Trust is an essential factor in the effectiveness and acceptability of data systems and analysis. Without trust, people may be unwilling to utilize or disclose their data with such systems, hampering their ability to give useful insights and take advantages of data systems. To analyze, the ability, fairness, and trust in data systems and analysis, researchers have used secondary, qualitative data. Researchers have used scholarly publications, research papers, and reports taken from credible and peer-reviewed research journals. Using the data, researchers came to the result that data systems analysis have enhanced decision making process and helped us to improve our productivity in daily-life activities, through its capabilities of capturing, stories, retrieving and manipulating data. Yet, there are issues regarding the fairness of data systems. While progress achieved in tackling fairness issues, trust issues concerning data systems and analysis exist owing to data breaches suffered by huge databases globally. To conclude, in order to create trust in data systems and analysis, firms need to concentrate on security, data quality, transparency and accountability of data. This requires implementing strong access limits, leveraging high-quality data sources, being transparent and honest about data collecting and analysis, and investing in training and development programmes.  © 2023 IEEE.",-,10.1109/CATS58046.2023.10424267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186140998&doi=10.1109%2fCATS58046.2023.10424267&partnerID=40&md5=df0c0a1131fc6e395ca94660c5b25a72,2023
Highly Efficient Machine Learning Approach for Automatic Disease and Color Classification of Olive Fruits,"The following ends have been established via an in-depth examination and assessment of numerous prior studies on olive fruit classifications: First, several of these researches rely on the use of an unrelated image library. Since every image features a single fruit with a background that contrasts sharply with the fruit's hue, they are all ready for testing. As was previously stated, this issue is unrelated to reality. In practical application, one must deal with a frame that holds hundreds of fruits. To keep the fruits steady, they are put on a conveyor with multiple channels. It's also notable that the majority of this study offered suggestions for useful technology that could yet be developed. Finally, it is important to emphasize that processing speed data is essential in this type of application and has not been collected in many of these experiments. The presented work deals with a new strategy based on two principles: first, a successful extraction of the fruits from the background; and second, the classification of olive fruits into eight categories based on colors and defects. The fruits were extracted from the backdrop using a modified version of the K-Means technique. The outcomes of the suggested fruit extraction were examined utilizing several assessment techniques. By contrasting the outcomes of pertinent procedures with the suggested proposal for fruit extraction, the efficacy and precision of the proposed method were verified. Depending on why the fruit needed to be separated, there were two stages to the process. Three colors were separated using the SVM algorithm, and five distinct defects were separated using the ANN algorithm Approximately 15,000 photos of olive fruits that were shot straight from the fruit conveyor were included in a robust database that was used in the proposed study to validate the effectiveness of the suggested technology. Efficiency was further validated by contrasting our outcomes with those of related technology. When the fruits were set on a white backdrop, the test accuracy results of the suggested approach showed that it was highly efficient in classifying the fruits in the shortest period; the suggested method had an effectiveness of 99.26%for fruit classification. The most important discovery was that it could classify fruits with an efficiency of 97.25%while they were being put on a fruit conveyor, which was in contrast to other approaches. The unique findings of the study that was presented hold promise for practical implementation.  © 2013 IEEE.",35683-35699,10.1109/ACCESS.2024.3362294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184797923&doi=10.1109%2fACCESS.2024.3362294&partnerID=40&md5=2e1d7b4e2d08e0bbe7874f02f67515cf,2021
Research on the development of a digital conservation system for cultural relics based on image recognition technology,"The system takes excavated cultural relics at archaeological sites during the excavation period as the research object, and composes the current situation of cultural relic protection during the excavation period. It proposes to build a 3D/2D model library of excavated cultural relics based on 3D laser scanning and imaging, data reconstruction, 3D modelling, 3D/2D graphics conversion and database; to realise a series of digital conservation such as accurate collection of archival data of excavated cultural relics and scientific management of cultural relics database, to shorten excavation cycle and improve excavation efficiency, and to provide theoretical basis and practical guidance for digital conservation of excavated cultural relics. © 2023 ACM.",55-59,10.1145/3592686.3592756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162233555&doi=10.1145%2f3592686.3592756&partnerID=40&md5=bdc7b7c6e5f2a3b423ddd8ff30db5333,2023
Automatically Identifying CVE Affected Versions with Patches and Developer Logs,"While vulnerability databases are important sources of information for software security, it is known that information in these databases is inconsistent. How to rectify these incorrect data is a challenging issue. In this article, we employ developer logs and patches to automatically identify vulnerable source code versions that each CVE really affects. Our tool organizes all versions of a piece of software into a version tree, and identifies the first vulnerable version, and the last vulnerable versions in the version tree trunk and branches. For evaluation, we took Linux Kernel as the case study and quantified the error rate of the vulnerable versions reported by the NVD. The total number of vulnerable Linux Kernel versions reported by the NVD was 43,727 (as of September 2020), of which the total number of false positives reached 2,497 and the total number of false negatives reached 9,330, accounting for 5.7% and 21.34%, respectively. In addition, we compare our tool with two vulnerability detection tools and show that our tool could achieve high detection accuracy.  © 2004-2012 IEEE.",905-919,10.1109/TDSC.2023.3264567,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153387068&doi=10.1109%2fTDSC.2023.3264567&partnerID=40&md5=eeab4646bf4e2ba52b93fef188eef445,2023
Web-based E-Commerce Customer Segmentation System Using RFM and K-Means Model,"Database marketing has gained significant importance in today's business landscape due to the availability of vast customer data held by many companies. This has led to the widespread adoption of database marketing across various industries, such as retail, e-commerce, banking, and telecommunications. Continuously understanding customer behavior is crucial as their trends and preferences evolve over time. This study aims to support companies in enhancing customer loyalty, retention, and fostering long-term relationships by better understanding their customers. To achieve this, companies can monitor customer behavior and adjust their marketing strategies, one effective approach being the utilization of Recency, Frequency, and Monetary (RFM) analysis. RFM analysis, which stands for Recency, Frequency, and Monetary, is a technique used to segment customers based on their past purchasing behavior. Recency assesses how recently a customer has made a purchase, Frequency measures the frequency of a customer's purchases, and Monetary evaluates the amount a customer spends per purchase. In this study, the K-Means unsupervised learning algorithm is employed to categorize customers into three groups: Loyal, Promising, and Need Attention based on Recency, Frequency, and Monetary features. The experimental findings indicate that the K-Means algorithm outperforms K-Medoids for customer segmentation, as evidenced by higher values for the Silhouette Coefficient (0.74), Davies Bouldin Index (0.51), and Calinski Harabasz Index (8972.97). To facilitate the interpretation of these results, a web dashboard was created using the Streamlit Python library. By visualizing the analysis outcomes through this platform, companies can gain valuable insights into customer behavior and preferences. © 2023 IEEE.",83-87,10.1109/3ICT60104.2023.10391650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184666161&doi=10.1109%2f3ICT60104.2023.10391650&partnerID=40&md5=6e9433f52d74a75d569b43c8300f9c98,2023
Deep learning-based restoration of multi-degraded finger-vein image by non-uniform illumination and noise,"The recognition performance deteriorates if degradation factors including blur, noise, and non-uniform illumination exist in the image when acquiring a finger-vein image. Especially, multiple degradation factors can occur when acquiring the finger-vein image, and they require the image restoration. However, previous flow-based model produced lower image quality than the other restoration models, and diffusion-based model had the disadvantage of slow inference speed. Therefore, this study suggests a deep learning-based generative adversarial network for multi-degraded finger-vein image restoration by non-uniform illumination and noise (MFNN-GAN). It considers multiple degradation factors such as non-uniform illumination and noise. Unlike the existing finger-vein image restoration model, MFNN-GAN is capable of adaptive restoration to multiple degradations. Therefore, even if the illumination by near-infrared (NIR) illuminator of finger-vein recognition device is weak or non-uniform, or the consequent captured image is noisy, good recognition performance can be achieved only by our method without replacing the illuminator or camera sensor. The experimental results obtained using finger-vein open datasets, session 1 images from database version 1 of the Hong Kong Polytechnic University finger-image (HKPU-DB) and finger-vein database of SDUMLA-HMT (SDUMLA-HMT-DB)-based degraded databases. The experimental results show that we obtained the lower equal error rate (EER) of finger-vein recognition using MFNN-GAN compared to other state-of-the-art algorithms. © 2024 The Authors",-,10.1016/j.engappai.2024.108036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185528521&doi=10.1016%2fj.engappai.2024.108036&partnerID=40&md5=e1d6cd1a85ea454f3503bac1b7190b2b,2023
Management of climate protection measures in peatland areas in Schleswig-Holstein: Development of a web application supporting the management of data about peatlands,"The peatland database application Schleswig-Holstein is presented, a web application for the management of geospatial and non-geospatial data covering peatland protection, which was designed and implemented on behalf of the nature conservation departments of the Ministry for Energy Transition, Climate Protection, Environment and Nature of Schleswig-Holstein (MEKUN)and the State Agency for the Environment of Schleswig-Holstein (LfU). The development of the web application was based on the IT requirements for the collection, research, evaluation, and reporting of peatland data. Other applications already used by the nature conservation authorities, such as a nature conservation measures database and a protected areas register, were linked to the peatland database. The implementation was carried out as a PHP application that was integrated into the Disy Cadenza evaluation and GIS platform. Particular challenges included dynamic integration of measures from the nature conservation measures database. For restoration measures, a function is provided that automatically determines the reduction of CO2emissions. © 2023 Gesellschaft fur Informatik (GI). All rights reserved.",143-152,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183879601&partnerID=40&md5=ec2568ca8b1db8570c611500183b6d7d,2024
SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization,"Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamically-reconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs). ROMAC clusters RPs into different feature sets and therefore selects suitable monitor points (MPs) for enhancing location estimation. Moreover, the CODE algorithm aims for establishing adaptive fingerprint database to alleviate the time-varying problem. Finally, CsLE is adopted to acquire the target position by leveraging the benefits of clustering information and estimated signal variations in order to rescale the weights from weighted k-nearest neighbors (WkNN) method. Both simulation and experimental results demonstrate that the proposed SALC system can effectively reconstruct the fingerprint database with an enhanced location estimation accuracy, which outperforms the other existing schemes in the open literature.  © 2013 IEEE.",439-452,10.1109/TNSE.2023.3300768,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166781881&doi=10.1109%2fTNSE.2023.3300768&partnerID=40&md5=abc7d89dabcdd377553c6dd7256fc4d9,2021
Blind Image Quality Index With Cross-Domain Interaction and Cross-Scale Integration,"With the assistance of Convolutional Neural Networks (CNNs), Image Quality Assessment (IQA) models have made great progress in evaluating both simulated distortion and authentic distortion. However, most of the existing IQA models only learn the features of distorted images, and thus do not make full use of the available feature representation of other domains. Furthermore, the common multi-scale fusion strategies are relatively simple, such as downsampling and concatenating, which further limits the prediction performance. To this end, we propose a novel blind image quality index with cross-domain interaction and cross-scale integration, which is designed based on the combination of CNN and Transformer. First, the hierarchical spatial-domain and gradient-domain representations are obtained through a typical CNN architecture. Then, based on the proposed gradient-query cross-attention, these two types of features are fully interacted in the Cross-Domain Interaction (CDI) module. To represent the distortion information more comprehensively, the Cross-Scale Integration (CSI) module is proposed to combine the information between different scales progressively. Finally, the quality score is obtained through a simple regression module. The experimental results on five public IQA databases of both simulated and authentic scenes show that the proposed model outperforms the compared state-of-the-art metrics. In addition, cross-database experiments show that the proposed model has strong generalization performance.  © 1999-2012 IEEE.",2729-2739,10.1109/TMM.2023.3303725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167815276&doi=10.1109%2fTMM.2023.3303725&partnerID=40&md5=0cf98d8ae74277cd9be8128eb2de5798,2024
Database-Integrated Machine Learning for Enhanced Performance,"Our study presents a novel database-integrated machine learning framework aimed at bridging the gap between database systems and machine learning, addressing data transfer costs and performance concerns. The framework introduces a machine learning management system between the database client and server, effectively storing and managing fine-grained operations such as computation graphs, derivatives, calculations, and model updates within user-defined functions in the database. These operations are encapsulated using Python, enabling precise control and ease of programming in the machine learning process. Additionally, the framework supports parallel computing and performance optimization, offering outstanding performance comparable to dedicated machine learning frameworks. Most importantly, it provides a programming experience akin to traditional machine learning frameworks, allowing developers to disregard the database's presence and focus on machine learning tasks. This research introduces a promising solution in the field of in-database machine learning, with the potential for far-reaching impacts in data-driven applications. © 2023 IEEE.",203-209,10.1109/BigDIA60676.2023.10429411,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186690267&doi=10.1109%2fBigDIA60676.2023.10429411&partnerID=40&md5=4018f542d97032e364e9f8547b01716b,2023
Parallel Acyclic Joins: Optimal Algorithms and Cyclicity Separation,"We study equi-join computation in the massively parallel computation (MPC) model. Currently, a main open question under this topic is whether it is possible to design an algorithm that can process any join with load O(N polylog N/p1/ρ∗ ) — measured in the number of words communicated per machine — where N is the total number of tuples in the input relations, ρ∗ is the join’s fractional edge covering number, and p is the number of machines. We settle the question in the negative for the class of tuple-based algorithms (all the known MPC join algorithms fall in this class) by proving the existence of a join query with ρ∗ = 2 that requires a load of Ω(N/p1/3) to evaluate. Our lower bound provides solid evidence that the “AGM bound” alone is not sufficient for characterizing the hardness of join evaluation in MPC (a phenomenon that does not exist in RAM). The hard join instance identified in our argument is cyclic, which leaves the question of whether O(N polylog N/p1/ρ∗ ) is still possible for acyclic joins. We answer this question in the affirmative by showing that any acyclic join can be evaluated with load O(N/p1/ρ∗ ), which is asymptotically optimal (there are no polylogarithmic factors in our bound). The separation between cyclic and acyclic joins is yet another phenomenon that is absent in RAM. Our algorithm owes to the discovery of a new mathematical structure — we call “canonical edge cover” — of acyclic hypergraphs, which has numerous non-trivial properties and makes an elegant addition to database theory. © 2024 Association for Computing Machinery. All rights reserved.",-,10.1145/3633512,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186142882&doi=10.1145%2f3633512&partnerID=40&md5=99382681d318fc00141bad5ddd2f8c77,2022
Protecting Data Integrity of Web Applications with Database Constraints Inferred from Application Code,"Database-backed web applications persist a large amount of production data and have high requirements for integrity. To protect data integrity against application code bugs and operator mistakes, most RDBMSes allow application developers to specify various types of integrity constraints. Unfortunately, applications (e.g., e-commerce web apps) often do not take full advantage of this capability and miss specifying many database constraints, resulting in many severe consequences, such as crashing the order placement page and corrupting the store inventory data. In this paper, we focus on the problem of missing database constraints in web applications. We first study several widely used open-source e-commerce and communication applications, and observe that all these applications have missed integrity constraints and many were added later as afterthoughts after issues occurred. Motivated by our observations, we build a tool called CFinder to automatically infer missing database constraints from application source code by cleverly leveraging the observation that many source code patterns usually imply certain data integrity constraints. By analyzing application source code automatically, CFinder can extract such constraints and check against their database schemas to detect missing ones. We evaluate CFinder with eight widelydeployed web applications, including one commercial company with millions of users. Overall, our tool identifies 210 previously unknown missing constraints. We have reported 92 of them to the developers of these applications, so far 75 are confirmed. Our tool achieves a precision of 78% and a recall of 79%.  © 2023 ACM.",632-645,10.1145/3575693.3575699,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147730589&doi=10.1145%2f3575693.3575699&partnerID=40&md5=d31764050389649b9a1cbe7cd8e2f901,2024
Update Algorithm of Secure Computer Database Based on Deep Belief Network,"In order to ensure the security of large-scale data transmission in a short time and in a wide range during online database updating, this paper presents a secure computer database updating algorithm based on DBN (Deep Belief Network). In this paper, the model adopts multi-layer depth structure for unsupervised feature learning, maps high-dimensional and nonlinear intrusion data to low-dimensional space, establishes the relationship mapping between high-dimensional and low-dimensional, and then uses fine-tuning algorithm to transform the model to achieve the best expression of features. At the same time, this method improves the data processing and method model without destroying the learned knowledge of the model and seriously affecting the real-time performance of detection. In order to overcome the problem of system instability caused by fixed empirical learning rate, this paper proposes a learning rate optimization strategy based on energy change. In the process of feature extraction, the features of different hidden layers are extracted to form combined features. Experiments show that the detection rate of this method can reach 95.31%, and the false alarm rate is 2.14%. This verifies the effectiveness of the secure computer database updating algorithm in this paper. Which can ensure the online update of the secure computer database.  © 2023 River Publishers.",1-26,10.13052/jcsm2245-1439.1311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180268403&doi=10.13052%2fjcsm2245-1439.1311&partnerID=40&md5=0ba6d8d19f18bb4023e7930612f923aa,2021
DITune: A Reinforcement Learning Based Framework for Automated Database Index Selection,"Index selection is a crucial and essential aspect of database management systems (DBMS) aiming to optimize performance, and it is also one of the most time-consuming and laborious tasks for database administrators (DBAs). Therefore, automatic methods and tools are urgently needed to help DBAs choose a good index configuration for workloads. We propose a recommender system named DITune (Database Index Tuning) which can automatically analyze workloads, explore the potential benefits of indexes, and adjust the index configuration. We first implement a rule-based method to embed the database workload and data distribution feature. Then we utilize the Deep Deterministic Policy Gradient (DDPG) algorithm to generate index recommendations and construct index configuration. Moreover, we design a benchmark based on the standard TPC-C to validate the performance of our method under the varying workload patterns, query complexity, and running phase length. The experiment results show the effectiveness and robustness of DITune and indicate that the data-driven prediction outperforms those models based on cost estimation. © 2024 IEEE.",-,10.1109/IMCOM60618.2024.10418292,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186143539&doi=10.1109%2fIMCOM60618.2024.10418292&partnerID=40&md5=54824c47e880a6ce2ebf1d39bbb455d7,2021
Aggregation Consistency Errors in Semantic Layers and How to Avoid Them,"Analysts often struggle with analyzing data from multiple tables in a database due to their lack of knowledge on how to join and aggregate the data. To address this, data engineers pre-specify ""semantic layers""which include the join conditions and ""metrics""of interest with aggregation functions and expressions. However, joins can cause ""aggregation consistency issues"". For example, analysts may observe inflated total revenue caused by double counting from join fanouts. Existing BI tools rely on heuristics for deduplication, resulting in imprecise and challenging-to-understand outcomes. To overcome these challenges, we propose ""weighing""as a core primitive to counteract join fanouts. ""Weighing""has been used in various areas, such as market attribution and order management, ensuring metrics consistency (e.g., total revenue remains the same) even for many-to-many joins. The idea is to assign equal weight to each join key group (rather than each tuple) and then distribute the weights among tuples. Implementing weighing techniques necessitates user input; therefore, we recommend a human-in-the-loop framework that enables users to iteratively explore different strategies and visualize the results.  © 2023 ACM.",-,10.1145/3597465.3605224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165577844&doi=10.1145%2f3597465.3605224&partnerID=40&md5=7e50133780d13ca51d529d82a07261a4,2020
Incremental clickstream pattern mining with search boundaries,"Recently, there has been a growing interest in sequential pattern mining in data mining, with a particular focus on clickstream pattern mining. These areas hold the potential for discovering valuable patterns. However, traditional mining algorithms in these domains often assume that databases are static, simplifying the mining process. In reality, databases are updated incrementally over time, partially rendering a portion of the previous results invalid. This necessitates rerunning algorithms on updated databases to obtain accurate frequent patterns. As database size increases, this approach can become time-consuming and affect performance. To tackle this issue, we propose PSB-CUP to mine frequent clickstream patterns in an incremental update manner. PSB-CUP employs the concept of search borders to reduce the search space and the information retained in memory. Furthermore, an IDList generation method called “partial imbalance join” was proposed to reconstruct possibly missing information during the incremental process. This join method, however, requires more extra information to be cached in exchange for speed. We then improve this technique by introducing “recursive imbalance join”, removing the need for extra cached data in the PSB-CUP + algorithm. The experimental results show that our proposed algorithms are efficient for incremental clickstream pattern mining. © 2024 Elsevier Inc.",-,10.1016/j.ins.2024.120257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185494013&doi=10.1016%2fj.ins.2024.120257&partnerID=40&md5=4a3d50c76e29f9656b4e088ff6f999fe,2021
LAMIS-DMDB: A new full field digital mammography database for breast cancer AI-CAD researches,"The current state of machine learning (ML) and deep learning (DL) shows that they are powerful tools that can be used to glean knowledge from a huge amount of data. While a great deal of research in this area has centered around improving the accuracy and efficiency of training and inference algorithms, there has been relatively little attention paid to the equally important problem of monitoring the quality of data. To tackle this challenge, we have created a cutting-edge breast database capable of employing ML and DL algorithms to detect and classify breast cancer. It is noteworthy that compared to other digital mammogram databases (DMDB), the Laboratory of Mathematics, Informatics and System (LAMIS) images have better resolution and are in full-field digital mammography (FFDM) format. Furthermore, the images can be accessed according to their abnormality (normal, benign, malignant), lesions classification (mass, calcification, architectural distortion, multiple finding), American College of Radiology (ACR) density classification (ACR1, ACR2, ACR3, ACR4) and Breast Imaging Reporting and Data System (BI-RADS). Specifically, the database is designed to provide a variety of metadata as well as clinical data in simple comma separated values (CSV) format that can be really beneficial to the breast cancer research community and to the development of computer-aided diagnosis (CADx) tools that rely on artificial intelligence (AI) algorithms, such as those based on DL models. © 2023",-,10.1016/j.bspc.2023.105823,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179755432&doi=10.1016%2fj.bspc.2023.105823&partnerID=40&md5=c0b064453f44b4a0c53bb1e9eebb64b4,2023
Accelerating DNA Sequence Analysis using content-addressable memory in FPGAs,"Biological sequence alignment is a widely used technique where the sequence databases are searched to find similar sequence to the input query. In this work we focus on the most popular local sequence alignment algorithm; Basic Local Alignment Search Tool (BLAST). It is a computationally intensive operation, and with exponential growing databases, makes it further complex to execute in real time. Field-programmable gate arrays (FPGAs) provides hardware-like performance and software-like programmability which makes them the ideal candidate for computationally complex tasks. This paper presents a content-addressable memory (CAM)-based implementation of BLAST on FPGA that accelerates the alignment process using concurrent computations. The searching of the input query is performed in parallel across the database sequence to produce the result in one clock cycle. The proposed design is implemented on Xilinx Virtex-7 FPGA device XC7VX690TFFG1761. Results indicate better feasibility and accelerated performance (149-180 MHz speed) compared to the available searching algorithms. ©2023 IEEE.",69-72,10.1109/SmartCloud58862.2023.00020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183316442&doi=10.1109%2fSmartCloud58862.2023.00020&partnerID=40&md5=3b044ff15b1ce17c8543f7a3993ef3b3,2023
Artificial Neural Network Models of Multimodal Biometric Authentication System,"As part of this research, software for multimodal biometric authentication using neural networks is developed to improve the efficiency of information system user authorization. The architectures of artificial neural networks, which are involved in the processes of recognizing a person by facial image and voice, are given. The internationally used databases (DataSet) of images and audio recordings for training of neural networks are considered. The process of training neural networks, the formed database of biometric personal data and the results obtained by the authors are described. © 2023 IEEE.",334-337,10.1109/SUMMA60232.2023.10349480,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182344711&doi=10.1109%2fSUMMA60232.2023.10349480&partnerID=40&md5=b36614f8bd9f8fa801f473744e73d74e,2020
CowScreeningDB: A public benchmark database for lameness detection in dairy cows,"Lameness is one of the costliest pathological problems affecting dairy animals. It is usually assessed by trained veterinary clinicians who observe features such as gait symmetry or gait parameters as step counts in real time. With the development of artificial intelligence, various modular systems have been proposed to minimize subjectivity in lameness assessment. However, the major limitation in their development is the unavailability of a public database, as most existing ones are either commercial or privately held. To tackle this limitation, we have introduced CowScreeningDB, a multi-sensor database which was built with data from 43 dairy cows. Cows were monitored using smart watches during their normal daily routine. The uniqueness of the database lies in its data collection environment, sampling methodology, detailed sensor information, and the applications used for data conversion and storage, which ensure transparency and replicability. This data transparency makes CowScreeningDB a valuable and objectively comparable resource for further development of techniques for lameness detection for dairy cows. In addition to publicly sharing the database, we present a machine learning technique which classifies cows as healthy or lame by using raw sensory data. To facilitate fair comparisons with state-of-the-art methods, we introduce a novel benchmark. Combining the database, the machine learning technique and the benchmark validate our major objective, which is to establish the relationship between sensor data and lameness. The developed technique reports an average accuracy of 77 % for the best case scenario and presents perspectives for further development. By introducing this framework which encompasses the database, the classification algorithm and the benchmark, we significantly reduce subjectively in lameness assessment. This contribution to lameness detection fosters innovation in the field and promotes transparent, reproducible research in the pursuit of more effective management of dairy cow lameness. Implications: Lameness detection is one of the main tasks in dairy systems, given its importance in the production ambit. However, the data used during detection is generally either held privately or sold commercially. In this study, we create a multi-sensor database (CowScreeningDB), which can be used for lameness. Because we have made the database public1 and free of charge for research purposes, it should act as a benchmark allowing to objectively compare techniques put forth to deal with lameness. We also provide details of the sampling system used, comprised of hardware and a baseline classification algorithm. © 2023 The Authors",-,10.1016/j.compag.2023.108500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179488657&doi=10.1016%2fj.compag.2023.108500&partnerID=40&md5=5fcad0a6570ca4674e09df16ef497674,2021
Design of the Configuration Engines with Ryu REST API in Database-oriented SDN Architecture,"Software-defined Networking (SDN) is a new paradigm of network management for achieving more flexible network configuration and utilizing network resources more efficiently. Our research group has proposed a database-oriented SDN architecture in which all the control information is stored in a database system. In this paper, we implement the configuration engine which generates the control information according to network settings specified by the network administrator and applies it to the switch. For this, we investigate the REST API provided by the Ryu controller, which is one of the famous OpenFlow Controllers. The engine we implemented constructs the appropriate parameters of the REST API based on the control information fetched from the database. Then, we show that the engine operates correctly and the network can be managed as intended. © 2023 Copyright held by the owner/author(s).",52-56,10.1145/3638837.3638845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187806497&doi=10.1145%2f3638837.3638845&partnerID=40&md5=572a1d2817ce13ad26cb28a365661467,2023
Leveraging Large Language Models for Automatic Hypotheses Testing over Heterogeneous Biological Databases,"An understanding of the molecular basis of musculoskeletal pain is necessary for the development of therapeutics, their management, and possible personalization. One-in-three Americans use OTC pain killers, and one tenth use prescription drugs to manage pain. The CDC also estimates that about 20% Americans suffer from chronic pain. As the experience of acute or chronic pain varies due to individual genetics and physiology, it is imperative that researchers continue to find novel therapeutics to treat or manage symptoms. In this paper, our goal is to develop a seed knowledgebase computational platform, called BioNursery, that will allow biologists to computationally hypothesize, define and test molecular mechanisms underlying pain. In our knowledge ecosystem, we accumulate curated information from users about the relationships among biological databases, analysis tools, and database contents to generate biological analyses modules, called -graphs, or process graphs. We propose a mapping function from a natural language description of a hypothesized molecular model to a computational workflow for testing in BioNursery. We use a crowd computing feedback and curation system, called Explorer, to improve proposed computational models for molecular mechanism discovery, and growing the knowledge ecosystem. © 2023 Owner/Author(s).",-,10.1145/3584371.3613022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175853625&doi=10.1145%2f3584371.3613022&partnerID=40&md5=8d0d9ca3ec626ab4fa52f8e00188b831,2020
Mapping Strategies for Declarative Queries over Online Heterogeneous Biological Databases for Intelligent Responses,"The emergence of Alexa and Siri, and more recently, OpenAI's Chat-GPT, raises the question whether ad hoc biological queries can also be computed without end-users' active involvement in the code writing process. While advances have been made, current querying architectures for biological databases still assume some degree of computational competence and significant structural awareness of the underlying network of databases by biologists, if not active code writing. Given that biological databases are highly distributed and heterogeneous, and most are not FAIR compliant, a significant amount of expertise in data integration is essential for a query to be accurately crafted and meaningfully executed. In this paper, we introduce a flexible and intelligent query reformulation assistant, called Needle, as a back-end query execution engine of a natural language query interface to online biological databases. Needle leverages a data model called BioStar that leverages a meta-knowledgebase, called the schema graph, to map natural language queries to relevant databases and biological concepts. The implementation of Needle using BioStar is the focus of this article.  © 2023 ACM.",567-574,10.1145/3555776.3577652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162882567&doi=10.1145%2f3555776.3577652&partnerID=40&md5=47ddf908581a187075bf75288d7fc8e6,2020
Pre-trained models for detection and severity level classification of dysarthria from speech,"Automatic detection and severity level classification of dysarthria from speech enables non-invasive and effective diagnosis that helps clinical decisions about medication and therapy of patients. In this work, three pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) are studied to extract features to build automatic detection and severity level classification systems for dysarthric speech. The experiments were conducted using two publicly available databases (UA-Speech and TORGO). One machine learning-based model (support vector machine, SVM) and one deep learning-based model (convolutional neural network, CNN) was used as the classifier. In order to compare the performance of the wav2vec2-BASE, wav2vec2-LARGE, and HuBERT features, three popular acoustic feature sets, namely, mel-frequency cepstral coefficients (MFCCs), openSMILE and extended Geneva minimalistic acoustic parameter set (eGeMAPS) were considered. Experimental results revealed that the features derived from the pre-trained models outperformed the three baseline features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features. In particular, when compared to the best-performing baseline feature (openSMILE), the HuBERT features showed in the detection problem absolute accuracy improvements that varied between 1.33% (the SVM classifier, the TORGO database) and 2.86% (the SVM classifier, the UA-Speech database). In the severity level classification problem, the HuBERT features showed absolute accuracy improvements that varied between 6.54% (the SVM classifier, the TORGO database) and 10.46% (the SVM classifier, the UA-Speech database) compared to the best-performing baseline feature (eGeMAPS). © 2024 The Author(s)",-,10.1016/j.specom.2024.103047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185893219&doi=10.1016%2fj.specom.2024.103047&partnerID=40&md5=6aa86d49ef22f75e677439de1de41b53,2023
Cardiovascular hardware simulator and artificial aorta-generated central blood pressure waveform database according to various vascular ages for cardiovascular health monitoring applications,"This study presents a database of central blood pressure waveforms according to cardiovascular health conditions, to supplement the lack of clinical data in cardiovascular health research, constructed by a cardiovascular simulator. Blood pressure (BP) is the most frequently measured biomarker, and in addition to systolic and diastolic pressure, its waveform represents the various conditions of cardiovascular health. A BP waveform is formed by overlapping the forward and reflected waves, which are affected by the pulse wave velocity (PWV). The increase in vascular stiffness with aging increases PWV, and the PWV-age distribution curve is called vascular age. For cardiovascular health research, extensive data of central BP waveform is essential, but the clinical data published so far are insufficient and imbalanced in quantity and quality. This study reproduces the central BP waveform using a cardiovascular hardware simulator and artificial aortas, which mimic the physiological structure and properties of the human. The simulator can adjust cardiovascular health conditions to the same level as humans, such as heart rate of 40–100 BPM, stroke volume of 40–100 mL, and peripheral resistance of 12 steps. Also, 6 artificial aortas with vascular ages in the 20–70 were fabricated to reproduce the increase in vascular stiffness due to aging. Vascular age calculated from measured stiffness of artificial aorta and central BP waveform showed an error of less than 3 years from the clinical value. Through this, a total of 636 waveforms were created to construct a central BP waveform database according to controlled various cardiovascular health conditions. © 2024 The Authors",-,10.1016/j.compbiomed.2024.108224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187204583&doi=10.1016%2fj.compbiomed.2024.108224&partnerID=40&md5=9f17701acf8389369b69ea40ae84e011,2024
T2FM: A novel hashtable based type-2 fuzzy frequent itemsets mining,"Association rule mining (ARM) is an important research issue in the field of data mining that aims to find relations among different items in binary databases. The conventional ARM algorithms consider the frequency of the items in binary databases, which is not sufficient for real time applications. In this paper, a novel hash table based Type-2 fuzzy mining algorithm (T2FM) with an efficient pruning strategy is presented for discovering multiple fuzzy frequent itemsets from quantitative databases. The algorithm employs a hash table based structure for efficient storage and retrieval of item/itemset which reduces the search efficiency to O(1) or constant time. Previously, type-2 based Apriori and FP-growth based fuzzy frequent itemsets mining were proposed, which required large amounts of computation and a greater number of candidate generation and processing. Meanwhile, the proposed approach reduces a huge amount of computation by finding the common keys before the actual intersection operation takes place. An efficient pruning strategy is proposed to avoid unpromising candidates in order to speed up the computations. Several experiments are carried out to verify the efficiency of the approach in terms of runtime and memory for different minimum support threshold and the results show that the designed approach provides better performance compared to the state-of-the-art algorithms. © 2024 – IOS Press. All rights reserved.",3231-3244,10.3233/JIFS-232918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185550369&doi=10.3233%2fJIFS-232918&partnerID=40&md5=fdbbbedde75ef5cf1f0cf4651f0feb2e,2021
Analysis and protection to user privacy in quantum private query with non-ideal light source,"Most existing quantum-key-distribution-based quantum private query (QPQ) protocols are designed based on ideal quantum communication devices. However, multiphoton pulses are unavoidable in various light sources due to practical devices. We analyze the security of the QPQ protocol with the user as the light source in the case of non-ideal light sources. The results show that the user’s privacy is seriously threatened when the database utilizes multiphoton pulses to launch attacks. To address this issue, we propose a decoy-state method to resist multiphoton attacks in the QPQ, which is used to verify the honesty of the database. By calculating the ratio of multiphoton pulses, shifting the key, and choosing the suitable light sources, the user can detect and effectively resist the database launching multiphoton attacks. This approach fills the research gap on the user-as-light-source aspect of QPQ protocols, enabling the QPQ to perform better in scenarios involving non-ideal light sources. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11128-024-04346-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188939979&doi=10.1007%2fs11128-024-04346-5&partnerID=40&md5=d941f1d691cf949f6935a34960745871,2021
Network pharmacology-based investigation of the molecular mechanisms underlying Sausselia Tianshanensis' potential antiesophageal squamous cell carcinoma activity,"The material basis of using Saussurea involucrata in the treatment of esophageal squamous cell carcinoma was studied through bioinformatics, network pharmacology, and molecular docking. The ESCC data chip GSE167488 was obtained from the GEO database. The GEO2R tool was used to identify significantly differentially expressed genes in ESCC. TCMSP was employed to screen for active ingredients and targets of Saussurea involucrata. Human genes related to ESCC were retrieved from the NCBI Gene database. The genes were intersected to obtain 28 common genes, corresponding to 7 effective components of Saussurea involucrata. PPI network and component-Target network were constructed. GO biological enrichment and KEGG pathway analysis were performed on the targets in the DAVID database to obtain cancerrelated signal transduction, immune regulation, and other disease pathways. The GEPIA database was used to analyze the survival of 20 targets, and 4 significantly different genes with different expressions in ESCC survival rate were screened. Molecular docking was performed and it was found that homoplantaginin, isoimperatorin, kaempferol, luteolin, rosarylline, and quercetin had a good combination with target MMP1, PTGS2, SERPINE1, and VCAM1.  © 2023 SPIE.",-,10.1117/12.3021729,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183031152&doi=10.1117%2f12.3021729&partnerID=40&md5=8a3724e5efc5aa92271008bb2b7c60aa,2023
Lightweight Time-Domain HRV Index Extraction from Compressive ECG Measurements for Resource Constrained Edge Health Computing Devices,"Electrocardiogram (ECG) derived heart rate variability (HRV) analysis is widely used in both physiological and psychological monitoring. For HRV analysis, an automatic R-peak to R-peak interval (RRI) determination is most essential and that is performed by processing the ECG signal. Continuous HRV analysis is most essential for predicting health problems and stress levels, but HRV analysis devices are restricted with limited battery capacity. To increase the battery lifetime of monitoring devices, we present a simple energy-efficient R-peak detection approach by directly processing a few compressive measurements instead of processing the ECG signal sampled at the Nyquist rate. The proposed CS-ECG based HRV analysis method consists of: compressed sensing (CS) based data reduction, straightforward RRI determination without the use of searchback algorithm with sets of thresholds, and the HRV parameter estimation. The accuracy and robustness of CS-ECG based time-domain HRV index extraction (7 HRV indexes) is evaluated using the MIT-BIH arrhythmia database, MIT-BIH polysomnographic database (slpdb) and Apnea-ECG database. Evaluation results show that the estimated HRV parameters from RR intervals in the CS domain are comparable with the HRV parameters estimated from the RR intervals extracted from the uncompressed ECG signal. Using deterministic binary block diagonal (DBBD) based measurement generation with a compression factor of 4, the CS-ECG based RRI determination can reduce computational resources by 75% without significant estimation error. Thus, the CS-ECG based HRV analysis method has potential to reduce overall energy consumption, memory space, and processing time by a factor of 4 as compared to the HRV analysis method with an uncompressed ECG signal.  © 2023 IEEE.",12-17,10.1109/ICSIMA59853.2023.10373467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183467483&doi=10.1109%2fICSIMA59853.2023.10373467&partnerID=40&md5=0342af98fe7e6be1b4ad9a5bd6fc218c,2023
Deep Learning Based Face Recognized Attendance Management System using Convolutional Neural Network,"In today's digital age, manual attendance tracking is plagued by inefficiency and the potential for inaccuracies, often leading to proxy attendance. The main aim of this research work is to manage and monitor the student's attendance by using face recognition technology. This proposed model is mainly categorized four major modules. First module is database creation. Second module is face detection. Then third module is face recognition and final module is automatic attendance updating process. Student images are compiled to create a comprehensive database, ensuring inclusivity across the class roster. The system utilizes the face recognition library, which relies on deep learning based algorithms for face detection and recognition during testing. This face recognition part Convolutional Neural Network algorithm is used. The system matches detected faces with the known database and marks attendance, ensuring a streamlined and accurate attendance tracking process. This innovative approach has the potential to revolutionize attendance management in educational settings, offering a contactless and efficient solution while mitigating proxy attendance concerns. The proposed model is to compare the accuracy level of face recognition.  © 2023 IEEE.",-,10.1109/ICTBIG59752.2023.10456032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189246538&doi=10.1109%2fICTBIG59752.2023.10456032&partnerID=40&md5=1bee201076f9399339c845e9ea1c6d81,2023
Hierarchical Knowledge Guided Learning for Real-World Retinal Disease Recognition,"In the real world, medical datasets often exhibit a long-tailed data distribution (i.e., a few classes occupy the majority of the data, while most classes have only a limited number of samples), which results in a challenging long-tailed learning scenario. Some recently published datasets in ophthalmology AI consist of more than 40 kinds of retinal diseases with complex abnormalities and variable morbidity. Nevertheless, more than 30 conditions are rarely seen in global patient cohorts. From a modeling perspective, most deep learning models trained on these datasets may lack the ability to generalize to rare diseases where only a few available samples are presented for training. In addition, there may be more than one disease for the presence of the retina, resulting in a challenging label co-occurrence scenario, also known as multi-label, which can cause problems when some re-sampling strategies are applied during training. To address the above two major challenges, this paper presents a novel method that enables the deep neural network to learn from a long-tailed fundus database for various retinal disease recognition. Firstly, we exploit the prior knowledge in ophthalmology to improve the feature representation using a hierarchy-aware pre-training. Secondly, we adopt an instance-wise class-balanced sampling strategy to address the label co-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we introduce a novel hybrid knowledge distillation to train a less biased representation and classifier. We conducted extensive experiments on four databases, including two public datasets and two in-house databases with more than one million fundus images. The experimental results demonstrate the superiority of our proposed methods with recognition accuracy outperforming the state-of-the-art competitors, especially for these rare diseases. © 1982-2012 IEEE.",335-350,10.1109/TMI.2023.3302473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167823893&doi=10.1109%2fTMI.2023.3302473&partnerID=40&md5=ef704542bd55a2dab7f36d00edca1987,2024
Development of Optimized User-Recognition Technology Using Multilayered XAI-Based ECG Signals,"Owing to the swift advancements in information and technology and wearable device technology, research involving bio-signals is being conducted for convenient personal identification authentication. Therefore, deep learning using electrocardiogram (ECG) signals is being developed as a next-generation user-recognition technology for application in real-life environments based on high security and accuracy. This study proposes a deep learning model that combines multilayered explainable artificial intelligence for user recognition in a real-life environment. The ECG-signal database utilizes the MIT-BIH normal sinus rhythm database (NSRDB) and a self-acquired database. A comparison between a single deep learning model and the proposed multilayer deep learning model using these signals confirmed that the proposed model achieved higher accuracy, with minimum accuracy differences of 15% and 0.2%. In addition, the recognition accuracies of the single deep learning model and multilayered deep learning model were compared five times to confirm the difference in output results caused by various factors in the single deep learning model. The user-recognition accuracies of a single deep learning model for the NSRDB exhibited a difference of 3.4%, whereas those of the multilayered model showed a difference of 0.9%. The user-recognition accuracy difference when applying the self-acquired database to the single deep learning model was 1.3%, but that of the multilayered deep learning model was 0.3%. Therefore, the multilayered deep learning model exhibited a stable user-recognition accuracy compared to that of the single structure, and the optimized area that affects the output result was confirmed.  © 2014 IEEE.",10856-10864,10.1109/JIOT.2023.3327738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181571132&doi=10.1109%2fJIOT.2023.3327738&partnerID=40&md5=8fca0e55f0944d725f2aa75ab1440daa,2020
Segmentation-based ID preserving iris synthesis using generative adversarial networks,"This study proposes a method for generating ID preserving synthetic iris database. The proposed method can be applied in the generation of a synthetic iris database for various iris recognition tasks. This work successfully combines the main idea of generative adversarial learning, segmentation, and identification to solve real-world problems. The method produces synthetic iris images from the segmentation masks given ID information. The segmentation mask, iris pose, is devised from the input image by using a segmentation network. By doing this, the ID-preserving iris synthesis method generates an unlimited number of synthetic iris images by processing the provided input images. The accuracy of the generated iris images is validated by measuring top-1, top-5, and Area under the Curve (AUC). The SegNet and IDNet performance was evaluated using class accuracy in terms of precision, recall, and F1-score alongside the computation model complexity. This study exhibits ease of use, compatibility, and accuracy in preserving ID information for the generated synthetic images compared to the other baseline methods. Evaluation results prove the efficacy of this work by comparing the randomly generated iris images using the current study alongside existing methods. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",27589-27617,10.1007/s11042-023-16508-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168624789&doi=10.1007%2fs11042-023-16508-1&partnerID=40&md5=e5a38a414d7ff0e6687c2451369d8297,2021
EVA: An End-to-End Exploratory Video Analytics System,"In recent years, deep learning models have revolutionized computer vision, enabling diverse applications. However, these models are computationally expensive, and leveraging them for video analytics involves low-level imperative programming. To address these efficiency and usability challenges, the database community has developed video database management systems (VDBMSs). However, existing VDBMSs lack extensibility and composability and do not support holistic system optimizations, limiting their practical application. In response to these issues, we present our vision for EVA, a VDBMS that allows for extensible support of user-defined functions and employs a Cascades-style query optimizer. Additionally, we leverage Ray's distributed execution to enhance scalability and performance and explore hardware-specific optimizations to facilitate runtime optimizations. We discuss the architecture and design of EVA, our achievements thus far, and our research roadmap. © 2023 Owner/Author(s).",-,10.1145/3595360.3595858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168377347&doi=10.1145%2f3595360.3595858&partnerID=40&md5=c3ff3fbdf8becb82533f918f839afd27,2023
Hilbert Domain Analysis of Wavelet Packets for Emotional Speech Classification,"This work investigates the significance of Hilbert domain characterization of wavelet packets in classifying different emotion of speech signal. The goal of this paper is to create a new emotional speech database and introduce a new feature extraction approach that can recognize various emotions. The proposed feature, wavelet cepstral coefficients (WCC) are based on Hilbert spectrum analysis of the wavelet packet of the speech signal. The speaker-independent machine learning models are developed using multiclass support vector machine (SVM) and k-nearest neighbourhood (KNN) classifier. The approach is tested with newly developed Telugu Indian database and the EMOVO (Italian emotional speech) database. Our proposed wavelet features achieve a peak accuracy of 73.5%, further boosted by NCA feature selection by 3–5%, resulting in an improved unweighted average recall (UAR) of 78% for database 1 and 87.50% for database 2, employing optimal wavelet features in conjunction with SVM classification. The proposed features outperformed the baseline Mel-frequency cepstral coefficients (MFCC) feature. The performance of newly formulated features is better than other existing methodologies tested with different language databases. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",2224-2250,10.1007/s00034-023-02544-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178895827&doi=10.1007%2fs00034-023-02544-7&partnerID=40&md5=a23751605cd8b10651d16bd2d1e86ae4,2024
Hilbert domain characterizations of wavelet packets for automated heart sound abnormality detection,"Heart valve disease (HVD) is a common disease that affects millions of people worldwide. Early detection and treatment are essential for improving the prognosis of patients with HVD. Phonocardiogram (PCG) signals are a non-invasive and inexpensive way to assess the mechanical activity of the heart. In this study, a novel method for HVD detection using Hilbert domain mapping of wavelet packet of PCG signals is proposed. Two standard PCG databases are used to evaluate the proposed method. Packet instantaneous frequency deviation (PIFD) and packet instantaneous energy deviation (PIED) features are extracted from the PCG signals and used for classification. A support vector machine (SVM) and K-nearest neighbour (KNN) based error-correcting output code (ECOC) approach is used to handle multiclass classification and minimize classification error. The proposed method achieves an unweighted average recall (UAR) of 99.8% on database 1 and 99.32% on database 2, which outperforms other baseline methods. The results suggest that the proposed method is a promising approach for HVD detection using PCG signals. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105793,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183711264&doi=10.1016%2fj.bspc.2023.105793&partnerID=40&md5=ec12189de47ff16c6a4f45f9fe2526ca,2021
Patterns of publications in social media-based co-creation: a bibliometric analysis,"Purpose: Today social media capabilities have enabled businesses and enterprises to more collaboration, engagement and co-creation with their customers. So the current paper expands on this notion. The aim of this study is a bibliometric analysis to examine the trends of publications in the field of co-creation based on social media. Design/methodology/approach: To data collection of quantitative analysis, Scopus database was selected and the collected data were analyzed using Bibliometrix-package. The Web of Science also was selected to retrieve highly cited and hot papers for qualitative part of analysis besides top 10 Scopus highest citation per year documents on June 6, 2020. Findings: The results indicate insights into research trends pertaining to social media-based co-creation, as follows: starting jump to the publications occurred in this researches from the year 2008 and the growth trend is progressing in recent years; the stressful points are “co-design,” “co-creation” and “value co-creation” and concepts such as “open innovation,” “co-innovation” and “co-new product design” are new topics that guide future direction; the USA and UK are leaders in number of multiple and single publications; the most active and top journals that are better suited to achieving a high citation rate per year for a related paper were introduced. In addition, the top documents and highly cited papers were qualitatively analyzed on the basis of times cited per year. Research limitations/implications: The current study is not free of limitations. The database was limited to only Scopus. So the patterns and trends generated in the study may not be generalized to all social media-based co-creation research. Of course, the authors did not intend to ignore other contributions. It is mainly because of the number of documents retrieved from Scopus database and the coverage, Scopus was selected. Moreover, other types of research techniques such as correspondence analysis can be incorporated to generate additional meaningful insight. Originality/value: In this time of social media and user-generated content portals, co-creation through social media has become quite popular. So the main innovation of this study is providing a visual presentation of the trends and patterns in the evaluation of social media-based co-creation from the first document about the research area published till 2020. The results of this paper can shed light on the factors that strengthen the contribution of studies in a research area. Generally, the bibliometric items the authors analyzed essentially show the entire field picture and guide researchers toward understanding future trends to produce impactful studies. © 2020, Emerald Publishing Limited.",578-596,10.1108/VJIKMS-09-2021-0222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125960781&doi=10.1108%2fVJIKMS-09-2021-0222&partnerID=40&md5=4b9ab61ab46537154bdfa393dc4656ee,2021
Divide and recombine approach for warranty database: Estimating the reliability of an automobile component,"The continuously updated database of failures and censored data of numerous products has become large, and on some covariates, information regarding the failure times is missing in the database. As the dataset is large and has missing information, the analysis tasks become complicated and a long time is required to execute the programming codes. In such situations, the divide and recombine (D&R) approach, which has a practical computational performance for big data analysis, can be applied. In this study, the D&R approach was applied to analyze the real field data of an automobile component with incomplete information on covariates using the Weibull regression model. Model parameters were estimated using the expectation maximization algorithm. The results of the data analysis and simulation demonstrated that the D&R approach is applicable for analyzing such datasets. Further, the percentiles and reliability functions of the distribution under different covariate conditions were estimated to evaluate the component performance of these covariates. The findings of this study have managerial implications regarding design decisions, safety, and reliability of automobile components. © 2023 Xi'an Jiaotong University",119-128,10.1016/j.dsm.2023.12.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188252081&doi=10.1016%2fj.dsm.2023.12.002&partnerID=40&md5=69b495e58dbf8a1bc1fe90c6ee451800,2023
Developing a Reliable Shallow Supervised Learning for Thermal Comfort using Multiple ASHRAE Databases,"The artificial intelligence (AI) system faces the challenge of insufficient training datasets and the risk of an uncomfortable user experience during the data gathering and learning process. The unreliable training data leads to overfitting and poor system performance which will result in wasting operational energy. This work introduces a reliable data set for training the AI subsystem for thermal comfort. The most reliable current training data sets for thermal comfort are ASHRAE RP-884 and ASHRAE Global Thermal Comfort Database II, but the direct use of these data for learning will give a poor learning result of less than 60&#x0025; accuracy. This paper presents the algorithm for data filtering and semantic data augmentation for the multiple ASHRAE databases for the supervised learning process. The result was verified with the visual psychrometric chart method that can check for overfitting and verified by developing the Internet of Things (IoT) control system for residential usage based on shallow supervised learning. The AI system was a Wide Artificial Neural Network (ANN) which is simple enough to be implemented in a local node. The filtering and semantic augmentation method can increase the accuracy to 96.1&#x0025;. The control algorithm that was developed based on the comfort zone identification can increase the comfort acknowledgement by 6.06&#x0025; leading to energy saving for comfort. This work can contribute to 717.2 thousand tonnes of CO2 equivalent per year which is beneficial for a more sustainable thermal comfort system and the development of a reinforced learning system for thermal comfort. IEEE",1-13,10.1109/TAI.2024.3376319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188422819&doi=10.1109%2fTAI.2024.3376319&partnerID=40&md5=3a18b78da3d61ae33f0653a346f99481,2021
Fingerprint and Iris liveness detection using invariant feature-set,"Presentation attacks that make the biometric systems vulnerable has become a growing concern in recent years keeping in view its widespread applications in the field of banking, medical, security systems etc. For instance, textured contact lenses, high-quality printouts and fabricated synthetic materials spoof the iris texture and fingerprints that lead to increase in false rejection. Till now, extensive work has been done on global features. However, this paper proposed local features with invariance properties. Thus, the paper proposes detection of spoofing attacks in which local features are extracted for micro-textural analysis with properties of invariance to scale, rotation and translation. The features are encoded using Lehmer code and transformed into histograms that act as feature descriptors for classification. The top 4 features are selected using Friedman test. Experiments are simulated on iris spoofing databases: IIITD-Contact Lens, IIITD-Iris Spoofing, Clarkson-2015, Warsaw-2015and fingerprint spoofing databases: LivDet-2013 and LivDet-2015. Results have been validated through intra-sensor, inter-sensor, cross-sensor and cross-material. In case of IIITD-CLI, an EER of 1.36% and an ACER of 1.45% is obtained. For IIS, 0.94% of EER and 1.61% of ACER is observed. For Clarkson database, 0.79% of EER and 2.10% of ACER is obtained. An ACER of 0.57% is obtained for LivDet-2013 and 0.47% for LivDet-2015. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",-,10.1007/s11042-023-17854-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181217257&doi=10.1007%2fs11042-023-17854-w&partnerID=40&md5=51507947fa609b7718c18952cb567747,2022
Enhanced spatio-temporal 3D CNN for facial expression classification in videos,"This article proposes a hybrid network model for video-based human facial expression recognition (FER) system consisting of an end-to-end 3D deep convolutional neural networks. The proposed network combines two commonly used deep 3-dimensional Convolutional Neural Networks (3D CNN) models, ResNet-50 and DenseNet-121, in an end-to-end manner with slight modifications. Currently, various methodologies exist for FER, such as 2-dimensional Convolutional Neural Networks (2D CNN), 2D CNN-Recurrent Neural Networks, 3D CNN, and features extracting algorithms such as PCA and Histogram of oriented gradients (HOG) combined with machine learning classifiers. For the proposed model, we choose 3D CNN over other methods since they preserve temporal information of the videos, unlike 2D CNN. Moreover, these aren’t labor-intensive such as various handcrafted feature extracting methods. The proposed system relies on the temporal averaging of information from frame sequences of the video. The databases are pre-processed to remove unwanted backgrounds for training 3D deep CNN from scratch. Initially, feature vectors from video frame sequences are extracted using the 3D ResNet model. These feature vectors are fed to the 3D DenseNet model’s blocks, which are then used to classify the predicted emotion. The model is evaluated on three benchmarking databases: Ravdess, CK + , and BAUM1s, which achieved 91.69%, 98.61%, and 73.73% accuracy for the respective databases and outperformed various existing methods. We prove that the proposed architecture works well even for the classes with less amount of training data where many existing 3D CNN networks fail. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",9911-9928,10.1007/s11042-023-16066-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163386178&doi=10.1007%2fs11042-023-16066-6&partnerID=40&md5=1bb232c8bb1396351d632f717ccaba97,2024
Pattern Matching to improve Accuracy and Efficiency of File Lifecycles Forecasting,"Time series prediction is a crucial task with applications in various domains. In this research, we introduce a transfer learning approach that goes beyond the historical time series of individual instances to enhance prediction performance. Traditional time series prediction methods often focus solely on historical data from a single time series, limiting their ability to capture already known patterns and behaviors. Our proposed method addresses this limitation by constructing a curated database of time series representative of the behavior to predict. We apply our suggested method to the prediction of the I/O (Input/Output) behavior of scientific applications such as NEMO, NAMD, and LQCD. Our non-redundant database contains diverse time series, allowing us to extract valuable insights and patterns that might not be discernible from individual series. By leveraging similarities and patterns across those time series, our approach improves prediction accuracy and robustness. The results we obtained are highly promising, demonstrating that in term of time serie ranking prediction accuracy our proposed inter-timeseries method achieves from 72.9% to 83.8% accuracy in most difficult scenarios to 97% for the others. To compare our solution to existing methods, we show an improvement of 10.1% accuracy compared to ARIMA.  © 2023 IEEE.",-,10.1109/SITA60746.2023.10373743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183329187&doi=10.1109%2fSITA60746.2023.10373743&partnerID=40&md5=43ec6eec1e3e0f69e591b4907c52deeb,2022
A novel indexing algorithm for latent palmprints leveraging minutiae and orientation field,"Latent palmprints represent crucial forensic evidence in criminal investigations, necessitating their storage in governmental databases. The identification of corresponding palmprints within large-scale databases using an automated palmprint identification system (APIS) is time-consuming and computationally intensive. To address this challenge, this paper introduces an innovative approach: delineating the region of interest (ROI) for palmprint segmentation and presenting a novel indexing algorithm founded on minutiae and the orientation field (OF). Additionally, a novel feature vector is proposed, leveraging minutiae triplets and ellipse properties, marking the pioneering algorithm to consider minutiae importance in palmprint indexing. Significantly, an improved version of an existing palmprint indexing algorithm tailored for latent palmprints is introduced. The study demonstrates the indexing and retrieval of both our feature vectors and those obtained by the improved palmprint indexing algorithm, using two clustering algorithms and locality-sensitive hashing (LSH). The method's robustness is evaluated across three diverse databases with extensive palmprint records. The experimental results underscore the superior performance of our approach compared to current state-of-the-art algorithms. © 2023 The Authors",-,10.1016/j.iswa.2023.200320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183389897&doi=10.1016%2fj.iswa.2023.200320&partnerID=40&md5=d8b32b9d3fe5ec2b87530c5be8218cb8,2023
Verb-Object Collocations in the Russian Collocations Database: Linguistic and Statistical Properties,"Russian Collocations Database comprises collocations extracted from nine dictionaries. The examples provide additional statistical information based on text corpora. The paper deals with those new characteristics that have been added to the database, and how the verb-object collocations that are represented in it intersect with corpus data. The database offers two kinds of interfaces that imply a simple or an advanced search. The former is aimed at language users while the latter can be used by linguists and show a wide range of quantitative characteristics. The paper also presents results of correlation analysis made between collocation lists extracted from dictionaries and corpora. Verb-object collocations from the top of the list of any association measure used in the database proved to be described in several dictionaries compared to the bottom of the list. Verbs tend to be more productive than nouns and produce more examples. © Tribun EU 2023.",121-129,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187220664&partnerID=40&md5=ac816d5abff502404071b30f7e26a420,2023
Guaranteeing the Õ(AGM/OUT) Runtime for Uniform Sampling and Size Estimation over Joins,"We propose a new method for estimating the number of answers OUT of a small join query Q in a large database D, and for uniform sampling over joins. Our method is the first to satisfy all the following statements. Support arbitrary Q, which can be either acyclic or cyclic, and contain binary and non-binary relations. Guarantee an arbitrary small error with a high probability always in O(AGM/OUT ) time, where AGM is the AGM bound (an upper bound of OUT), and O hides the polylogarithmic factor of input size. We also explain previous join size estimators in a unified framework. All methods including ours rely on certain indexes on relations in D, which take linear time to build offline. Additionally, we extend our method using generalized hypertree decompositions (GHDs) to achieve a lower complexity than O(AGM/OUT ) when OUT is small, and present optimization techniques for improving estimation efficiency and accuracy.  © 2023 ACM.",113-125,10.1145/3584372.3588676,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164261854&doi=10.1145%2f3584372.3588676&partnerID=40&md5=62283aa3c7ebd64d4d823c6ac7c32b46,2023
Data-driven multiscale finite-element method using deep neural network combined with proper orthogonal decomposition,"In this paper, a data-driven multiscale finite-element method (data-driven FE2) is proposed using a deep neural network (DNN) and proper orthogonal decomposition (POD) to describe nonlinear heterogeneous materials. The concurrent classical FE2 needs the iterative calculations of microscopic boundary-value problem for representative volume element (RVE) at all integration points of the macroscopic structures. These iterative procedures need large computational time. To overcome this limitation, the proposed data-driven FE2 method solves the macroscopic problem by assigning data to all integration points that satisfy microscopic equilibrium by constructing a material genome database in which the microscopic problem of RVE is pre-calculated in online computing. Here, we developed a DNN model that can accurately and efficiently predict microscopic behavior by connecting POD for material genome database construction. Therefore, we improved the data-driven FE2 technique one step further by efficiently generating available material genome database. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",661-675,10.1007/s00366-023-01813-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152414444&doi=10.1007%2fs00366-023-01813-y&partnerID=40&md5=45bc2c441921768333ef4993a7c5ab14,2021
"A Unifying Framework for Incompleteness, Inconsistency, and Uncertainty in Databases",Deploying possible world semantics and the challenge of computing the certain answers to queries. © 2024 Copyright held by the owner/author(s).,74-83,10.1145/3624717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186454400&doi=10.1145%2f3624717&partnerID=40&md5=4d44e9aa2c439c16139dedfa1fe5e9c6,2024
Multi-resolution analysis and deep neural network architecture based hybrid feature extraction technique for plant disease identification and severity estimation,"The crop protection techniques using efficient plant disease identification are highly beneficial for Smart Agriculture Management which further decreases the gap between the demand/supply of consumers & farmers. However, an efficient feature extraction technique is desirable to achieve high accuracy, least graphical processing, and low processing time for coloured images. Therefore, this work proposes a hybrid novel feature extraction technique for coloured plant leaf images to overcome these challenges. This work is divided into two parts: 1). Plant disease identification and 2). Plant disease severity estimation. In this technique, multiresolution analysis using large-scale coefficients of Discrete Wavelet Transform (DWT) is performed using orthogonal wavelets. The orthogonal wavelets such as Daubechies, Biorthogonal, Symlets and Coiflets with appropriate vanishing moments are utilized to select significant features. Then, the Eigen decomposition of large-scale DWT-based coloured image features is utilized using Principal Component Analysis (PCA), where only the Eigenvalues with large variance are considered for further feature selection. Several experiments are carried out for the selection of felicitous sub-bands & decomposition levels. The Deep Neural network (DNNs) architectures such as AlexNet, ResNet 101 and Inception V3 are incorporated to classify the obtained features. The DNNs operate on Convolutional Neural Networks, which utilize the weights from pre-learned ImageNet Database classes to classify various classes. The combination of DNNs with Transfer Learning and Fine Tuning is made to tune the parameters for better efficacy. Experiments were performed on the PlantVillage Database, which contains healthy/diseased images of plants such as Apple, Corn, Grapes and tomato leaves. A new database is created by capturing the images of Rice Crop Leaves, including healthy and diseased leaves. A total of 8000 images are processed for disease identification. The proposed technique provided 99% accuracy for disease identification task and 90% for disease severity estimation in the real-life Rice Dataset and outperformed the existing techniques in this field of research. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022.",1163-1183,10.1007/s12065-022-00800-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144546331&doi=10.1007%2fs12065-022-00800-4&partnerID=40&md5=93f0594fd695fe59e90899c574bcf96e,2023
Automated Assessment for Databases Units,"With ever-growing class-sizes, automated assessment can be an extremely valuable tool in higher education. In this paper, we present our tool for automatically assessing SQL programming and reflect on five years of its use. We highlight some of the changes and challenges we have encountered as well as lessons learned. Our tool has proven successful in both its primary goal and in secondary goals such as encouraging student participation. Since its inception it has grown incrementally and been adapted for other contexts. It is now undergoing a major overhaul to expand its remit to include elements of database design and theory. We will discuss how this is being done and how we are aiming to construct a single, integrated assessment tool. Ultimately, the tool could be adapted to other contexts as well and our aim is to raise awareness of the issues facing automated assessment and encourage its adoption.  © 2024 Owner/Author.",17-20,10.1145/3633053.3633059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182745652&doi=10.1145%2f3633053.3633059&partnerID=40&md5=cc382fadb77f2eb6e72c6a0f85d57745,2024
Roadside Sensors for Traffic Management,"Knowledge of modern state-of-the-practice traffic flow sensors provides traffic managers, researchers, and students an understanding of the operation, strengths, and limitations of current sensor technologies and enables them to make an informed decision as to which is appropriate for a particular application. Accordingly, this article describes the intrusive and nonintrusive traffic flow sensor technologies in use today, their applications and selection criteria, and typical output data. Furthermore, it provides examples of representative sensor models. The technologies discussed are mature with respect to current traffic management applications, although some may not provide the data or accuracy required for a specific application or may not perform as needed under the operational conditions anticipated at the installation site. Sensors selected for a first-time application should be field tested under conditions that will be encountered in day-to-day operation before large-scale purchases of the devices are made. As alternative traffic data and information sources, such as commercial data vendors, Wi-Fi and Bluetooth sensing of smartphone locations, and connected and automated vehicle data, become increasingly available, they are progressively finding their way into modern traffic management systems as a complement to conventional roadside sensors. IEEE",2-26,10.1109/MITS.2023.3346842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183952269&doi=10.1109%2fMITS.2023.3346842&partnerID=40&md5=b3db7f961442282bff23cb4e91b089b0,2022
Atlas: A Toolset for Efficient Model-Driven Data Exchange in Data Spaces,"The European Union has recently outlined the idea of data spaces, which require complex support for various aspects of data management, such as specification, publication, storage, access, etc. In this paper, we introduce Atlas, an extensible toolset integrating techniques and approaches from two worlds - interoperable model-driven data exchange and multi-model data management. On the one hand, the toolset supports consistent model-driven authoring of data specifications based on common semantic vocabularies, including their technical artifacts such as data schemas for CSV, XML and JSON, and standards-based ontology mappings to ensure interpretability of the data as RDF. Data producers then provide their data according to the specifications, in a format with which they are the most comfortable. On the other hand, Atlas enables the users to efficiently store and query the provided data using multi-model databases. It does so by exploiting the compliance of the provided data with the data specification for minimizing the manual effort needed to store the data in the data consumer's database, regardless of the data format chosen by the data producer.  © 2023 IEEE.",4-8,10.1109/MODELS-C59198.2023.00009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182403703&doi=10.1109%2fMODELS-C59198.2023.00009&partnerID=40&md5=0d3a46e031507641d9b0963596fcb099,2023
Poster: The Risk of Insufficient Isolation of Database Transactions in Web Applications,"Web applications utilizing databases for persistence frequently expose security flaws due to race conditions. The commonly accepted remedy to this problem is to envelope related database operations in transactions. Unfortunately, sole trust in transactions to isolate competing sets of database interactions is often misplaced. While the precise isolation properties of transactions depend on the configuration of the database management system (DBMS), the default configuration of common DBMS exposes transactions to anomalies that render their protection worthless. We give a comprehensive overview on the behavior of common DBMSes with respect to transactions and show that their default settings are insufficient to provide comprehensive protection. Furthermore we conduct a preliminary study on how commonly transactions and isolation configuration adjustments are deployed across 4.222 open source PHP applications that use SQL, finding 2.789 transactions and only 418 isolation adjustments indicators. Our findings indicate that race conditions are an underappreciated vulnerability class and adjustments are too rare to for transactions to reliably provide sufficient protection. © 2023 Copyright held by the owner/author(s).",3576-3578,10.1145/3576915.3624394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179851534&doi=10.1145%2f3576915.3624394&partnerID=40&md5=2b8470f07dd369d4cffd91d379bfc31a,2023
CBOEP: Generating negative enhancer-promoter interactions to train classifiers,"For training and testing enhancer-promoter interaction (EPI) classifiers, the question on which non-positive EPIs are selected as negative instances must be answered. Most previous methods use the dataset of the EPI classifier TargetFinder where negative EP pairs are sampled from non-positive EP pairs. Consequently, over 92% of EPIs in the TargetFinder-positive and negative sets of cell line GM12878 have a 2-fold or greater positive/negative class imbalance of promoter occurrences between the positive and negative EP pairs. This situation negatively impacts the predictability of EPI classifiers trained using the datasets.Thus, we first proposed the condition that the negative EPIs should satisfy. Second, we devised a method called CBOEP (class balanced occurrences of enhancers and promoters), to generate negative EPI sets that approximately fulfil this condition for a given positive EPI set. CBOEP solves the finding problem by reducing it to the maximum-flow problem. Third, we applied the generated negative EPI sets to existing EPI classifiers, TransEPI and TargetFinder. The negative datasets lead to higher prediction performance than the existing negative EPI datasets. The source code is available at https://github.com/maruyama-lab-design/CBOEP. © 2023 ACM.",-,10.1145/3584371.3612997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175836051&doi=10.1145%2f3584371.3612997&partnerID=40&md5=d248461ee6e3faf0e42eba2a199b0eae,2020
A Novel Assessment of Lung Cancer Classification System Using Binary Grasshopper with Artificial Bee Optimisation Algorithm with Double Deep Neural Network Classifier,"Lung cancer is the leading cause of death from cancer worldwide. Finding pulmonary nodules is a critical step in the diagnosis of early-stage lung cancer. It has the potential to become a tumour. Computed tomography (CT) scans for lung disease analysis offer useful data. Finding malignant pulmonary nodules and determining whether lung cancer is benign or malignant are the main goals. Before further image preparation, image denoising is an important process for removing noise from images (feature extraction, segmentation, surface detection, and so on) maximising the preservation of edges and other intact features. This study employs a novel evolutionary method dubbed the binary grasshopper optimisation algorithm in order to address some of the shortcomings of feature selection and provide an efficient feature selection algorithm, the artificial bee colony (BGOA-ABC) algorithm enhance categorisation. Then, to categorise the chosen features, we employ a hybrid classifier known as a double deep neural network (DDNN) algorithm. A technique used by MATLAB that segments impacted areas utilising improved IPCT (Profuse) aggregation technology employing datasets from the cancer image archive (CIA), the image database resource initiative (IDRI), and the lung imaging database consortium. Various performance metrics are evaluated and associated with different cutting-edge techniques, classifiers that are already in use. © The Institution of Engineers (India) 2024.",-,10.1007/s40031-024-01027-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188057762&doi=10.1007%2fs40031-024-01027-w&partnerID=40&md5=1c70200f409b5e31142a5d4a8558ef3a,2021
Dominating Set Database Selection for Visual Place Recognition,"This paper introduces a novel approach for creating a visual place recognition (VPR) database for localization in indoor environments from RGBD scanning sequences. The proposed method formulates the problem as a minimization challenge by utilizing a dominating set algorithm applied to a graph constructed from spatial information, referred to as the 'DominatingSet' algorithm. Experimental results on various datasets, including 7-scenes, BundleFusion, RISEdb, and a specifically recorded sequences in a highly repetitive office setting, demonstrate that our technique significantly reduces database size while maintaining comparable VPR performance to state-of-the-art approaches in challenging environments. Additionally, our solution enables weakly-supervised labeling for all images from the sequences, facilitating the automatic fine-tuning of VPR algorithm to target environment. Additionally, this paper presents a fully automated pipeline for creating VPR databases from RGBD scanning sequences and introduces a set of metrics for evaluating the performance of VPR databases. The code and released data are available on our web-page - https://prime-slam.github.io/place-recognition-db/.  © 2023 IEEE.",473-479,10.1109/ICAR58858.2023.10406721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185838598&doi=10.1109%2fICAR58858.2023.10406721&partnerID=40&md5=ccbd8bcacb2b54bebf54f4feb994e2e8,2020
Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes,"The task of deepfakes detection is far from being solved by speech or vision researchers. Several publicly available databases of fake synthetic video and speech were built to aid the development of detection methods. However, existing databases typically focus on visual or voice modalities and provide no proof that their deepfakes can in fact impersonate any real person. In this paper, we present the first realistic audio-visual database of deepfakes SWAN-DF, where lips and speech are well synchronized and video have high visual and audio qualities. We took the publicly available SWAN dataset of real videos with different identities to create audio-visual deepfakes using several models from DeepFaceLab and blending techniques for face swapping and HiFiVC, DiffVC, YourTTS, and FreeVC models for voice conversion. From the publicly available speech dataset LibriTTS, we also created a separate database of only audio deepfakes LibriTTS-DF using several latest text to speech methods: YourTTS, Adaspeech, and TorToiSe. We demonstrate the vulnerability of a state of the art speaker recognition system, such as ECAPA-TDNN-based model from SpeechBrain, to the synthetic voices. Similarly, we tested face recognition system based on the MobileFaceNet architecture to several variants of our visual deepfakes. The vulnerability assessment show that by tuning the existing pretrained deepfake models to specific identities, one can successfully spoof the face and speaker recognition systems in more than 90% of the time and achieve a very realistic looking and sounding fake video of a given person. © 2023 IEEE.",-,10.1109/IJCB57857.2023.10449189,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186844495&doi=10.1109%2fIJCB57857.2023.10449189&partnerID=40&md5=ac38575b627e279177400c71a5c4e395,2024
"PanoEmo, a set of affective 360-degree panoramas: a psychophysiological study","There is a significant increase in the use of virtual reality in scientific experiments in the fields of ergonomics, education, and psychology among others. Many researchers successfully provoked different affective states in participants in order to capture physiological correlates or apply psychotherapeutic techniques. All these studies employed different stimuli, like 3D pictures, computer-built graphics and 180- or 360-degree panoramic photographs. In an attempt to begin the standardization of the measurements, we propose PanoEmo, a set of affective 360-degree photographic panoramas. Our aim was to explore the emotional reactions in response to PanoEmo, based on self-report scales, somatic, and vegetative affective indices. Fifty-five participants watched 45 photographic panoramas of different valence during 20 s without a special task. Self-reported valence correlated positively to zygomaticus major and negatively to corrugator supercilii electromyographic activity. Zygomaticus major also correlated positively to arousal. Respiratory rate correlated negatively to valence. Pleasant panoramas provoked a slower respiratory rate, while unpleasant ones increased it. Skin conductance response was positively related to self-reported arousal. Unexpectedly, heart rate did not correlate to self-report measures during the whole epoch, but it correlated positively around 5 s after the panorama onset. As a limitation, we should mention that our database contains a much higher number of positive panoramas. Although we expected the equal number of negative, neutral, and positive panoramas, we found a prevalence of positive ones. Nonetheless, subsequent studies should enrich the set with more negative panoramas to get a homogeneously distributed database. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",-,10.1007/s10055-023-00900-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180684067&doi=10.1007%2fs10055-023-00900-1&partnerID=40&md5=77c305013336c7319122961e634c1e55,2020
Turbo: Effective Caching in Differentially-Private Databases,"Differentially-private (DP) databases allow for privacy-preserving analytics over sensitive datasets or data streams. In these systems, user privacy is a limited resource that must be conserved with each query. We propose Turbo, a novel, state-of-the-art caching layer for linear query workloads over DP databases. Turbo builds upon private multiplicative weights (PMW), a DP mechanism that is powerful in theory but ineffective in practice, and transforms it into a highly-effective caching mechanism, PMW-Bypass, that uses prior query results obtained through an external DP mechanism to train a PMW to answer arbitrary future linear queries accurately and ""for free""from a privacy perspective. Our experiments on public Covid and CitiBike datasets show that Turbo with PMW-Bypass conserves 1.7 - 15.9× more budget compared to vanilla PMW and simpler cache designs, a significant improvement. Moreover, Turbo provides support for range query workloads, such as timeseries or streams, where opportunities exist to further conserve privacy budget through DP parallel composition and warm-starting of PMW state. Our work provides a theoretical foundation and general system design for effective caching in DP databases.  © 2023 Owner/Author(s).",579-594,10.1145/3600006.3613174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176955004&doi=10.1145%2f3600006.3613174&partnerID=40&md5=96348fc7460c64762af644db361ad7ee,2023
StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability,"Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present. In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S). To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score. Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models. The database and codes are available at https://github.com/QMME/StableVQA. © 2023 ACM.",1066-1076,10.1145/3581783.3611860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171845376&doi=10.1145%2f3581783.3611860&partnerID=40&md5=b41e227cfe96bf1409c973a8eb792943,2024
TrajParquet: A Trajectory-Oriented Column File Format for Mobility Data Lakes,"Columnar data formats, such as Apache Parquet, are increasingly popular nowadays for scalable data storage and querying data lakes, due to compressed storage and efficient data access via data skipping. However, when applied to spatial or spatio-temporal data, advanced solutions are required to go beyond pruning over single attributes and towards multidimensional pruning. Even though there exist solutions for geospatial data, such as GeoParquet and SpatialParquet, they fall short when applied to trajectory data (sequences of spatio-temporal positions). In this paper, we propose TrajParquet, a format for columnar storage of trajectory data, which is highly efficient and scalable. Also, we present a query processing algorithm that supports spatio-temporal range queries over TrajParquet. We evaluate TrajParquet using real-world data sets and in comparison with extensions of GeoParquet and SpatialParquet, suitable for handling spatio-temporal data.  © 2023 Owner/Author(s).",-,10.1145/3589132.3625623,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182507017&doi=10.1145%2f3589132.3625623&partnerID=40&md5=b753037fa3743a648177d0c10eccefbd,2022
A novel Mutual Information based PCA approach for face identification,"Principal component analysis (PCA) is a statistical tool designed to reduce dimensionality, removing redundant information in the database. When the database is a fusion of inter-similar but not intra-similar groups of databases, PCA has its limitations: poor discriminatory power and large computational load - as cumulative variance accumulation is used for selecting top features considering the global variance of the entire database. In this work, combining the information contained in the database and the Principal Components, a new approach that improves the discriminatory power of the conventional PCA is presented. This approach is developed mainly in two stages. First is the weighting of the training set through a weight vector based on mutual information. The second one is the linear projection of a weighted database using principal components. The ultimate aim of this work, given a facial image and a database of facial images, is to identify the image in the database which is most similar to the given image. The main contribution of this work is to consider the significance of the database images in the identification process in terms of the information they possess about the given image. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",22503-22519,10.1007/s11042-023-16261-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166915758&doi=10.1007%2fs11042-023-16261-5&partnerID=40&md5=7b79eb16231924d2979e00931c1ed359,2020
A Multidataset Characterization of Window-Based Hyperparameters for Deep CNN-Driven sEMG Pattern Recognition,"The control performance of myoelectric prostheses would not only depend on the feature extraction and classification algorithms but also on interactions of dynamic window-based hyperparameters (WBHP) used to construct input signals. However, the relationship between these hyperparameters and how they influence the performance of the convolutional neural networks (CNNs) during motor intent decoding has not been studied. Therefore, we investigated the impact of various combinations of WBHP (window length and overlap) employed for the construction of raw two-dimensional (2-D) surface electromyogram (sEMG) signals on the performance of CNNs when used for motion intent decoding. Moreover, we examined the relationship between the window length of the 2-D sEMG and three commonly used CNN kernel sizes. To ensure high confidence in the findings, we implemented three CNNs, which are variants of the existing models, and a newly proposed CNN model. Experimental analysis was conducted using three distinct benchmark databases, two from upper limb amputees and one from able-bodied subjects. The results demonstrate that the performance of the CNNs improved as the overlap between consecutively generated 2-D signals increased, with 75% overlap yielding the optimal improvement by 12.62% accuracy and 39.60% F1-score compared to no overlap. Moreover, the CNNs performance was better for kernel size of seven than three and five across the databases. For the first time, we have established with multiple evidence that WBHP would substantially impact the decoding outcome and computational complexity of deep neural networks, and we anticipate that this may spur positive advancement in myoelectric control and related fields. © 2023 IEEE.",131-142,10.1109/THMS.2023.3329536,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180350609&doi=10.1109%2fTHMS.2023.3329536&partnerID=40&md5=011f04d88ae6503eb7fb9b2ccf459762,2020
Time frequency distribution and deep neural network for automated identification of insomnia using single channel EEG-signals,"It is essential to have enough sleep for a healthy life; otherwise, it may lead to sleep disorders such as apnea, narcolepsy, insomnia, and periodic leg movements. A polysomnogram (PSG) is typically used to analyze sleep and identify different sleep disorders. This work proposes a novel convolutional neural network (CNN)-based technique for insomnia detection using single-channel electroencephalogram (EEG) signals instead of complex PSG. Morlet wavelet-based continuous wavelet transforms and smoothed pseudo-Wigner-Ville distribution (SPWVD) are explored in the proposed method to obtain scalograms of EEG signals of duration 1s along with convolutional layers for features extraction and image classification. The Morlet transform is found to be a better time-frequency distribution. We have developed Morlet wavelet-based CNN (MWTCNNet) for the classification of healthy and insomniac patients using cyclic alternating pattern (CAP) and sleep disorder research centre (SDRC) databases with C4-A1 single-channel EEG derivation. We have used multiple cohorts/settings of the CAP and SDRC databases to analyse the performance of proposed model. The proposed MWTCNNet achieved an accuracy, sensitivity, and specificity of 98.9%, 99.03%, and 98.66%, respectively, using the CAP database, and 99.03%, 99.20%, and 98.87%, respectively, with the SDRC database. Our proposed model performs better than existing state-of-the-art models and can be tested on a vast, diverse database before being installed for clinical application.  © 2003-2012 IEEE.",186-194,10.1109/TLA.2024.10431420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185259388&doi=10.1109%2fTLA.2024.10431420&partnerID=40&md5=9cbbe9080cde7823629f4962a83a052b,2024
An Efficient and Robust Facial Image Encryption Algorithm for Biometric Identity Systems,"In comparison to alternative identification methods such as graphical-based passwords, textual-based passwords, and email verification, biometric authentication systems provide improved convenience and security for accessing information systems. Nevertheless, biometric-based systems, including facial recognition, are at risk of tampering attacks on their databases. To guarantee the security of stored facial images and counteract tampering database attacks, it is essential to implement a robust encryption scheme specifically designed for facial images. Through the implementation of encryption, the images become impervious to alterations by unauthorized individuals who may gain access to the storage infrastructure. This research paper introduces a robust facial image encryption algorithm specifically created to tackle database tampering attacks. The algorithm has been developed with the purpose of seamlessly integrating it into facial authentication systems. The suggested encryption technique uses the Henon map to shuffle the pixels and applies class-3 one-dimensional cellular automata rules for pixel substitution. The security analysis and experimental findings demonstrate that the proposed method offers an extensive key space and robustly withstands various attacks. The encrypted image shows a minimal correlation between adjacent pixels, indicating high effectiveness. © 2023 IEEE.",249-254,10.1109/ICCCIS60361.2023.10425178,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186525074&doi=10.1109%2fICCCIS60361.2023.10425178&partnerID=40&md5=2499a01aa068bbcce538f1125616fe08,2024
Object recognition using cognition based decision tree clustering in multi-level artificial neural network classifier and self similarity as feature criterion,"This work showed the capability of handling large number of classes for classification with human cognition inspired methods. A cognition based techniques for both feature extraction, (self-similarity feature, Intensity Level Multi Fractal Dimension (ILMFD)) as well as classification purpose (decision tree clustering based multi-level Artificial Neural Network classifier-MLANN-DTC) were employed to implement facial recognition based object detection system. A DTC based approach reduces the search space time and also provides opportunity for very less amount of classes (a smaller part of the large number of classes) to be handled by the respective classifier for classification. It also mimics fast recognition capability of humans. In this work, two different databases were used for experiment, first one is our own collected facial images from rotation based video clips (117 persons and 40 facial images per person) named as NS database, and other is standard ORL database (40 persons and 10 facial images per person). In pre-processing step, the facial images were segmented to obtain facial part using context window based texture of pixels (CWTP) & back-propagation neural network (BPNN) based model and then a scale and rotation independent ILMFD feature was computed from each segmented image. Further, a combination of K-means and hierarchal clustering was used to build super classes. All classes’ data were distributed among these 6 super classes (heuristically chosen) for own NS database and 3 for ORL database as per their similarity based on ILMFD features. Multi-level ANNs models were employed for all super classes and further their classification results were fed into decision clustering based model to obtain fine-tuned results, which showed significant improvement in terms of classification efficiency. This approach believes in center tendency of largest cluster to refer the actual class decision from multiple decisions obtain corresponding to multiple input data of the same class. In this work, the MLANN-DTC based proposed model has produced 89.542 ± 1.167% and 87.098 ± 2.066% classification efficiency (± standard deviation) for single input and for group based decision (decision clustering), 95.042 ± 0.719% and 89 ± 2.549% for NS and ORL database respectively. This improved classification results motivate its application for other object recognition and classification problems. The basic idea of this work also supports better handling of classification which deals with a large number of classes. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11042-024-18691-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186948667&doi=10.1007%2fs11042-024-18691-1&partnerID=40&md5=e22d4c7918f908443e0b2931bbc64f85,2022
Utilization of information from CNN feature maps for offline word-level writer identification,"This paper proposes a text-independent method for authorship identification using handwritten word images. Our method is text-independent and imposes no limitations on the size of the input word images being analyzed. To begin with, the SIFT algorithm is utilized to extract key-point regions (fragments) across different levels of abstraction, encompassing allographs, characters, or character combinations. These fragments are subsequently processed through a CNN network, yielding feature maps corresponding to convolution layers. The information in these maps is then transformed into a fixed-dimension representation using a modified version of the HOG feature descriptor. The noteworthy contribution of our proposal lies in harnessing additional cues from CNN's feature maps for writer identification. We propose a measure to gauge the importance or ‘saliency’ of feature maps within a CNN layer during training. This measure originates from applying Sparse Principal Component Analysis (SPCA) to Histogram Of Gradient (HOG) features. Once saliency values are obtained, we combine them with HOG representations to create writer descriptors tailored to a CNN layer. The derived descriptors are then passed through a set of SVM classifiers that are scored at two levels to determine the identity of the handwritten word image. The efficacy of our system has been demonstrated on the word-level data of the CVL, IAM and CERUG-EN datasets with an accuracy of 82.10%, 87.68%, and 75.80% respectively. The results obtained are shown to be promising when compared with previous works. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.121709,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173466619&doi=10.1016%2fj.eswa.2023.121709&partnerID=40&md5=c4cf8a6f6192c300defbbd43f26c40b4,2022
Performance Analysis of Forensic Programs on SQLite Corpus,"In this study, the objective was to meticulously assess the efficacy of multiple forensic programs when applied to a diverse corpus of SQLite databases. The study's author meticulously curated databases of varying sizes and intricate structures, striving to create a representative sample. The forensic programs were rigorously tested across different versions of SQLite, revealing nuanced performance nuances. It is crucial to acknowledge that the programs' efficiency is inherently intertwined with the unique attributes of the analyzed database and the specific version of SQLite in use. This study not only delves into the intricate details of these performance variations but also offers invaluable insights. By shedding light on the dynamic interplay between forensic tools and SQLite databases, this research equips professionals with essential knowledge, enabling them to make informed decisions when selecting tools for forensic analyses.  © 2023 IEEE.",1-6,10.1109/APWiMob59963.2023.10365603,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182947597&doi=10.1109%2fAPWiMob59963.2023.10365603&partnerID=40&md5=e10f7a94bf230bf09faddfbaf9741781,2023
Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis,"In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount. Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge. This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance. Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics. Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses. Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another. Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape. The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain. In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection. In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security. © 2023 Copyright for this paper by its authors.",250-263,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182943711&partnerID=40&md5=f5f95fd05bf42aa60918483f7b2b2346,2022
Enhancing Analytical Select Statements Using Reference Aliases,"Data analytics is an inseparable part of the current information systems. Various tools can provide the analysis and produce results in any graphical form, enclosed by the complex filtering. Behind the scenes is a data layer and methods for accessing, manipulating, and processing data. SQL language and databases can serve that. This paper deals with data processing and performance optimization by focusing on function processing and reference. It points to the existing syntax and statement execution steps but provides various enhancements and performance optimization. Existing feature management solutions include result caching, function-based indexes, virtual columns, materialized views, or optimization of the functions to be directly applicable in SQL or PL/SQL limiting context switches. Oracle Database 23c introduced various performance enhancements and a new approach to column and expression aliases. Our proposed solution focuses on identifying and extracting aliases, storing the references in the memory and database layer, optimizing the transfer between them by swapping, as well as checkpointing and function call migrations. It provides a robust layer and complex architecture enclosing the management by the transactions. Each layer is critically discussed by pointing to the performance, structural advantages, and limitations. Complexly, our proposed architecture brings significant performance benefits for complex analytical queries but can also be applied in online transaction processing.  © 2013 IEEE.",27311-27330,10.1109/ACCESS.2024.3366455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186845683&doi=10.1109%2fACCESS.2024.3366455&partnerID=40&md5=f3e33900567c911720374f926f645d01,2020
Towards Data-Based Cache Optimization of B+-Trees,"The rise of in-memory databases and systems with considerably large memories and cache sizes requires the rethinking of the proper implementation of index structures like B+-trees in such systems. While disk block-sized nodes and binary search were considered as good in the past, smaller node sizes and cache-friendly linear search within nodes can be noticeably more performant nowadays. Considering the probabilistic distribution of lookup values to the B+-tree as part of a memory-friendly and cache-aware layout is a consequent next step, which is studied in this paper. Favoring frequently visited nodes and paths in the regard of cache hits can improve the overall performance of the tree and, thus, of the entire database system. We provide such an optimized B+-tree layout, which takes the probabilistic distribution of the lookup values as a basis. Experimental evaluation shows that choosing rather small node sizes in combination with our optimization algorithm can improve the performance by up to 26 % in comparison to a default baseline. © 2023 Copyright held by the owner/author(s).",63-69,10.1145/3592980.3595316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163771038&doi=10.1145%2f3592980.3595316&partnerID=40&md5=1837707bbdbf4d1972a708878acd082f,2023
Arabic stems are being built in a database utilising an automated process.,"This study explores the use of machine learning methods to improve morphological analysis of the Arabic language. In order to create a database of Arabic stems that academics can utilize, it examines the most well-known techniques and introduces a novel tool for finding Arabic stems from a corpus made up entirely of Arabic words.We used a corpus of 30 million Arabic words to create this database. From this corpus of words, we were able to attain a precision of 94% using our stem detection method. The benchmarking strategy and these in-depth experimental findings offer a strong basis for assessing how well our novel strategy performs.  © 2023 IEEE.",-,10.1109/SITA60746.2023.10373691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331709&doi=10.1109%2fSITA60746.2023.10373691&partnerID=40&md5=2a516965b00e6a5e35b025a2c9cace2a,2022
Mining frequent temporal duration-based patterns on time interval sequential database,"Sequential databases have wide applications, such as market basket analysis, medical prediction, and sign language recognition. Most prior research is based on pointed-based sequential databases, which assume each item/event occurs instantaneously. However, in many real-world scenarios, events persist over intervals of varying durations, such as varying time intervals of a symptom or a gesture of sign language. Assigning the same weight to different times of events and neglecting the duration of events can hinder the recognition of interesting patterns, such as concurrent symptoms preceding a disease. To address these issues, this paper integrates duration with temporal patterns in interval-based sequential databases, introduces the concept of temporal duration-based patterns (TDPs), and designs two algorithms called FTDPMiner-EP (Frequent TDPMiner based on endpoint representation) and FTDPMiner-TM (Frequent TDPMiner based on triangular matrix representation) by using different extension methods to mine frequent TDPs. Due to the complex relationships between events, temporal pattern mining is more challenging than sequential pattern mining. Strategies are used in this paper to accelerate the algorithms' search process. Experiments are conducted on both real and synthetic databases, which show good performance of the two algorithms. © 2024 Elsevier Inc.",-,10.1016/j.ins.2024.120421,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186768121&doi=10.1016%2fj.ins.2024.120421&partnerID=40&md5=5e9a9ce77ed0b6be9ca97d9cbe360ee4,2021
Cooperative Memory Management for Table and Temporary Data,"The traditional paradigm for managing memory in database management systems (DBMS) treats memory used for caching table data and memory for temporary data as separate entities. This leads to inefficient utilization of the available memory capacity for mixed workloads. With memory being a significant factor in the costs of operating a DBMS, utilizing memory as efficiently as possible is highly desirable. As an alternative to the traditional paradigm, we propose managing the entire available memory in a cooperative manner to achieve better memory utilization and consequently higher cost-effectiveness for DBMSs. Initial experimental evaluation of cooperative memory management using a prototype implementation shows promising results and leads to several interesting further research directions. © 2023 Owner/Author.",-,10.1145/3596225.3596230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168869100&doi=10.1145%2f3596225.3596230&partnerID=40&md5=c5ca0c473efd4a96a45a13408d3c6f4e,2021
Parallel Quantum Rapidly-Exploring Random Trees,"In this paper, we present the Parallel Quantum Rapidly-Exploring Random Tree (Pq-RRT) algorithm, a parallel version of the Quantum Rapidly-Exploring Random Trees (q-RRT) algorithm [1]. Parallel Quantum RRT is a parallel quantum algorithm formulation of a sampling-based motion planner that uses Quantum Amplitude Amplification to search databases of reachable states for addition to a tree. In this work we investigate how parallel quantum devices can more efficiently search a database, as the quantum measurement process involves the collapse of the superposition to a base state, erasing probability information and therefore the ability to efficiently find multiple solutions. Pq-RRT uses a manager/parallel-quantum-workers formulation, inspired by traditional parallel motion planning, to perform simultaneous quantum searches of a feasible state database. We present symbolic runtime comparisons between parallel architectures, then results regarding likelihoods of multiple parallel units finding any and all solutions contained with a shared database, with and without reachability errors, allowing efficiency predictions to be made. We offer simulations in dense obstacle environments showing efficiency, density/heatmap, and speed comparisons for Pq-RRT against q-RRT, classical RRT, and classical parallel RRT. We then present Quantum Database Annealing, a database construction strategy that uses a temperature construct to define database creation over time for balancing exploration and exploitation. Authors",1-1,10.1109/ACCESS.2024.3383313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189137595&doi=10.1109%2fACCESS.2024.3383313&partnerID=40&md5=f2d95457d88e9d13677fe915c05c2584,2022
Patient Information Retrieval Based on BERT Variants and Clinical Texts in Electronic Medical Records,"Information retrieval is a task related to search a database for the most relevant similar objects to a given query object. In medicine, patient information retrieval is important to get the patients that are the most similar to a patient being considered. The resulting data can be further used for physicians to produce adaptive treatment plans as well as for other applications such as disease classification, re-Admission prediction, and stay-length prediction. Due to its significance, several traditional information retrieval approaches were applied on medical databases. However, there is still a growing need for an effective solution to patient information retrieval in the context where more and more electronic medical records and their clinical texts are captured. In this paper, we focus on this task and propose to perform local learning on the BERT-based embeddings from clinical texts of patients to achieve an effective solution. The advanced properties of BERT variants help better represent each patient using the clinical texts instead of other data types like medication codes and demographic data. The experimental results on MIMIC III database have confirmed the effectiveness of our proposed solution. Above all, the better differences between our solution and the others in F-measure are statistically significant in all the cases. © 2023 ACM.",188-194,10.1145/3617695.3617702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180130328&doi=10.1145%2f3617695.3617702&partnerID=40&md5=213cf4e3f97f7395115e60ff0cf60bcc,2023
Elastic Use of Far Memory for In-Memory Database Management Systems,"The separation and independent scalability of compute and memory is one of the crucial aspects for modern in-memory database systems (IMDBMSs) in the cloud. The new, cache-coherent memory interconnect Compute Express Link (CXL) promises elastic memory capacity through memory pooling. In this work, we adapt the well-known IMDBMS, SAP HANA, for memory pools by features of table data placement and operational heap memory allocation on far memory, and study the impact of the limited bandwidth and higher latency of CXL. Our results show negligible performance degradation for TPC-C. For the analytical workloads of TPC-H, a notable impact on query processing is observed due to the limited bandwidth and long latency of our early CXL implementation. However, our emulation shows it would be acceptably smaller with the improved CXL memory devices. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",35-43,10.1145/3592980.3595311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163708380&doi=10.1145%2f3592980.3595311&partnerID=40&md5=e2b0f79327fc6a17325e6aee8c98ba0f,2023
Implementation of Grover&#x2019;s Iterator for Quantum Searching with an Arbitrary Number of Qubits,"Grover&#x2019;s algorithm harnesses the power of quantum computing to swiftly locate specific elements in an unstructured database, outperforming classical computers in tasks like database searching. This algorithm capitalizes on the unique ability of qubits to be in both 0 and 1 states simultaneously (superposition), allowing it to scan the entire search space at once. It then boosts the probability of the target element, making it more prominent. While the foundational concepts of Grover&#x2019;s algorithm are well-documented, practical implementation using quantum operators, especially for large search spaces, remains less explored beyond basic examples with a small number of qubits. Existing general synthesis techniques often involve numerous operators or are time-consuming. Our proposed methods specifically address the amplitude-amplification component of Grover&#x2019;s algorithm for any size of search space. These methods detail the required types and quantities of qubits and operators, emphasizing minimal usage and efficient assembly. Developed and evaluated in Python, our methods consistently identified target elements with over 95% accuracy and achieved configurations comparably compact as those in the existing literature but at a faster pace. We anticipate that these methods facilitate practical implementations of Grover&#x2019;s algorithm across various domains. Authors",1-1,10.1109/ACCESS.2024.3380198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188966844&doi=10.1109%2fACCESS.2024.3380198&partnerID=40&md5=081e373bdf98fdacf014252f32e33a40,2021
A Database for Kitchen Objects: Investigating Danger Perception in the Context of Human-Robot Interaction,"In the future, humans collaborating closely with cobots in everyday tasks will require handing each other objects. So far, researchers have optimized human-robot collaboration concerning measures such as trust, safety, and enjoyment. However, as the objects themselves influence these measures, we need to investigate how humans perceive the danger level of objects. Thus, we created a database of 153 kitchen objects and conducted an online survey (N=300) investigating their perceived danger level. We found that (1) humans perceive kitchen objects vastly differently, (2) the object-holder has a strong effect on the danger perception, and (3) prior user knowledge increases the perceived danger of robots handling those objects. This shows that future human-robot collaboration studies must investigate different objects for a holistic image. We contribute a wiki-like open-source database to allow others to study predefined danger scenarios and eventually build object-aware systems: https://hri-objects.leusmann.io/. © 2023 Owner/Author.",-,10.1145/3544549.3585884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158169270&doi=10.1145%2f3544549.3585884&partnerID=40&md5=bb997a2680406b55989a86f72cd47922,2022
An Improved Reversible Database Watermarking Method based on Histogram Shifting,"Database watermarking is typically employed to address the issues of data theft, illegal replication, and copyright infringement that may arise during the sharing of databases. Unfortunately, the existing methods often cause permanent distortion to the original data, and it is challenging to strike a balance between the watermark embedding capacity and data distortion. Therefore, this paper proposes a reversible database watermarking method based on histogram shifting, rhombus prediction, and double embedding with high capacity and low distortion, called RPDE-HSW. By utilizing the rhombus prediction, we respectively constructed two prediction error histograms in each subgroup and expanded the watermark capacity through the adoption of double-layer embedding and single-bin embedding 2 bits. A scrambling algorithm is used to make the attribute value distribution more discretized, resulting in a sparse distribution of the database histogram. Subsequently, we optimized the selection rules for the watermark embedding carrier, effectively eliminating the redundant distortion caused by histogram shifting. Experimental results demonstrate that the proposed method achieves smaller data distortion and higher watermark embedding capacity, outperforming some other state-of-the-art works, and does not affect the classification results and data mining.  © 2023 ACM.",103-114,10.1145/3577163.3595091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165787808&doi=10.1145%2f3577163.3595091&partnerID=40&md5=0585bcdb7f1467b48f88f88b726ea94b,2024
The Self-Detection Method of the Puppet Attack in Biometric Fingerprinting,"Fingerprint authentication has become a staple in securing access to personal devices and sensitive information in our daily lives, with the security level of such systems being paramount. Recent attention has been drawn to the puppet attack, a forced fingerprint unlocking scenario that exploits legitimate user fingerprints for unauthorized access. Traditional authentication methods are constrained by their reliance on additional sensors and are typically limited to static authentication scenarios, lacking versatility in dynamic or mobile contexts. In this study, we employ physical modeling to elucidate puppet attack, unraveling the distinctive stress patterns and points of application associated with forced interactions. By scrutinizing the physical alterations induced during such attacks, our investigation unveils discernible changes in the texture of fingerprints, specifically reflecting variations linked to different force patterns. Consequently, we introduce a detection system that operates without the need for external sensors, solely utilizing fingerprint images to extract texture features, thereby offering a broadly applicable solution. To address the challenge posed by the absence of puppet attack samples in existing datasets, we constructed a comprehensive database, incorporating a substantial number of puppet attack fingerprints collected from 70 volunteers aged between 20 and 75. This database facilitates a more robust detection of puppet attack. Our system demonstrates accuracy rates of 85.5%, 97.2%, 86.5%, and 78.1% across four distinct scenarios within our puppet attack database. IEEE",1-1,10.1109/JIOT.2024.3365714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187310724&doi=10.1109%2fJIOT.2024.3365714&partnerID=40&md5=999200e41864715d3ee51823e5f0d4a1,2021
Intent-aware Ranking Ensemble for Personalized Recommendation,"Ranking ensemble is a critical component in real recommender systems. When a user visits a platform, the system will prepare several item lists, each of which is generally from a single behavior objective recommendation model. As multiple behavior intents, e.g., both clicking and buying some specific item category, are commonly concurrent in a user visit, it is necessary to integrate multiple single-objective ranking lists into one. However, previous work on rank aggregation mainly focused on fusing homogeneous item lists with the same objective while ignoring ensemble of heterogeneous lists ranked with different objectives with various user intents. In this paper, we treat a user's possible behaviors and the potential interacting item categories as the user's intent. And we aim to study how to fuse candidate item lists generated from different objectives aware of user intents. To address such a task, we propose an Intent-aware ranking Ensemble Learning (IntEL) model to fuse multiple single-objective item lists with various user intents, in which item-level personalized weights are learned. Furthermore, we theoretically prove the effectiveness of IntEL with point-wise, pair-wise, and list-wise loss functions via error-ambiguity decomposition. Experiments on two large-scale real-world datasets also show significant improvements of IntEL on multiple behavior objectives simultaneously compared to previous ranking ensemble models. © 2023 Copyright held by the owner/author(s).",1004-1013,10.1145/3539618.3591702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168703067&doi=10.1145%2f3539618.3591702&partnerID=40&md5=40866539fc996531533319dd7d2f2cc8,2022
Research on the Construction of a Knowledge Graph of a Certain Equipment Based on the Domestic Independent and Controllable Environment,"In the domestic Kylin Operating System, this paper develops device knowledge built using java, python 3.9, and Vue2.0 languages, used the B/S architecture. The system based on equipment knowledge platform includes database datas and database management. The system collates and integrates the device knowledge, realizing the automatic extraction of entry data and the automatic establishment and revision of knowledge graph. Database management contains knowledge management and knowledge graph management, providing the user with convenient knowledge document management, reference and learning services. At the same time, the system is also used in other industries to provide information document management, retrieval, analysis, creation of knowledge graph and other services. © 2023 IEEE.",1-7,10.1109/AIIIP61647.2023.00007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186127433&doi=10.1109%2fAIIIP61647.2023.00007&partnerID=40&md5=0a7d379537ec599d225fbbf097832131,2023
Microexpression recognition model based on non negative matrix decomposition in intelligent classroom,"At present, intelligent systems with the function of automatic micro expression recognition are gradually applied in intelligent classrooms, but there is still a problem of low recognition rate. Therefore, based on the dual graph regularization, the research constructs a joint non negative matrix decomposition algorithm model for micro expression recognition, and verifies its effectiveness. The experimental results showed that the research algorithm had the highest performance in both micro and macro expression (micro) databases, at 75.4 %. In algorithm comparison, the recognition rates of algorithm L and algorithm M in different databases were higher than those of group A and B algorithms. Among them, Algorithm 6 in Group A had the highest recognition rate in all three databases, with the highest being 57.4 %; Algorithm 12 in Group B had the highest recognition rate in Zhongke Microexpression 2 and micro and macro expression databases, with 61.1 % and 59.4 %; Algorithm 11 had the highest recognition rate in self generated micro expression databases, with 54.0 %. And algorithm L and algorithm M had a minimum of 58.5 % and a maximum of 75.4 %. In parameter sensitivity analysis, the recognition rates of parameters η, χ, and γ in all databases showed good recognition results within a certain range of values, but they would decrease when they exceeded a specific value. Overall, the algorithm model proposed in the study has high effectiveness in improving the recognition rate of micro expressions, which is significant for the micro expression recognition of students in practical intelligent classrooms. © 2024 The Author(s)",-,10.1016/j.iswa.2024.200343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186463002&doi=10.1016%2fj.iswa.2024.200343&partnerID=40&md5=8e38f63906ce59321ee1c2d771f6b6de,2020
User Disengagement-Oriented Target Enforcement for Multi-Tenant Database Systems,"Unexpected long query latency of a database system can cause domino effects on all the upstream services and severely degrade end users’ experience with unpredicted long waits, resulting in an increasing number of users disengaged with the services and thus leading to a high user disengagement ratio (UDR). A high UDR usually translates to reduced revenue for service providers. This paper proposes UTSLO, a UDR-oriented SLO guaranteed system, which enables a database system to support multi-tenant UDR targets in a cost-effective fashion through UDR-oriented capacity planning and dynamic UDR target enforcement. The former aims to estimate the feasibility of UDR targets while the latter dynamically tracks and regulates per-connection query latency distribution needed for accurate UDR target guarantee. In UTSLO, the database service capacity can be fully exploited to efficiently accommodate tenants while minimizing resources required for UDR target guarantee. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",394-409,10.1145/3620678.3624668,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178514399&doi=10.1145%2f3620678.3624668&partnerID=40&md5=1373d0f35211bf5686fcb8372aa48dc4,2021
Joint Discriminative Analysis With Low-Rank Projection for Finger Vein Feature Extraction,"Over the last decades, finger vein biometric recognition has generated increasing attention because of its high security, accuracy, and natural anti-counterfeiting. However, most of the existing finger vein recognition approaches rely on image enhancement or require much prior knowledge, which limits their generalization ability to different databases and different scenarios. Additionally, these methods rarely take into account the interference of noise elements in feature representation, which is detrimental to the final recognition results. To tackle these problems, we propose a novel jointly embedding model, called Joint Discriminative Analysis with Low-Rank Projection (JDA-LRP), to simultaneously extract noise component and salient information from the raw image pixels. Specifically, JDA-LRP decomposes the input image into noise and clean components via low-rank representation and transforms the clean data into a subspace to adaptively learn salient features. To further extract the most representative features, the proposed JDA-LRP enforces the discriminative class-induced constraint of the training samples as well as the sparse constraint of the embedding matrix to aggregate the embedded data of each class in their respective subspace. In this way, the discriminant ability of the jointly embedding model is greatly improved, such that JDA-LRP can be adapted to multiple scenarios. Comprehensive experiments conducted on three commonly used finger vein databases and four palm-based biometric databases illustrate the superiority of our proposed model in recognition accuracy, computational efficiency, and domain adaptation.  © 2005-2012 IEEE.",959-969,10.1109/TIFS.2023.3326364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174829773&doi=10.1109%2fTIFS.2023.3326364&partnerID=40&md5=6b0a952fe60edd7d00263c354f985af3,2024
Exploiting Unlabeled RSSI Fingerprints in Multi-Building and Multi-Floor Indoor Localization through Deep Semi-Supervised Learning Based on Mean Teacher,"Conventional indoor localization techniques, based on Wi-Fi fingerprinting under supervised learning (SL), cannot exploit unlabeled received signal strength indicators (RSSIs) measured at unknown locations. Unlabeled RSSIs could be (1) part of an initial, static fingerprint database, which are submitted by volunteers during the offline phase when the database is constructed, or (2) newly measured ones submitted by the users of an indoor localization system already deployed in the field during the online phase. In this paper, a new indoor localization framework is proposed exploiting unlabeled RSSI fingerprints in multi-building and multi-floor indoor localization through deep semi-supervised learning (SSL) based on the Mean Teacher method. The proposed framework consists of three neural network models trained in two phases, i.e., an initial model in pre-Training and the student and teacher models in semi-supervised training. The pre-Training phase aims to train the initial model with labeled data, for a limited number of epochs, to mitigate the cold start problem and expedite the subsequent semi-supervised training. During the semi-supervised training, the student and the teacher models, which are cloned from the pre-Trained initial model, are trained with unlabeled as well as labeled data for fine tuning of their weights. To evaluate the performance of the proposed framework, experiments are conducted with the scalable indoor localization model based on a deep neural network (DNN) and the UJIIndoorLoc database, both of which are well-Accepted benchmarks in multi-building and multi-floor indoor localization. Different real-world scenarios are simulated with both labeled and unlabeled data by randomly splitting the data in the UJIIndoorLoc database into labeled and unlabeled data. The results show that the proposed framework can improve the indoor localization performance of the adopted backbone network-i.e., the scalable DNN model-by up to 12.78% in terms of the minimum weighted three-dimensional localization error when only 25% of RSSI fingerprints are labeled. The localization performance of the proposed framework, in this case, is nearly equivalent to that of the scalable DNN model under the conventional framework based on SL with 100% labeled data thanks to its capability of exploiting unlabeled data through deep SSL.  © 2023 IEEE.",155-160,10.1109/CANDAR60563.2023.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184997392&doi=10.1109%2fCANDAR60563.2023.00028&partnerID=40&md5=c9c7b8bdd0db26cd73d35cb304b4d3c6,2021
Construction of a linguistically interactive thesaurus for English second language acquisition based on an eigenvalue-fitting superiority algorithm,"This paper completes the overall design of a linguistic interactive terminology database based on the characteristics of second language acquisition and terminology and completes the construction of the terminology database by combining a goodness-of-fit detection algorithm based on terminology eigenvalue extraction. The efficiency of terminology information recognition is analyzed and compared with the terminology conversion rate of the eigenvalue goodness-of-fit algorithm using a neural network learning model of long and short-term memory to optimize the performance of the terminology database. The metric approach's classifier performance evaluation metrics are used to compare the accuracy and recall of the two algorithms accurately. The results show that the accuracy of the fitted superiority classifier with the application of word eigenvalue embedding compared to the LSTM classifier for the classification of electric power terms is improved by about 11% in all categories, and the average accuracy of the classifier exceeds 76.5%. © 2023 Xuan Li and Hongxia Zheng, published by Sciendo.",-,10.2478/amns.2023.2.00811,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175353707&doi=10.2478%2famns.2023.2.00811&partnerID=40&md5=44b73a7102585734fa75c85e8ea881c4,2022
SperMD: the expression atlas of sperm maturation,"The impairment of sperm maturation is one of the major pathogenic factors in male subfertility, a serious medical and social problem affecting millions of global couples. Regrettably, the existing research on sperm maturation is slow, limited, and fragmented, largely attributable to the lack of a global molecular view. To fill the data gap, we newly established a database, namely the Sperm Maturation Database (SperMD, http://bio-add.org/SperMD). SperMD integrates heterogeneous multi-omics data (170 transcriptomes, 91 proteomes, and five human metabolomes) to illustrate the transcriptional, translational, and metabolic manifestations during the entire lifespan of sperm maturation. These data involve almost all crucial scenarios related to sperm maturation, including the tissue components of the epididymal microenvironment, cell constituents of tissues, different pathological states, and so on. To the best of our knowledge, SperMD could be one of the limited repositories that provide focused and comprehensive information on sperm maturation. Easy-to-use web services are also implemented to enhance the experience of data retrieval and molecular comparison between humans and mice. Furthermore, the manuscript illustrates an example application demonstrated to systematically characterize novel gene functions in sperm maturation. Nevertheless, SperMD undertakes the endeavor to integrate the islanding omics data, offering a panoramic molecular view of how the spermatozoa gain full reproductive abilities. It will serve as a valuable resource for the systematic exploration of sperm maturation and for prioritizing the biomarkers and targets for precise diagnosis and therapy of male subfertility. © 2024, The Author(s).",-,10.1186/s12859-024-05631-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182440952&doi=10.1186%2fs12859-024-05631-x&partnerID=40&md5=55cd649b3d6013c8717578b48165ef8d,2021
POSTER: RadiK: Scalable Radix Top-K Selection on GPUs,"By identifying the k largest or smallest elements in a set of data, top-k selection is critical for modern high-performance databases and machine learning systems, especially with large data volumes. However, previous studies on its GPU implementation are mostly merge-based and rely heavily on the high-speed but size-limited on-chip memory, thereby resulting in a restricted upper bound on k. This paper introduces RadiK, a highly optimized GPU-parallel radix top-k selection that is scalable with k, input length, and batch size. With a carefully designed optimization framework targeting high memory bandwidth and resource utilization, RadiK supports far larger k than the prior art, achieving up to 2.5× speedup for non-batch queries and up to 4.8× speedup for batch queries. We also propose a lightweight refinement that strengthens the robustness of RadiK against skewed distributions by adaptively scaling the input elements. © 2024 Copyright held by the owner/author(s).",472-474,10.1145/3627535.3638478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187198053&doi=10.1145%2f3627535.3638478&partnerID=40&md5=f4415649ab03e0985ce6bc5e3dd2de7a,2020
"A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications","With the rapid evolution of the Internet, Internet of Things (IoT), and geographic information systems (GIS), spatio-temporal Big Data (STBD) is experiencing exponential growth, marking the onset of the STBD era. Recent studies have concentrated on developing algorithms and techniques for the collection, management, storage, processing, analysis, and visualization of STBD. Researchers have made significant advancements by enhancing STBD handling techniques, creating novel systems, and integrating spatio-temporal support into existing systems. However, these studies often neglect resource management and system optimization, crucial factors for enhancing the efficiency of STBD processing and applications. Additionally, the transition of STBD to the innovative Cloud-Edge-End unified computing system needs to be noticed. In this survey, we comprehensively explore the entire ecosystem of STBD analytics systems. We delineate the STBD analytics ecosystem and categorize the technologies used to process GIS data into five modules: STBD, computation resources, processing platform, resource management, and applications. Specifically, we subdivide STBD and its applications into geoscience-oriented and human-social activity-oriented. Within the processing platform module, we further categorize it into the data management layer (DBMS-GIS), data processing layer (BigData-GIS), data analysis layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource management module and each layer in the processing platform are classified into three categories: task-oriented, resource-oriented, and cloud-based. Finally, we propose research agendas for potential future developments.  © 2015 IEEE.",174-193,10.1109/TBDATA.2023.3342619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179815991&doi=10.1109%2fTBDATA.2023.3342619&partnerID=40&md5=af22a205ae775f07c0f85a9dc74d1ce2,2020
Fine-grained grid computing model for Wi-Fi indoor localization in complex environments,"The fingerprinting-based approach using the wireless local area network (WLAN) is widely used for indoor localization. However, the construction of the fingerprint database is quite time-consuming. Especially when the position of the access point (AP) or wall changes, updating the fingerprint database in real-time is difficult. An appropriate indoor localization approach, which has a low implementation cost, excellent real-time performance, and high localization accuracy and fully considers complex indoor environment factors, is preferred in location-based services (LBSs) applications. In this paper, we proposed a fine-grained grid computing (FGGC) model to achieve decimeter-level localization accuracy. Reference points (RPs) are generated in the grid by the FGGC model. Then, the received signal strength (RSS) values at each RP are calculated with the attenuation factors, such as the frequency band, three-dimensional propagation distance, and walls in complex environments. As a result, the fingerprint database can be established automatically without manual measurement, and the efficiency and cost that the FGGC model takes for the fingerprint database are superior to previous methods. The proposed indoor localization approach, which estimates the position step by step from the approximate grid location to the fine-grained location, can achieve higher real-time performance and localization accuracy simultaneously. The mean error of the proposed model is 0.36 ​m, far lower than that of previous approaches. Thus, the proposed model is feasible to improve the efficiency and accuracy of Wi-Fi indoor localization. It also shows high-accuracy performance with a fast running speed even under a large-size grid. The results indicate that the proposed method can also be suitable for precise marketing, indoor navigation, and emergency rescue. © 2024 The Authors",-,10.1016/j.jnlest.2024.100234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184005334&doi=10.1016%2fj.jnlest.2024.100234&partnerID=40&md5=1dc61f4e96b32331b9ac6b836e972f24,2024
Adapting Performance Analytic Techniques in a Real-World Database-Centric System: An Industrial Experience Report,"Database-centric architectures have been widely adopted in large-scale software systems in various domains to deal with the ever-increasing amount and complexity of data. Prior studies have proposed a wide range of performance analytic techniques aimed at assisting developers in pinpointing software performance inefficiencies and diagnosing performance issues. However, directly applying these existing techniques to large-scale database-centric systems can be challenging and may not perform well due to the unique nature of such systems. In particular, compared to typical database-based systems like online shopping systems, in database-centric systems, a majority of the business logic and calculations reside in the database instead of the application. As the calculations in the database typically use domain-specific languages such as SQL, the performance issues of such systems and their diagnosis may be significantly different from the systems dominated by traditional programming languages such as Java. In this paper, we share our experience of adapting performance analytic techniques in a large-scale database-centric system from our industrial collaborator. Our adapted performance analysis pays special attention to the database and the interactions between the database and the application with minimal reliance on expert knowledge and manual effort. Moreover, we document our encountered challenges and how they are addressed during the development and adoption of our solution in the industrial setting as well as the corresponding lessons learned. We also discuss the real-world performance issues detected by applying our analysis to the target database-centric system. We anticipate that our solution and the reported experience can be helpful for practitioners and researchers who would like to ensure and improve the performance of database-centric systems. © 2023 ACM.",1855-1866,10.1145/3611643.3613893,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180552047&doi=10.1145%2f3611643.3613893&partnerID=40&md5=1d3585440fb9393ef92ad45824ee06ac,2021
Panoramic radiograph quality assessment: Database and algorithm,"High-quality panoramic radiographs are crucial for providing accurate diagnosis and appropriate treatment to the patients, which have been widely employed in dentist clinic. Unfortunately, panoramic radiographs can be bothered with various distortions during the capture and imaging process, which can further negatively affect the judgment of dentists. Therefore, to deal with the challenge of panoramic radiographs quality assessment, we first carry out a large scale panoramic radiograph quality assessment database, which contains 2009 images labeled by experienced dentists. Then we propose a novel objective quality assessment method based on the convolutional neural network (CNN). Specifically, the multi-scale features from the different stages of the CNN are employed to make full use of both low-level and high-level information. Moreover, weight-sharing auxiliary networks are utilized to improve the quality understanding of the proposed method. The experimental results show that the proposed method outperforms all the compared methods for panoramic radiographs quality assessment tasks and is more capable of handling the practical medical diagnosis situations. The proposed method can also be used as preliminary screening tool and provide useful guidelines for the dentists. © 2023",-,10.1016/j.displa.2023.102625,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183454157&doi=10.1016%2fj.displa.2023.102625&partnerID=40&md5=37628c84fb0088ee9c58f7c6f39f50f0,2024
IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators,"This study focuses on media bias detection, crucial in today’s era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input’s bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec’s significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework’s effectiveness. © 2024 Association for Computational Linguistics.",1038-1050,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188708735&partnerID=40&md5=3222836b80641d86a2ece4ece35ebf6f,2021
Doubly Efficient Private Information Retrieval and Fully Homomorphic RAM Computation from Ring LWE,"A (single server) private information retrieval (PIR) allows a client to read data from a public database held on a remote server, without revealing to the server which locations she is reading. In a doubly efficient PIR (DEPIR), the database is first preprocessed, but the server can subsequently answer any client's query in time that is sub-linear in the database size. Prior work gave a plausible candidate for a public-key variant of DEPIR, where a trusted party is needed to securely preprocess the database and generate a corresponding public key for the clients; security relied on a new non-standard code-based assumption and a heuristic use of ideal obfuscation. In this work we construct the stronger unkeyed notion of DEPIR, where the preprocessing is a deterministic procedure that the server can execute on its own. Moreover, we prove security under just the standard ring learning-with-errors (RingLWE) assumption. For a database of size N and any constant ϵ>0, the preprocessing run-time and size is O(N1+ϵ), while the run-time and communication-complexity of each PIR query is polylog(N). We also show how to update the preprocessed database in time O(Nϵ). Our approach is to first construct a standard PIR where the server's computation consists of evaluating a multivariate polynomial; we then convert it to a DEPIR by preprocessing the polynomial to allow for fast evaluation, using the techniques of Kedlaya and Umans (STOC '08). Building on top of our DEPIR, we construct general fully homomorphic encryption for random-access machines (RAM-FHE), which allows a server to homomorphically evaluate an arbitrary RAM program P over a client's encrypted input x and the server's preprocessed plaintext input y to derive an encryption of the output P(x,y) in time that scales with the RAM run-time of the computation rather than its circuit size. Prior work only gave a heuristic candidate construction of a restricted notion of RAM-FHE. In this work, we construct RAM-FHE under the RingLWE assumption with circular security. For a RAM program P with worst-case run-time T, the homomorphic evaluation runs in time T1+ϵ · (|x| + |y|). © 2023 ACM.",595-608,10.1145/3564246.3585175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163140747&doi=10.1145%2f3564246.3585175&partnerID=40&md5=0c63b9b9a48449d26b1cf131e3fd60cc,2021
The Establishment and Management of Mass Appraisal Database,"As a complicate and systemic program, the mass appraisal of real estate relies heavily upon data. The accuracy, integrity and applicability of data greatly affects the effectiveness of appraisal work. Nowadays, because of the diversity of data acquisition ways, real estate data increase explosively, which brings challenges to data collection, storage, management, sharing and usage. To normalize the real estate data, thus guarantee the precise and completeness of appraisal result, it is necessary to construct a real estate database of high-reliability, high-scalability, and high-integrity. Due to the differences in cultures and national conditions, the real estate data are various among countries. As home to more than 17% of the world's population, China is facing to huge volume real estate data of high complexity, which makes it hard to collect and manage data. On this background, by analyzing real estate data, we discuss the technology route of constructing real estate database under GIS, and study data integration technique for real estate mass appraisal. At last, we take Shenzhen city as an example and verify the effeteness of our methods.  © 2023 IEEE.",485-489,10.1109/ISCTech60480.2023.00094,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186766973&doi=10.1109%2fISCTech60480.2023.00094&partnerID=40&md5=731d5a5b4bff9363b19a65940c8c9109,2021
Cosine convolutional neural network and its application for seizure detection,"Traditional convolutional neural networks (CNNs) often suffer from high memory consumption and redundancy in their kernel representations, leading to overfitting problems and limiting their application in real-time, low-power scenarios such as seizure detection systems. In this work, a novel cosine convolutional neural network (CosCNN), which replaces traditional kernels with the robust cosine kernel modulated by only two learnable factors, is presented, and its effectiveness is validated on the tasks of seizure detection. Meanwhile, based on the cosine lookup table and KL-divergence, an effective post-training quantization algorithm is proposed for CosCNN hardware implementation. With quantization, CosCNN can achieve a nearly 75% reduction in the memory cost with almost no accuracy loss. Moreover, we design a configurable cosine convolution accelerator on Field Programmable Gate Array (FPGA) and deploy the quantized CosCNN on Zedboard, proving the proposed seizure detection system can operate in real-time and low-power scenarios. Extensive experiments and comparisons were conducted using two publicly available epileptic EEG databases, the Bonn database and the CHB-MIT database. The results highlight the performance superiority of the CosCNN over traditional CNNs as well as other seizure detection methods. © 2024 Elsevier Ltd",-,10.1016/j.neunet.2024.106267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189151972&doi=10.1016%2fj.neunet.2024.106267&partnerID=40&md5=cfc83bfd88491f58c2b8911a249b832a,2022
Total Variation PCA-Based Descriptors for Electrocardiography Identity Recognition,"Electrocardiographic (ECG) signals have been successfully used in biometric recognition. However, the accuracy of ECG-based biometric systems is generally lower than systems based on other physiological traits. This study introduces a local feature learning method aimed at enhancing the performance of ECG-based biometric recognition systems. Specifically, we first extracted the multi-scale differential feature (MDF) for each point in the training ECG heartbeats using the difference between each point and its neighboring points. Second, we learn feature mapping to project these MDFs into low-dimensional descriptors in an unsupervised manner, where 1) the errors between the original MDF and reconstructed MDF are minimized. 2) The total variation in the reconstructed MDFs is minimized. Third, we represented each ECG heartbeat as a histogram feature using clustering and pooling descriptors. Finally, we adopted global feature learning methods to obtain a representation of an ECG heartbeat. Experiments on the MIT-BIH Arrhythmia, ECG-ID, and Physikalisch Technische Bundesanstalt databases verified the performance of the proposed method over existing ECG biometric recognition methods using within-session analysis. Moreover, we evaluated the performance of the proposed method using an across-session analysis of the ECG-ID database.  © 2013 IEEE.",3815-3824,10.1109/ACCESS.2023.3349148,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181567796&doi=10.1109%2fACCESS.2023.3349148&partnerID=40&md5=197e66dcdc18856dc419337d2224b5e5,2021
A Lightweight Hybrid Model Using Multiscale Markov Transition Field for Real-Time Quality Assessment of Photoplethysmography Signals,"Objective: The proliferation of wearable devices has escalated the standards for photoplethysmography (PPG) signal quality. This study introduces a lightweight model to address the imperative need for precise, real-time evaluation of PPG signal quality, followed by its deployment and validation utilizing our integrated upper computer and hardware system. Methods: Multiscale Markov Transition Fields (MMTF) are employed to enrich the morphological information of the signals, serving as the input for our proposed hybrid model (HM). HM undergoes initial pre-training utilizing the MIMIC-III and UCI databases, followed by fine-tuning the Queensland dataset. Knowledge distillation (KD) then transfers the large-parameter model's knowledge to the lightweight hybrid model (LHM). LHM is subsequently deployed on the upper computer for real-time signal quality assessment. Results: HM achieves impressive accuracies of 99.1% and 96.0% for binary and ternary classification, surpassing current state-of-the-art methods. LHM, with only 0.2 M parameters (0.44% of HM), maintains high accuracy despite a 2.6% drop. It achieves an inference speed of 0.023 s per image, meeting real-time display requirements. Furthermore, LHM attains a 97.7% accuracy on a self-created database. HM outperforms current methods in PPG signal quality accuracy, demonstrating the effectiveness of our approach. Additionally, LHM substantially reduces parameter count while maintaining high accuracy, enhancing efficiency and practicality for real-time applications. Conclusion: The proposed methodology demonstrates the capability to achieve high-precision and real-time assessment of PPG signal quality, and its practical validation has been successfully conducted during deployment. Significance: This study contributes a convenient and accurate solution for the real-time evaluation of PPG signals, offering extensive application potential.  © 2013 IEEE. ",1078-1088,10.1109/JBHI.2023.3331975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177031651&doi=10.1109%2fJBHI.2023.3331975&partnerID=40&md5=1596112bfccfe522a7c259c3431459c3,2020
PSC-Net: Integration of Convolutional Neural Networks and transformers for Physiological Signal Classification,"Objective: Physiological signals, such as electrocardiogram (ECG) and wrist pulse signals (WPS), play an important role in diagnosing and preventing cardiovascular and other physiological diseases. Therefore, accurate classification of physiological signals has become the key to assist physicians in diagnosis. However, this field still faces several prominent challenges, including limited availability of data, imbalanced datasets, convergence issues with loss functions, and the need for model architectures capable of accurately detecting waveform patterns. Methods: This study introduces the Physiological Signal Classification Network (PSC-Net), which combines the strengths of Convolutional Neural Networks (CNNs) and transformers for applications in medical artificial intelligence. Specifically, local temporal features are extracted using the GRWA-LSTM network (GLNet) proposed in this paper. Within the transformer, two GRU layers replace the fully-connected layer to enhance global feature extraction for physiological signal data. Residual connection integrates the outputs of GLNet and Transformer through global average pooling and weight settings. To address challenges related to small and imbalanced datasets, we propose an enhanced data augmentation algorithm based on SMOTE Tomek, along with an improved loss function. Additionally, automatic learning rates are optimized using the Dung Beetle Algorithm (DBA). Results: Our proposed method achieves superior accuracies of 83.33%, 100.0%, 95.74%, and 98.85% on four physiological signal datasets (including one clinical dataset): Five Types of Pulses Database, Coronary Heart Disease (CHD) database, MIT-BIH Arrhythmia Database, and MIT-BIH ST Change Database. These results attest to the model's robust generalization capability and its promising application prospects in assisting diagnoses. © 2024",-,10.1016/j.bspc.2024.106040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184136811&doi=10.1016%2fj.bspc.2024.106040&partnerID=40&md5=ca79224e823f3ddf43fcc346b4f110ff,2021
Closing the Performance Gap between Leveling and Tiering Compaction via Bundle Compaction,"So far, most LSM-tree-based storage engines adopt either leveling or tiering compaction. We note that while leveling compaction can deliver high search performance and low space amplification, it has a high rate of write amplification (therefore delivering poor write performance). On the other hand, tiering compaction has a low rate of write amplification (therefore delivering good write performance) but has poor search performance and high space amplification. Aiming to close the performance gap between leveling and tiering databases, this paper proposes a new storage engine called B+LSM. The novel ideas of B+LSM lie in two aspects: (1) B+LSM replaces the underlying level structure of LSM-tree with a B+-tree-like tree, and each tree node is defined as a Bundle Compaction Unit (BCU), whose size is allowed to be dynamically changed with workload statistics to balance read and write performance. (2) B+LSM proposes a new node-grained compaction scheme called Bundle Compaction. Bundle compaction is always triggered to merge all the data within a BCU node, partition them into bundles, and then send bundles to the children. Such a compaction scheme can take advantage of leveling and tiering compaction by auto-tuning the size of BCU nodes. We implemented B+LSM and compared it with LevelDB, RocksDB, PebblesDB, and L2SM on the YCSB workloads. The results show that B+LSM can achieve high time performance and reduce space amplification on both static and dynamic workloads.  © 2023 ACM.",143-154,10.1145/3588195.3592982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169606779&doi=10.1145%2f3588195.3592982&partnerID=40&md5=53c0c8da8109c02e9010d68493867238,2021
Digital construction of higher education management based on multimodal machine database,"In this paper, we first study the multimodal interaction activities in higher education management from the two points of multimodal interaction types and multimodal interaction activity evaluation and propose the perfect way of digital construction of multimodal higher education management from the perspective of students, teachers and higher education institutions. Then, the higher education management system is constructed based on semi-supervised fusion feature learning and homogeneous multimodal features of multimodal machines, and the system architecture and database design are explained in detail. Next, the research literature statistics and system performance of 'Digitalization of Higher Education Management' are analyzed through experimental research. The results show that the periodicals' literature statistics show a rapid increase in articles, mainly from 2003 to 2004 and 2006-2007. The higher education management system maintained an accuracy rate of 80.49% regarding system performance. It is confirmed that the management system constructed in this study can predict the learners' status more accurately and predictably, contributing to the development of intelligent education management in higher education institutions. © 2024 Authors. All rights reserved.",-,10.2478/amns.2023.2.00862,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175353358&doi=10.2478%2famns.2023.2.00862&partnerID=40&md5=7eba53c58c3109d89e4054708fbe4ba8,2021
Assessing Face Image Quality: A Large-scale Database and a Transformer Method,"The amount of face images has been witnessing an explosive increase in the last decade, where various distortions inevitably exist on transmitted or stored face images. The distortions lead to visible and undesirable degradation on face images, affecting their quality of experience (QoE). To address this issue, this paper proposes a novel Transformer-based method for quality assessment on face images (named as TransFQA). Specifically, we first establish a large-scale face image quality assessment (FIQA) database, which includes 42,125 face images with diversifying content at different distortion types. Through an extensive crowdsource study, we obtain 712,808 subjective scores, which to the best of our knowledge contribute to the largest database for assessing the quality of face images. Furthermore, by investigating the established database, we comprehensively analyze the impacts of distortion types and facial components (FCs) on the overall image quality. Accordingly, we propose the TransFQA method, in which the FC-guided Transformer network (FT-Net) is developed to integrate the global context, face region and FC detailed features via a new progressive attention mechanism. Then, a distortion-specific prediction network (DP-Net) is designed to weight different distortions and accurately predict final quality scores. Finally, the experiments comprehensively verify that our TransFQA method significantly outperforms other state-of-the-art methods for quality assessment on face images. IEEE",1-18,10.1109/TPAMI.2024.3350049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182369714&doi=10.1109%2fTPAMI.2024.3350049&partnerID=40&md5=58cd8be4207cfef8871b34f12c2a00b7,2022
Tree-Convolutional-Transformer: A Hybrid Model for Query Plan Representation,"Optimizing a physical execution plan is a crucial task in enhancing the query performance of a database. Especially, the efficient vectorization of a query plan is a pivotal challenging problem. Machine learning has risen as a dominant approach, but, two main limitations are still existing. Firstly, struggling to encapsulate the interdependencies among nodes within the plan. Secondly, underestimating the abundant database statistics. In this paper, we propose a novel tree-structured model, termed Tree-Convolutional-Transformer incorporating convolutional operations and attention mechanisms, to effectively learn interdependencies among nodes. Furthermore, in order to enrich the learned feature representation, statistical information gleaned from the database system is also integrated into the encoding of the query plan. Through comprehensive experiments on a real-world dataset (IMDb) and query workloads, results demonstrate the effectiveness and feasibility of the proposed model in capturing the dependencies among nodes within the physical execution plan tree.  © 2023 ACM.",126-133,10.1145/3637494.3637516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185529661&doi=10.1145%2f3637494.3637516&partnerID=40&md5=17cebc47977f0313d358a9a2c90d98ea,2022
A Latent Fingerprint in the Wild Database,"Latent fingerprints are among the most important and widely used evidence in crime scenes, digital forensics and law enforcement worldwide. Despite the number of advancements reported in recent works, we note that significant open issues such as independent benchmarking and lack of large-scale evaluation databases for improving the algorithms are inadequately addressed. The available databases are mostly of semi-public nature, lack of acquisition in the wild environment, and post-processing pipelines. Moreover, they do not represent a realistic capture scenario similar to real crime scenes, to benchmark the robustness of the algorithms. Further, existing databases for latent fingerprint recognition do not have a large number of unique subjects/fingerprint instances or do not provide ground truth/reference fingerprint images to conduct a cross-comparison against the latent. In this paper, we introduce a new wild large-scale latent fingerprint database that includes five different acquisition scenarios: reference fingerprints from (1) optical and (2) capacitive sensors, (3) smartphone fingerprints, latent fingerprints captured from (4) wall surface, (5) Ipad surface, and (6) aluminium foil surface. The new database consists of 1,318 unique fingerprint instances captured in all above mentioned settings. A total of 2,636 reference fingerprints from optical and capacitive sensors, 1,318 fingerphotos from smartphones, and 9,224 latent fingerprints from each of the 132 subjects were provided in this work. The dataset is constructed considering various age groups, equal representations of genders and backgrounds. In addition, we provide an extensive set of analysis of various subset evaluations to highlight open challenges for future directions in latent fingerprint recognition research. Authors",1-1,10.1109/TIFS.2024.3368892,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186084827&doi=10.1109%2fTIFS.2024.3368892&partnerID=40&md5=1d58e96d2c68c0a9f873e2aae4413a01,2020
Underground target location method based on positive image-based camera position and pose estimate,"At present, the traditional underground target positioning technology is passive positioning. It needs to rely on external equipment such as communication networks, hosts and reference base stations with known position coordinates to achieve target positioning. The location system is complex and the ability to resist disasters is weak. To get rid of the problems existing in the passive positioning system, this paper proposes an underground target location method based on a positive image-based camera position and pose estimate. The method consists of two stages: an offline stage and a real-time positioning stage. In the offline stage, a number of identification signboards are arranged at known positions in the tunnel, and the coordinate position information of each identification signboard is recorded, and it is built into the identification signboard-position information database. In the real-time positioning stage, the main body of this stage is the camera, and the camera, video analysis module, and storage module storing the signboard-position information database is embedded into the embedded system, and the embedded system is carried by the positioning target. The embedded system uses the camera and video analysis module to find one of the signboards, finds the coordinate position information of the signboard in the signboard-position information database, and establishes the signboard coordinate system with the coordinate position as the origin; according to the angle The coordinate value of the point in the pixel coordinate system and the signboard coordinate system, as well as the PnP algorithm, calculate the coordinate value of the corner point in the camera coordinate system. Through the different coordinate values of the same corner point in the camera coordinate system and the signboard coordinate system, the mutual relationship between the two coordinate systems is solved by inversion, that is, the camera extrinsic parameters. The relative pose of the camera in the signboard coordinate system is estimated by the camera extrinsic parameters. The above method can measure the coordinates of the target in the well and the orientation of the target without the help of base station, host, and communication networks, and has the characteristics of strong disaster resistance and simple system structure.  © 2023 ACM.",231-238,10.1145/3614008.3614044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178005356&doi=10.1145%2f3614008.3614044&partnerID=40&md5=5b31bec81066cba037d1669fb5b79fc2,2023
Sampling over Union of Joins,"Data scientists often draw on multiple relational data sources for analysis. A standard assumption in learning and approximate query answering is that the data is a uniform and independent sample of the underlying distribution. To avoid the cost of join and union, given a set of joins, we study the problem of obtaining a random sample from the union of joins without performing the full join and union. We present a general framework for random sampling over the set union of chain, acyclic, and cyclic joins, with sample uniformity and independence guarantees. We study the novel problem of union of joins size evaluation and propose two approximation methods based on histograms of columns and random walks on data. We propose an online union sampling framework that initializes with cheap-to-calculate parameter approximations and refines them on the fly during sampling. We evaluate our framework on workloads from the TPC-H benchmark and explore the trade-off of the accuracy of union approximation and sampling efficiency.  © 2023 Owner/Author.",273-275,10.1145/3555041.3589400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162900368&doi=10.1145%2f3555041.3589400&partnerID=40&md5=67bb3774825faa4a268e285a6a80efce,2023
Underwater Image Quality Assessment: Benchmark Database and Objective Method,"Underwater image quality assessment (UIQA) plays a crucial role in monitoring and detecting the quality of acquired underwater images in underwater imaging systems. Currently, the investigation of UIQA encounters two major challenges. First, a lack of large-scale UIQA databases for benchmarking UIQA algorithms remains, which greatly restricts the development of UIQA research. The other limitation is that there is a shortage of effective UIQA methods that can faithfully predict underwater image quality. To alleviate these two challenges, in this paper, we first construct a large-scale UIQA database (UIQD). Specifically, UIQD contains a total of 5369 authentic underwater images that span abundant underwater scenes and typical quality degradation conditions. Extensive subjective experiments are executed to annotate the perceived quality of the underwater images in UIQD. Based on an in-depth analysis of underwater image characteristics, we further establish a novel baseline UIQA metric that integrates channel and spatial attention mechanisms and a transformer. Channel- and spatial attention modules are used to capture the image channel and local quality degradations, while the transformer module characterizes the image quality from a global perspective. Multilayer perception is employed to fuse the local and global feature representations and yield the image quality score. Extensive experiments conducted on UIQD demonstrate that the proposed UIQA model achieves superior prediction performance compared with the state-of-the-art UIQA and IQA methods. The proposed UIQD and UIQA models will be released at <uri>https://github.com/YT2015?tab=repositories.</uri> IEEE",1-14,10.1109/TMM.2024.3371218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186969398&doi=10.1109%2fTMM.2024.3371218&partnerID=40&md5=a63567e4bec6b5c99d9085685560c5c3,2021
TM-search: An Efficient and Effective Tool for Protein Structure Database Search,"The quickly increasing size of the Protein Data Bank is challenging biologists to develop a more scalable protein structure alignment tool for fast structure database search. Although many protein structure search algorithms and programs have been designed and implemented for this purpose, most require a large amount of computational time. We propose a novel protein structure search approach, TM-search, which is based on the pairwise structure alignment program TM-align and a new iterative clustering algorithm. Benchmark tests demonstrate that TM-search is 27 times faster than a TM-align full database search while still being able to identify ∼90% of all high TM-score hits, which is 2-10 times more than other existing programs such as Foldseek, Dali, and PSI-BLAST. © 2024 American Chemical Society.",1043-1049,10.1021/acs.jcim.3c01455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184834886&doi=10.1021%2facs.jcim.3c01455&partnerID=40&md5=e2edc2c9a5b44f38bff14a8b0bd383ff,2020
Are You the Main Character?: Visibility Labor and Attributional Practices on TikTok,"This paper revisits hypertext theory from the 1990s (George Landow, Jay David Bolter, etc.) and database theory from the 2000s (Lev Manovich, Victoria Vesna, etc.) with attention to explaining new authoring practices on the video-sharing platform TikTok. Because hyperlinking is automated on the platform whenever composers select audio clips, effects, hashtags, and author references to other videos for ""dueting""and ""stitching""to remix from pre-existing databases of material, TikTok is characterized by rich attributional practices of citation. At the same time, the site's users are keenly aware that search and recommendation algorithms may obfuscate their published materials and that additional labor may be required for their contributions to be visible in the larger hyperlinked matrix of database participation. At the same time, users may also choose to de-link their content or the content of others. Case studies are drawn from variations of the ""main character""meme and recent moral panics about supposedly dangerous TikTok ""trends."" © 2023 Owner/Author.",-,10.1145/3603163.3609049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174267813&doi=10.1145%2f3603163.3609049&partnerID=40&md5=db95b62cfd116051d1aaf2fb28c9b8a4,2020
An Analysis of Data Mining Techniques in Software Engineering Database Design,"With the advent of Internet information technology, data mining technology has been applied in various fields of China's social and industrial development, and has promoted the quality development of the industry [1]. Nowadays, people are widely influenced by Internet computer technology, and the application of computer technology has been indispensable in life, work and study. At the same time, data mining technology arises from Internet communication technology and is used as an important technical means of operation and development by various industries, especially the application of data mining technology in university software engineering teaching is becoming more and more widespread, but there are still many unavoidable problems that require scholars to pay more attention. The article analyzes and discusses the significance of applying data mining technology in software engineering [2], and on this basis, the application path of data mining technology in software engineering database design is proposed, including: mining information, mining loopholes, mining execution records and open source. It is hoped that relevant technology users can efficiently apply data mining technology to make software engineering It is more reasonable and efficient, so that software engineering can be further developed. This paper presents the implementation and performance analysis of the teacher management decision-making system based on data mining, and introduces the implementation process of the system in detail.  © 2023 The Authors. Published by Elsevier B.V.",47-56,10.1016/j.procs.2023.11.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184136322&doi=10.1016%2fj.procs.2023.11.007&partnerID=40&md5=6366a87b48de3876320525d8cea39f43,2023
A newly developed and verified transport capacity of Monte Carlo photon particles in RMC,"In reactor shielding designs, deep-penetration shielding issues, and direct photon heating, photon transport physics is crucial. In the Reactor Monte Carlo (RMC) code, a state-of-the-art photon transport capacity is implemented in this research. For criticality and fixed-source calculations, pure photon and coupled neutron-photon transport are now simulated for four photon-atomic interactions, including Rayleigh scattering, Compton scattering, Photoelectric effect, and Pair production, as well as three major processes of the secondary photons generation (Atomic relaxation, Electron-positron annihilation, and Bremsstrahlung by electrons and positrons). In order to properly balance energy release and deposition during neutron-induced photon production, the yield of photons by neutrons is scaled by a correction factor to account for the delayed gamma radiation, and the effective multiplication factor keff is used in an uncritical system. Using an Automatic Library Generator (ALG) code, a new database for photon transport is processed with the ENDF/B evaluated nuclear data library. Numerous cases, such as a point source in an infinite geometry, a photon beam source in a cylinder geometry, and a fuel assembly in the VERA benchmark, are verified against the Monte Carlo (MC) code OpenMC. Despite some notable differences in the comparison of the photon flux at certain energy ranges, which are caused by differences in the database and also the physics implementation, the results of the energy and spatial distributions of photon flux, and the photon heating show generally good agreement and relative errors are nearly within a triple statistical standard deviation. RMC now offers advanced photon transport capacity for reaction core and shielding applications. © 2023 Elsevier B.V.",-,10.1016/j.cpc.2023.108935,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171989797&doi=10.1016%2fj.cpc.2023.108935&partnerID=40&md5=844c3461fecfb646f005d6ddea2d9730,2022
Software engineering database programming control system based on embedded system,"To improve the programming efficiency of a software database, this paper analyzes the possibility of running an embedded system in a software engineering database by constructing an embedded software data programming control system with the help of an independent time step algorithm and main function. By analyzing and calculating the format of DXF files and commonly used group code values, the format of file storage and the method of expressing data in the graphical element processing module are summarized to ensure the accuracy of software processing and the stability of the software processing process. According to the laws summarized in the analysis, the necessity of introducing an embedded system in the software data programming control process was proposed and simulated, and tested. In the test process, we focus on the process time consumption, space resource occupation rate, running accuracy, and step length data. The test results show that the maximum programming process time of the embedded software data programming control system is only 4.5s, the minimum software space resource occupation rate is 19.7%, the highest operation accuracy is 98.9%, and the calculation time per step is about 0.002s, which is significantly better than the programming system based on remote wireless synchronization system and the computer software programming system based on C language technology. The data calculation results of the independent time step algorithm and the main function prove the feasibility of introducing embedded systems in the software programming process and improving the reusability of software programming code for embedded systems.  © 2023 Ke Luo et al.;published by Sciendo.",-,10.2478/amns.2023.1.00473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167917328&doi=10.2478%2famns.2023.1.00473&partnerID=40&md5=b234cc1cd907b4ffa78c15ac1bd0e569,2024
An Efficient Partial Video Copy Detection for a Large-scale Video Database,"Partial video copy detection (PVCD) aims to discover copy segments of query videos from a video database, which plays an important role in video copyright protection, filtering, tracking, etc. For a large-scale video database, PVCD can be divided into two stages: the first stage involves searching for video-level copies of the query video in the database, and the second stage is to further localize the copy segments within the video-level copies. Thus, two major challenges arise: (1) efficiently and effectively calculating the similarity between videos; (2) localizing mixed-duration video pairs. To address the above challenges, we propose an efficient PVCD approach for a large-scale video database, based on the Bag-of-Words (BoW) framework, which decouples video-level similarity and copy localization into cell-level. This approach consists of two modules. The first is an efficient video similarity measurement (VSM) module for the large-scale video database. VSM aggregates cell-level similarity into video-level similarity, and with a dual index, it greatly improves retrieval speed while accurately measuring spatiotemporal transformations. The second is a greedy pattern detection (GPD) module for video copy localization. GPD quickly and accurately detects similarity patterns through a greedy strategy on the similarity matrix formed by matching frames in each cell, then aggregates them into complete predicted copy segments. On the comprehensive dataset self-SVD, VSM significantly outperforms state-of-the-art methods by 7.28% in mAP, and the retrieval speed is increased by over 318 times. Additionally, for short videos at the scale of hundreds of millions, the response speed can theoretically reach seconds. On the copy localization dataset MIX, composed of mixed-duration videos, GPD also achieves the best performance.  © 2023 IEEE.",117-125,10.1109/BIGCOM61073.2023.00024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185892806&doi=10.1109%2fBIGCOM61073.2023.00024&partnerID=40&md5=f7e286e0660daeceaddd2b764cd6d290,2023
RapidEarth: A Search-by-Classification Engine for Large-Scale Geospatial Imagery,"Data exploration and analysis in various domains often necessitate the search for specific objects in massive databases. A common search strategy, often known as search-by-classification, resorts to training machine learning models on small sets of positive and negative samples and to performing inference on the entire database to discover additional objects of interest. While such an approach often yields very good results in terms of classification performance, the entire database usually needs to be scanned, a process that can easily take several hours even for medium-sized data catalogs. In this work, we present RapidEarth, a geospatial search-by-classification engine that allows analysts to rapidly search for interesting objects in very large data collections of satellite imagery in a matter of seconds, without the need to scan the entire data catalog. RapidEarth embodies a co-design of multidimensional indexing structures and decision branches, a recently proposed variant of classical decision trees. These decision branches allow RapidEarth to transform the inference phase into a set of range queries, which can be efficiently processed by leveraging the aforementioned multidimensional indexing structures. The main contribution of this work is a geospatial search engine that implements these technical findings.  © 2023 ACM.",-,10.1145/3589132.3625601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182515213&doi=10.1145%2f3589132.3625601&partnerID=40&md5=bc61605bbc9f37f356175c7515d14f48,2022
Establishment and implementation of a clinical application and research database for surgical robots in China,"Robots are widely used in surgeries worldwide. To improve robotic surgical treatments, we established a database of clinical applications and research on surgical robots. Robotics-related literature included in the China national knowledge infrastructure (CNKI), Wanfang, Vipshop, Chinese science citation database (CSCD), Web of Science, EMBASE, PubMed, and Cochrane databases up to September 2022 was searched and entered into the database. Information on all patients who had undergone robotic surgery at our hospital since 2016 was also included. Literature and case information was classified and evaluated according to standard guidelines and statements. The Gansu Provincial Hospital was the first to use evidence-based medical research methods to successfully establish a database of clinical applications and research on surgical robots. This database comprised literature search, upload, quality evaluation, risk of bias assessment, case information entry, and access functions. Based on the database, we conducted related studies in general surgery, gynecology, urology, thoracic surgery, and cardiovascular surgery, and published 16 meta-analyses and 15 clinical studies. Establishing this database removes language, retrieval, and evaluation barriers in clinicians' use of clinical evidence and lays the foundation for the efficient and accurate use of clinical data for future research. Further, it facilitates evidence-based evaluation of the effectiveness of robotic therapy, which may guide future medical practice. This typical case of interdisciplinary research aims to build a platform to disseminate knowledge and technology in robotic surgery. This article is categorized under: Technologies > Computer Architectures for Data Mining Application Areas > Data Mining Software Tools Application Areas > Education and Learning. © 2023 Wiley Periodicals LLC.",-,10.1002/widm.1525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179337930&doi=10.1002%2fwidm.1525&partnerID=40&md5=f365f568167264161d15c8d3ed1cf391,2022
Main Memory Database Recovery Strategies,"Most of the current application scenarios, such as trading, real-time bidding, advertising, weather forecasting, social gaming, etc., require massive real-time data processing. Main memory database systems have proved to be an efficient alternative to such applications. These systems maintain the primary copy of the database in the main memory to achieve high throughput rates and low latency. However, a database in RAM is more vulnerable to failures than in traditional disk-oriented databases because of the memory volatility. DBMSs implement recovery activities (logging, checkpoint, and restart) for recovery proposes. Although the recovery component looks similar in disk- and memory-oriented systems, these systems differ dramatically in the way they implement their architectural components, such as data storage, indexing, concurrency control, query processing, durability, and recovery. This tutorial aims to provide a thorough review of in-memory database recovery techniques. To achieve this goal, we intend to review the main concepts of database recovery and architectural choices to implement an in-memory database system. Only then, we present the techniques to recover in-memory databases and discuss the recovery strategies of a representative sample of modern in-memory databases. Besides, the tutorial presents some aspects related to challenges and future directions of research in MMDBs in order to provide guidance for other researchers.  © 2023 ACM.",31-35,10.1145/3555041.3589402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162906777&doi=10.1145%2f3555041.3589402&partnerID=40&md5=8c348426d08f9aacd4d405db46dfadaa,2023
MM-DIRECT: Main memory database instant recovery with tuple consistent checkpoint,"Main memory databases (MMDBs) technology handles the primary database in Random Access Memory (RAM) to provide high throughput and low latency. However, volatile memory makes MMDBs much more sensitive to system failures. The contents of the database are lost in these failures, and, as a result, systems may be unavailable for a long time until the database recovery process has been finished. Therefore, novel recovery techniques are needed to repair crashed MMDBs as quickly as possible. This paper presents MM-DIRECT (Main Memory Database Instant RECovery with Tuple consistent checkpoint), a recovery technique that enables MMDBs to schedule transactions simultaneously with the database recovery process at system startup. Thus, it gives the impression that the database is instantly restored. The approach implements a tuple-level consistent checkpoint to reduce the recovery time. To validate the proposed approach, experiments were performed in a prototype implemented on the Redis database. The results show that the instant recovery technique effectively provides high transaction throughput rates even during the recovery process and normal database processing. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",-,10.1007/s00778-024-00846-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189009589&doi=10.1007%2fs00778-024-00846-z&partnerID=40&md5=092095921f18973f1b235698f133031a,2021
Optimizing Multi-Media Database Performance: A Comprehensive Review of Fragmentation Methods and Dynamic Access Patterns,"Large databases with a lot of data and many users can be difficult to manage and slow to query. Fragmentation is a technique that can be used to improve performance by dividing the database into smaller pieces that can be stored and accessed more efficiently. This research provides a systematic overview of the literature on fragmentation methods for databases. We performed a comprehensive literature review on fragmentation techniques proposed in articles published between 2012 and 2022, aiming to gather a systematic understanding of the subject. We examined the articles to determine if they took into account multimedia data or another data type, as well as other criteria such as the use of a cost model, ease of implementation, completeness, different algorithms applied to databases, and partitioning schemes. We also investigated whether the methods were dynamic, meaning that they could be adapted to changes in the database. Our investigation discovered a significant finding about the underutilization of dynamic fragmentation approaches and cost models in the context of multi-media databases. By discovering this gap, we have thrown light on an underutilized area of database management, providing opportunities for increased efficiency and cost-effectiveness. Furthermore, we stress the critical need of addressing the integration of a heritage database into existing infrastructure, which poses a substantial difficulty in the overall database management process. This discovery necessitates additional investigation and integration of dynamic fragmentation algorithms and cost models, which will result in improved performance, scalability, and resource allocation for multi-media databases.  © 2023 IEEE.",366-374,10.1109/ICICIS58388.2023.10391183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184663518&doi=10.1109%2fICICIS58388.2023.10391183&partnerID=40&md5=19fc5004b68014849d6497214a95b09b,2021
Multi-Surface Multi-Technique (MUST) Latent Fingerprint Database,"Latent fingerprint recognition involves acquisition and comparison of latent fingerprints with an exemplar gallery of fingerprints. The diversity in the type of surface leads to different procedures to recover the latent fingerprint. The appearance of latent fingerprints vary significantly due to the development techniques, leading to large intra-class variation. Due to lack of large datasets acquired using multiple mechanisms and surfaces, existing algorithms for latent fingerprint enhancement and comparison may perform poorly. In this study, we propose a Multi-Surface Multi-Technique (MUST) Latent Fingerprint Database. The database consists of more than 16,000 latent fingerprint impressions from 120 unique classes (120 fingers from 12 participants). Including corresponding exemplar fingerprints (livescan and rolled) and extended gallery, the dataset has nearly 21,000 impressions. It has latent fingerprints acquired under 35 different scenarios and additional four subsets of exemplar prints captured using live scan sensor and inked-rolled prints. With 39 different subsets, the database illustrates intra-class variations in latent fingerprints. The database has a potential usage towards building robust algorithms for latent fingerprint enhancement, segmentation, comparison, and multi-task learning. We also provide annotations for manually marked minutiae, acquisition Pixel Per Inch (PPI), and semantic segmentation masks. We also present the experimental protocol and the baseline results for the proposed dataset. The availability of the proposed database can encourage research in handling intra-class variation in latent fingerprint recognition.  © 2005-2012 IEEE.",1041-1055,10.1109/TIFS.2023.3280742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161061722&doi=10.1109%2fTIFS.2023.3280742&partnerID=40&md5=a8cf4d50a9b7a28c9c46b2f3c335ee7b,2021
Comparative analysis of metagenomic classifiers for long-read sequencing datasets,"Background: Long reads have gained popularity in the analysis of metagenomics data. Therefore, we comprehensively assessed metagenomics classification tools on the species taxonomic level. We analysed kmer-based tools, mapping-based tools and two general-purpose long reads mappers. We evaluated more than 20 pipelines which use either nucleotide or protein databases and selected 13 for an extensive benchmark. We prepared seven synthetic datasets to test various scenarios, including the presence of a host, unknown species and related species. Moreover, we used available sequencing data from three well-defined mock communities, including a dataset with abundance varying from 0.0001 to 20% and six real gut microbiomes. Results: General-purpose mappers Minimap2 and Ram achieved similar or better accuracy on most testing metrics than best-performing classification tools. They were up to ten times slower than the fastest kmer-based tools requiring up to four times less RAM. All tested tools were prone to report organisms not present in datasets, except CLARK-S, and they underperformed in the case of the high presence of the host’s genetic material. Tools which use a protein database performed worse than those based on a nucleotide database. Longer read lengths made classification easier, but due to the difference in read length distributions among species, the usage of only the longest reads reduced the accuracy. The comparison of real gut microbiome datasets shows a similar abundance profiles for the same type of tools but discordance in the number of reported organisms and abundances between types. Most assessments showed the influence of database completeness on the reports. Conclusion: The findings indicate that kmer-based tools are well-suited for rapid analysis of long reads data. However, when heightened accuracy is essential, mappers demonstrate slightly superior performance, albeit at a considerably slower pace. Nevertheless, a combination of diverse categories of tools and databases will likely be necessary to analyse complex samples. Discrepancies observed among tools when applied to real gut datasets, as well as a reduced performance in cases where unknown species or a significant proportion of the host genome is present in the sample, highlight the need for continuous improvement of existing tools. Additionally, regular updates and curation of databases are important to ensure their effectiveness. © 2024, The Author(s).",-,10.1186/s12859-024-05634-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182098964&doi=10.1186%2fs12859-024-05634-8&partnerID=40&md5=56157df940c3583c3726ad00013b9b78,2022
International Workshop on Data Management on New Hardware (DaMoN),"New hardware, such as multi-core CPUs, GPUs, FPGAs, new memory and storage technologies, low-power devices, bring new challenges and opportunities in optimizing database systems performance. Consequently, exploiting the characteristics of modern hardware has become an important topic of database systems research. In the last two decades, the DaMoN Workshop has established itself as the primary database venue to present ideas on how to exploit new hardware for data management, in particular how to improve performance or scalability of databases, how new hardware unlocks new database application scenarios, and how data management could benefit from future hardware.  © 2023 Owner/Author.",299-300,10.1145/3555041.3590816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162925790&doi=10.1145%2f3555041.3590816&partnerID=40&md5=3ff4ce30f4df1d20f9c1c1f177f0f653,2023
Research on the Reform and Practice of Informatization Mode of Adult Higher and Continuing Education Academic Records Management in the Context of Three-Whole Parenting,"Information technology reform is an important way for schools to realize efficient management and information technology should be introduced into the management of student registration files to accelerate the process of informationization of registration file management. This paper builds an informatization database of adult higher continuing education student registration file management based on informatization technology and carries out the design of the main business process of student registration files. The multi-layer stacked LSTM network in the ELMo model is utilized to learn the preprocessing of the text of the school records data, and the CART decision tree algorithm is improved based on the reinforcement learning mechanism in order to improve the classification prediction accuracy of the decision tree in the dataset. In order to verify the feasibility of the school registration file informationization database constructed in this paper, a test analysis is conducted for the application of data mining technology and informationization database. The results show that the check accuracy of the CART decision tree algorithm improved by the reinforcement learning mechanism is around 80%, and the throughput of this paper's database reaches 4827ops/s when the data writing volume is 1∗105GB. When the number of threads reaches 128, the throughput of this paper's informative database is around 5486ops/s, and when the data volume reaches 12∗105 entries, the database in this paper the query response time is around 602ms. Under the background of three-round education, it is necessary to fully integrate information technology with the management of school registration files, which will lead to the innovation of the informationization mode of the management of school registration files in adult higher education.  © 2023 Shiwei Mei, published by Sciendo.",-,10.2478/amns.2023.2.01470,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183766251&doi=10.2478%2famns.2023.2.01470&partnerID=40&md5=7a512bee13f89db1fc4a9a15948c4a5e,2021
An efficient indoor large map global path planning for robot navigation,"Large indoor cluttered environment representation is still a challenging task when non-uniform triangle cell-based or quadrangle cell-based decomposition is used to build the map. This paper aims at proposing a new method to represent a large indoor environment for efficient and global robot path planning using a trade-off among three criteria: path length, distance to obstacles, and path search complexity. In this regard, three steps are involved: (i) the design of a tiled map that represents several regular sub-regions of the given large environment; (ii) the selection of the best representation between non-uniform triangle cell-based submap and regular quadrangle cell-based submap for each tile using the predefined efficient path planning criteria. Hence, the entire large environment is represented by a hybrid cell-based map; (iii) the path search using the well-known A* algorithm. Moreover, to find the best representation, firstly, a generative method based on cellular automata is used to build a large synthetic database of maps of the same size as the tiles. Each map of the database is associated with the representation which provides the most efficient path planning. Given a tile, the corresponding image is used to find the closed tile image from the database, and the associated representation is selected as the best representation. Extensive simulations, as well as experiments, suggest that for a given large, cluttered environment, the hybrid representation with a mix of triangle-based cells and quadrangles-based cells can provide a more efficient global path compared to the traditional regular quadrangle or triangle representations. © 2024 Elsevier Ltd",-,10.1016/j.eswa.2024.123388,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184496900&doi=10.1016%2fj.eswa.2024.123388&partnerID=40&md5=dc559ce2093e7ff09e83d63adebc9190,2021
PROPOSAL FOR A LANDSCAPE EVALUATION SYSTEM,"This paper describes a system, currently being designed, for both perceptual analysis and aesthetic evaluation of a landscape. The choice of this topic is motivated by the fact that systems related to landscape visibility (e.g. Geographical Information Systems (GIS) for visual impact assessment) are not fully satisfactory when it comes to assessing the aesthetic appearance. They mainly analyse geometric aspects, such as the width of visual basins or the interference of visual trajectories, which can be expressed by objective and comparable parameters. Instead, for effective landscape knowledge and protection, it is important to consider other factors that cannot be easily measured, namely the quality of human perception, i.e. the aesthetic judgements that people can express about a landscape. Based on these considerations, a system has been designed in order to analyse the elements that can influence the aesthetic judgement of a landscape and therefore simulate the more probable aesthetic judgement. Unlike GIS generally works, this system does not use maps, but perspective views obtained by means of vehicle-mounted cameras, as in mobile mapping technology (MMT). Research into the system described below consisted of two parts: firstly how to form the database on which the system is based and secondly how to use the system. The database contains a large number of views analysed in terms of geometric, qualitative, thematic, topological and gestalt aspects; the results of these analyses are recorded in tables and improved through a parameter expressing an aesthetic judgement. This aesthetic judgement is obtained by processing the responses of a group of participants to a sociological and/or neurological survey (i.e. Functional Magnetic Resonance Imaging). In the operational phase, a new view will be evaluated by comparison with the views stored in the database The new view will be given a judgment, obtained by processing the judgments of the most similar views. The idea of this system applies both to the assessment of a single view and to the evaluation of territorial contexts. Once this system has been defined, it will have to be tested through practical application. © Author(s) 2023.",311-316,10.5194/isprs-archives-XLVIII-1-W1-2023-311-2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162104889&doi=10.5194%2fisprs-archives-XLVIII-1-W1-2023-311-2023&partnerID=40&md5=effd53ba3fef5de98853c38503beb096,2021
“There is no ambiguity on what to return”: Investigating the Prevalence of SQL Misconceptions,"In recent years, database education has been receiving more attention, with research in various directions such as the development of tools for education, the analysis of students’ homework, and the exploration of misconceptions. Misconceptions are mistakes in student reasoning that lead to errors during problem-solving. Recent work has documented misconceptions and errors in SQL. In this study we test the prevalence of several of these misconceptions through a multiple-choice questionnaire, to see if they hold on a larger, more diverse, student population. We found that all misconceptions are held to some extent, with prevalence scores ranging from one to fifty-two percent of the student population. Additionally, we have uncovered previously unidentified areas of struggle, allowing us to identify new misconceptions. © 2023 Copyright held by the owner/author(s).",-,10.1145/3631802.3631821,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185532799&doi=10.1145%2f3631802.3631821&partnerID=40&md5=b0d533bd43e84bcaec30ca61493137ba,2023
Students' Perceptions on Engaging Database Domains and Structures,"Several educational studies have argued for the contextualization of assignments, i.e., for providing a context or a story instead of an abstract or symbolic problem statement. Such contextualization may have beneficial effects such as higher student engagement and lower dropout rates. In the domain of database education, textbooks and educators typically provide an example database for context. These are then used to introduce key concepts related to database design, and to illustrate querying. However, it remains unstudied what kinds of database contexts are engaging for novices. In this paper, we study which aspects of database domain and complexity students find engaging through student reflections on a database creation assignment. We identify six factors regarding engaging domains, and five factors for engaging complexity. The main factor for domain-related engagement was Personal interest, the main factor for complexity engagement was Matching information requirements. Our findings can help database educators and book authors to design engaging exercise databases targeted for novices. © 2023 Owner/Author.",122-128,10.1145/3545945.3569727,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149895529&doi=10.1145%2f3545945.3569727&partnerID=40&md5=ebecdeb6111504e34bd90ad18ce48c06,2020
Processing of Experimental Results Using Databases on the Example of the Batch Process of Isoprene Polymerization,"This article presents an approach to organizing a database for storing and processing the results of experiments on the polymerization of isoprene using microheterogeneous catalytic systems in a batch mode. The proposed approach can be applied both to work with production data and with the results of computational experiments. The implementation of this approach will automate the process of recording data into a database, which will greatly facilitate the structuring of information. As an example, the work presents the results of a computational experiment on the polymerization of isoprene in a reactor in the presence of a titanium-containing catalyst system. © 2023 IEEE.",150-155,10.1109/SUMMA60232.2023.10349498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182326523&doi=10.1109%2fSUMMA60232.2023.10349498&partnerID=40&md5=adc654ed83aabfd180d5b8db16ae9fe5,2020
LVT Face Database: A Benchmark Database for Visible and Hidden Face Biometrics,"Although the estimation of eHealth parameters from face visuals (images and videos) has grown as a major area of research in the past years, deep-learning-based models are still challenged by RGB lack of robustness, for instance with changing illumination conditions. As a means to overcome these limitations and to unlock new opportunities, thermal imagery has arisen as a favorable alternative to solidify different technologies such as heart rate estimation from faces. However, the reduced number of databases containing thermal imagery and the lack of health annotation of the subjects in them limits the exploration of this spectrum. Motivated by this, in this paper, we present our Label-EURECOM Visible and Thermal (LVT) Face Database for face biometrics. This database is the first that contains paired visible and thermal images and videos from 52 subjects with metadata of 22 soft biometrics and health parameters. Moreover, we establish the first study introducing the potential of thermal images for weight estimation from faces on our database.  © 2023 IEEE.",-,10.1109/BIOSIG58226.2023.10345997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182399096&doi=10.1109%2fBIOSIG58226.2023.10345997&partnerID=40&md5=9552e1122690e4475d11bc76d9de8008,2023
A New Primitive for Processing Temporal Joins,"This paper presents the extended temporal aligner as a temporal primitive, and proposes a set of reduction rules that employ this primitive to convert a temporal join operator to its non-temporal equivalent. The rules cover all types of temporal joins, including inner join, outer joins, and anti-join. Preliminary experimental results demonstrate that the integration of the extended temporal aligner and the reduction rules can efficiently process temporal join queries.  © 2023 ACM.",106-109,10.1145/3609956.3609968,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171266037&doi=10.1145%2f3609956.3609968&partnerID=40&md5=18c3ca9439bcea951e85c3be904baf36,2022
Extracting adverse drug events from clinical Notes: A systematic review of approaches used,"Background: An adverse drug event (ADE) is any unfavorable effect that occurs due to the use of a drug. Extracting ADEs from unstructured clinical notes is essential to biomedical text extraction research because it helps with pharmacovigilance and patient medication studies. Objective: From the considerable amount of clinical narrative text, natural language processing (NLP) researchers have developed methods for extracting ADEs and their related attributes. This work presents a systematic review of current methods. Methodology: Two biomedical databases have been searched from June 2022 until December 2023 for relevant publications regarding this review, namely the databases PubMed and Medline. Similarly, we searched the multi-disciplinary databases IEEE Xplore, Scopus, ScienceDirect, and the ACL Anthology. We adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 statement guidelines and recommendations for reporting systematic reviews in conducting this review. Initially, we obtained 5,537 articles from the search results from the various databases between 2015 and 2023. Based on predefined inclusion and exclusion criteria for article selection, 100 publications have undergone full-text review, of which we consider 82 for our analysis. Results: We determined the general pattern for extracting ADEs from clinical notes, with named entity recognition (NER) and relation extraction (RE) being the dual tasks considered. Researchers that tackled both NER and RE simultaneously have approached ADE extraction as a “pipeline extraction” problem (n = 22), as a “joint task extraction” problem (n = 7), and as a “multi-task learning” problem (n = 6), while others have tackled only NER (n = 27) or RE (n = 20). We further grouped the reviews based on the approaches for data extraction, namely rule-based (n = 8), machine learning (n = 11), deep learning (n = 32), comparison of two or more approaches (n = 11), hybrid (n = 12) and large language models (n = 8). The most used datasets are MADE 1.0, TAC 2017 and n2c2 2018. Conclusion: Extracting ADEs is crucial, especially for pharmacovigilance studies and patient medications. This survey showcases advances in ADE extraction research, approaches, datasets, and state-of-the-art performance in them. Challenges and future research directions are highlighted. We hope this review will guide researchers in gaining background knowledge and developing more innovative ways to address the challenges. © 2024 Elsevier Inc.",-,10.1016/j.jbi.2024.104603,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185411162&doi=10.1016%2fj.jbi.2024.104603&partnerID=40&md5=95aa2f8a4c15334f109b4a1d6a683d4c,2024
KeRRaS: Sort-Based Database Query Processing on Wide Tables Using FPGAs,"Sorting is an important operation in database query processing. Complex pipeline-breaking operators (e.g., aggregation and equi-join) become single-pass algorithms on sorted tables. Therefore, sort-based query processing is a popular method for FPGA-based database system acceleration. However, most accelerators have a limit on the table width or the number of columns they can sort. This limit is often set by the width of the data path or the amount of BRAM present on the FPGA. In this paper we propose KeRRaS, an abstract sorting algorithm that enables existing sort-based query processors to support arbitrarily wide tables while offering scalability, preserving modularity, and having low resource overhead. Moreover, we present an implementation of KeRRaS based on morphing sort-merge, a resource-efficient FPGA-based query accelerator. The implementation behaves similarly to morphing sort-merge on narrow tables, and scales well as the number of key columns increases. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",1-9,10.1145/3592980.3595300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163778078&doi=10.1145%2f3592980.3595300&partnerID=40&md5=2ddc166841c1b77653c1635aa4fcf480,2023
Open Access Database of Industry 4.0 Tasks for the Development of AI-Based Classifier,"Robots and humans coworkers are sharing more and more portions of the smart manufacturing globally, meeting the need for high flexibility and rapid changes in the production layout. To be fully effective, however, such transition from classic robotics to the so-called collaborative robotics has to address several open problems, mostly related with safety and task optimization. Promising answers are coming from the motion capture technology, where wearable and optoelectronic sensing devices are deployed to gather human centric data to provide the robots with some form of awareness respect with the human activity and position. Tracking the hand of the operator, in particular, offers many advantages as we use our hands to explore and interact with the surroundings and to communicate. This has been highlighted by the several works focusing on gesture hand configuration recognition. This work present HANDMI4, a new open access database of hand motion tracking data, which includes a wide range of static hand grasp configurations and some classic dynamic industry tasks. Such database was generated using two of the most mature technologies for motion capture: IMU-based data glove and camera-based triangulation. To test the capability of such dataset to foster AI-based task classifier, a set of machine learning techniques were implemented and tested. In particular, KNN weighted reached 94,4% and 100% of task classification accuracy for the data glove and the camera system, respectively. With this open access database we aim to boost the research around task classification through motion capture technology to enable the next revolution in smart manufacturing.  © 2023 IEEE.",-,10.1109/SSI58917.2023.10387755,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184805530&doi=10.1109%2fSSI58917.2023.10387755&partnerID=40&md5=a1bc42b21b10fd300ad2066d8fd69436,2020
Towards precision medicine in breast imaging: A novel open mammography database with tailor-made 3D image retrieval for AI and teaching,"This project addresses the global challenge of breast cancer, particularly in low-resource settings, by creating a pioneering mammography database. Breast cancer, identified by the World Health Organization as a leading cause of cancer death among women, often faces diagnostic and treatment resource constraints in low- and middle-income countries. To enhance early diagnosis and address educational setbacks, the project focuses on leveraging artificial intelligence (AI) technologies through a comprehensive database. Developed in collaboration with Ambra Health, a cloud-based medical image management software, the database comprises 941 mammography images from 100 anonymized cases, with 62 % including 3D images. Accessible through http://mamografia.unifesp.br, the platform facilitates a simple registration process and an advanced search system based on 169 clinical and imaging variables. The website, customizable to the user's native language, ensures data security through an automatic anonymization system. By providing high-resolution, 3D digital images and supplementary clinical information, the platform aims to promote education and research in breast cancer diagnosis, representing a significant advancement in resource-constrained healthcare environments. © 2024",-,10.1016/j.cmpb.2024.108117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187958674&doi=10.1016%2fj.cmpb.2024.108117&partnerID=40&md5=d9507e3fae91267b483d9991aae4ef25,2021
Gaussian Process Regression-Based Near-Infrared d-Luciferin Analogue Design Using Mutation-Controlled Graph-Based Genetic Algorithm,"Molecular discovery is central to the field of chemical informatics. Although optimization approaches have been developed that target-specific molecular properties in combination with machine learning techniques, optimization using databases of limited size is challenging for efficient molecular design. We present a molecular design method with a Gaussian process regression model and a graph-based genetic algorithm (GB-GA) from a data set comprising a small number of compounds by introducing mutation probability control in the genetic algorithm to enhance the optimization capability and speed up the convergence to the optimal solution. In addition, we propose reducing the number of parameters in the conventional GB-GA focusing on efficient molecular design from a small database. We generated a target-specific database by combining active learning and iterative design in the evolutionary methodologies and chose Gaussian process regression as the prediction model for molecular properties. We show that the proposed scheme is more efficient for optimization toward the target properties from goal-directed benchmarks with several drug-like molecules compared to the conventional GB-GA method. Finally, we provide a demonstration whereby we designed D-luciferin analogues with near-infrared fluorescence for bioimaging, which is desirable for effective in vivo light sources, from a small-size data set. © 2024 American Chemical Society.",1522-1532,10.1021/acs.jcim.3c00870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186106051&doi=10.1021%2facs.jcim.3c00870&partnerID=40&md5=fd203bc3584c198b76a2fa72bf3e90a5,2021
The Impact of Feature Extraction on Classification Accuracy Examined by Employing a Signal Transformer to Classify Hand Gestures Using Surface Electromyography Signals,"Interest in developing techniques for acquiring and decoding biological signals is on the rise in the research community. This interest spans various applications, with a particular focus on prosthetic control and rehabilitation, where achieving precise hand gesture recognition using surface electromyography signals is crucial due to the complexity and variability of surface electromyography data. Advanced signal processing and data analysis techniques are required to effectively extract meaningful information from these signals. In our study, we utilized three datasets: NinaPro Database 1, CapgMyo Database A, and CapgMyo Database B. These datasets were chosen for their open-source availability and established role in evaluating surface electromyography classifiers. Hand gesture recognition using surface electromyography signals draws inspiration from image classification algorithms, leading to the introduction and development of the Novel Signal Transformer. We systematically investigated two feature extraction techniques for surface electromyography signals: the Fast Fourier Transform and wavelet-based feature extraction. Our study demonstrated significant advancements in surface electromyography signal classification, particularly in the Ninapro database 1 and CapgMyo dataset A, surpassing existing results in the literature. The newly introduced Signal Transformer outperformed traditional Convolutional Neural Networks by excelling in capturing structural details and incorporating global information from image-like signals through robust basis functions. Additionally, the inclusion of an attention mechanism within the Signal Transformer highlighted the significance of electrode readings, improving classification accuracy. These findings underscore the potential of the Signal Transformer as a powerful tool for precise and effective surface electromyography signal classification, promising applications in prosthetic control and rehabilitation. © 2024 by the authors.",-,10.3390/s24041259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185542440&doi=10.3390%2fs24041259&partnerID=40&md5=c1814c3140871ab9e12a91816e6b8339,2021
CASIA-Iris-Africa: A Large-scale African Iris Image Database,"Iris biometrics is a phenotypic biometric trait that has proven to be agnostic to human natural physiological changes. Research on iris biometrics has progressed tremendously, partly due to publicly available iris databases. Various databases have been available to researchers that address pressing iris biometric challenges such as constraint, mobile, multispectral, synthetics, long-distance, contact lenses, liveness detection, etc. However, these databases mostly contain subjects of Caucasian and Asian docents with very few Africans. Despite many investigative studies on racial bias in face biometrics, very few studies on iris biometrics have been published, mainly due to the lack of racially diverse large-scale databases containing sufficient iris samples of Africans in the public domain. Furthermore, most of these databases contain a relatively small number of subjects and labelled images. This paper proposes a large-scale African database named Chinese Academy of Sciences Institute of Automation (CASIA)-Iris-Africa that can be used as a complementary database for the iris recognition community to mediate the effect of racial biases on Africans. The database contains 28 717 images of 1 023 African subjects (2 046 iris classes) with age, gender, and ethnicity attributes that can be useful in demographically sensitive studies of Africans. Sets of specific application protocols are incorporated with the database to ensure the database’s variability and scalability. Performance results of some open-source state-of-the-art (SOTA) algorithms on the database are presented, which will serve as baseline performances. The relatively poor performances of the baseline algorithms on the proposed database despite better performance on other databases prove that racial biases exist in these iris recognition algorithms. The database will be made available on our website: http://www.idealtest.org. © Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature 2024.",383-399,10.1007/s11633-022-1402-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182232231&doi=10.1007%2fs11633-022-1402-8&partnerID=40&md5=1ed934e00501f71bfb6f3c445cd15f2e,2022
"COASTAL INUNDATION SIMULATION DUE TO SEA LEVEL RISE IN TERENGGANU, MALAYSIA","Coastal Inundation is the amount of water levels that reach above normal level due to flooding in coastal areas. This natural phenomenon is due to the rising of local sea level. This study aims to develop a coastal inundation simulation along the coastline of Terengganu. To ensure the success of this study, the rate and magnitude of sea level rise are calculated using Radar Altimetry Database System (RADS) from multi-mission satellite altimetry data. The rate of sea level rise is computed using robust fit regression technique and overlaid with global Digital Elevation Model (DEM) data, known as TanDEM-X. The overlaid data are utilised to simulate the probable areas to be affected by inundation from year 2040 to 2100 using ArcGIS software. The results obtained from the period of 1993 to 2015 show that the average rate of sea level rise along the Terengganu shoreline is 4.84 mm/year with a standard deviation of 0.49 mm/year. By 2100, the estimated average regional sea level rise is projected to increase by 0.412 m. This study could help coastal development planning, defence, and safety monitoring.  Copyright © 2023 M. S. Musa et al.",261-267,10.5194/isprs-archives-XLVIII-4-W6-2022-261-2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148702558&doi=10.5194%2fisprs-archives-XLVIII-4-W6-2022-261-2023&partnerID=40&md5=6e452e1b6007c6acba44ee8e00bf642c,2023
Augusta: From RNA-Seq to gene regulatory networks and Boolean models,"Computational models of gene regulations help to understand regulatory mechanisms and are extensively used in a wide range of areas, e.g., biotechnology or medicine, with significant benefits. Unfortunately, there are only a few computational gene regulatory models of whole genomes allowing static and dynamic analysis due to the lack of sophisticated tools for their reconstruction. Here, we describe Augusta, an open-source Python package for Gene Regulatory Network (GRN) and Boolean Network (BN) inference from the high-throughput gene expression data. Augusta can reconstruct genome-wide models suitable for static and dynamic analyses. Augusta uses a unique approach where the first estimation of a GRN inferred from expression data is further refined by predicting transcription factor binding motifs in promoters of regulated genes and by incorporating verified interactions obtained from databases. Moreover, a refined GRN is transformed into a draft BN by searching in the curated model database and setting logical rules to incoming edges of target genes, which can be further manually edited as the model is provided in the SBML file format. The approach is applicable even if information about the organism under study is not available in the databases, which is typically the case for non-model organisms including most microbes. Augusta can be operated from the command line and, thus, is easy to use for automated prediction of models for various genomes. The Augusta package is freely available at github.com/JanaMus/Augusta. Documentation and tutorials are available at augusta.readthedocs.io. © 2024 The Authors",783-790,10.1016/j.csbj.2024.01.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185178942&doi=10.1016%2fj.csbj.2024.01.013&partnerID=40&md5=12487261214b42b12f5ef3ade72bcfc4,2021
Mining high average utility itemsets using artificial fish swarm algorithm with computed multiple minimum average utility thresholds,"It is obvious that the problem of Frequent Itemset Mining (FIM) is very popular in data mining, which generates frequent itemsets from a transaction database. An extension of the frequent itemset mining is High Utility Itemset Mining (HUIM) which identifies itemsets with high utility from the transaction database. This gains popularity in data mining, because it identifies itemsets which have more value but the same was not identified as frequent by Frequent Itemset Mining. HUIM is generally referred to as Utility Mining. The utility of the items is measured based on parameters like cost, profit, quantity or any other measures preferred by the users. Compared to high utility itemsets (HUIs) mining, high average utility itemsets (HAUIs) mining is more precise by considering the number of items in the itemsets. In state-of-The-Art algorithms that mines HUIS and HAUIs use a single fixed minimum utility threshold based on which HAUIs are identified. In this paper, the proposed algorithm mines HAUIs from transaction databases using Artificial Fish Swarm Algorithm (AFSA) with computed multiple minimum average utility thresholds. Computing the minimum average utility threshold for each item with the AFSA algorithm outperforms other state-of-The-Art HAUI mining algorithms with multiple minimum utility thresholds and user-defined single minimum threshold in terms of number of HAUIs. It is observed that the proposed algorithm outperforms well in terms of execution time, number of candidates generated and memory consumption when compared to the state-of-The-Art algorithms.  © 2024-IOS Press. All rights reserved.",1597-1613,10.3233/JIFS-231852,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183465862&doi=10.3233%2fJIFS-231852&partnerID=40&md5=531d7cfc307cddc836f58315399a9284,2020
"Facial Expression Recognition Using Visible, IR, and MSX Images by Early and Late Fusion of Deep Learning Models","Facial expression recognition (FER) is one of the best non-intrusive methods for understanding and tracking mood and mental states. In this study, we propose early and late fusion methods to recognize five facial expressions (angry, happy, neutral, sad, and surprised) using different combinations from a publicly available database (VIRI) with visible, infrared, and multispectral dynamic imaging (MSX) images and the (NVIE) database. A distinctive feature is the use of concatenation and combining techniques to combine ResNet-18 with transfer learning (TL) to create a model that is significantly more accurate than individual models. In the early fusion, we concatenated features from the modalities and classified facial expressions (FEs). In the late fusion, we combined the outputs of the modalities using weighted sums. For this purpose, we used different weighting factors depending on the accuracy of the individual models. The experimental results demonstrated that the proposed model outperformed the previous works by providing an accuracy of 83.33% when we trained the model (1-step training). Through further fine-tuning (3-step training), we obtained an improved performance of 84.44%. We conducted additional experiments by combining them with another modality (MSX) available in the database. By performing experiments with an additional modality (MSX), we obtained improved performance, which confirms that the additional modality combined with existing modalities can help improve the performance of fusion models for facial expression recognition. We also experimented by changing the backbones (Vgg-16, ShuffleNetv2, MobileNetv2, and GhostNet) in addition to ResNet-18 for visible and MSX data. ResNet-18 outperformed the other backbones in facial expression recognition for visible and MSX data.  © 2013 IEEE.",20692-20704,10.1109/ACCESS.2024.3362247,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184795555&doi=10.1109%2fACCESS.2024.3362247&partnerID=40&md5=15851993216cd35df5b52facead273c7,2021
Occlusion-aware facial expression recognition: A deep learning approach,"Facial expression recognition plays a crucial role in computer vision and human–computer interaction, with applications ranging from emotion analysis to social robotics. However, accurate recognition becomes challenging in the presence of occlusions, such as facial hair, glasses, and self-occlusion. This study addresses the problem of facial expression recognition despite occlusions and proposes a novel approach to overcome this challenge. The aim of this research is to develop a robust facial expression recognition framework that effectively handles occlusions and improves recognition accuracy. To achieve this, Hopfield networks, Deep Belief Networks (DBN), and Lanczos interpolation are integrated into the proposed method. Lanczos interpolation helps preserve image quality and reduces during resizing. The Hopfield network is utilized for feature extraction, capturing facial expression features even in the presence of occlusions. The DBN is employed for representation learning, fine-tuning the network using DenseNet to adapt to occluded facial expressions. To evaluate the proposed approach, extensive experiments were conducted on various datasets, including the Static Facial Expressions in the Wild (SFEW) dataset, AffectNet dataset, Real-world Affective Faces Database (RAF-DB), MMI Facial Expression Database, Oulu-CASIA NIR&VIS Facial Expression Database, and Extended Cohn-Kanade (CK +) dataset. The results demonstrate the superiority of our method in handling occlusions and achieving improved facial expression recognition accuracy compared to existing approaches. The contributions of this work are twofold: First, a novel framework is proposed that integrates Lanczos interpolation, Hopfield networks, DBN, and to effectively address the challenge of occlusions in facial expression recognition. Second, extensive experimental validation is provided on diverse datasets, highlighting the superior performance of our approach in handling occlusions and achieving accurate recognition. In conclusion, the proposed approach demonstrates its efficacy in improving facial expression recognition under occlusions. By effectively handling occlusions and capturing relevant features, our method opens up possibilities for enhanced emotion analysis, human–computer interaction, and social robotics applications. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",32895-32921,10.1007/s11042-023-17013-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172474613&doi=10.1007%2fs11042-023-17013-1&partnerID=40&md5=00ac08a8d5dcfd482a115dd81e82f73b,2021
Termout: a tool for the semi-automatic creation of term databases,"We propose a tool for the semi-automatic production of terminological databases, divided in the steps of corpus processing, terminology extraction, database population and management. With this tool it is possible to obtain a draft macrostructure (a lemma-list) and data for the microstructural level, such as grammatical (morphosyntactic patterns, gender, formation process) and semantic information (hypernyms, equivalence in another language, definitions and synonyms). In this paper we offer an overall description of the software and an evaluation of its performance, for which we used a linguistics corpus in English and Spanish. © Computational Terminology in NLP and Translation Studies, ConTeNTS 2023 - Incorporating the 16th Workshop on Building and Using Comparable Corpora, BUCC 2023, associated with 14th International Conference on Recent Advances in Natural Language Processing 2023, RANLP 2023 - Proceedings.",9-18,10.26615/978-954-452-090-8_002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185003979&doi=10.26615%2f978-954-452-090-8_002&partnerID=40&md5=ad1e21b68807ea218cf30eeeada71fc5,2023
Prediction of punching shear strength in flat slabs: ensemble learning models and practical implementation,"This study proposes new models to predict the punching shear strength of flat slabs without transverse reinforcement by harnessing the power of machine learning through ensemble learning models. Leveraging two distinct databases—one with 522 samples with six input variables and another comprising 745 samples with four essential input variables. Six ensemble learning models, including Random Forest, AdaBoost, Light GBM, GBRT, CatBoost, and XGBoost, are evaluated. Through a combination of Bayesian optimisation and tenfold cross-validation technique, the CatBoost model emerges as the standout performer, achieving a coefficient of determination (R2) of 0.97 for both training and testing datasets. Notably, these models exhibit their superior prediction accuracy as compared to existing design codes and empirical equations. To further validate robustness of the models and evaluate the randomness of the databases, Monte Carlo simulations are employed. Additionally, the adaptability of XGBoost, CatBoost, and GBRT was tested using 200 random data points that extended beyond the original database's range, showcasing their capacity to provide reliable predictions in extended scenarios. Finally, a user-friendly interface application is developed for estimating the punching shear strength of RC flat slabs. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.",4207-4228,10.1007/s00521-023-09296-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179314902&doi=10.1007%2fs00521-023-09296-0&partnerID=40&md5=2be9c001f5a180b5173304afb6035c32,2021
Fairly Decentralizing a Hybrid Concurrency Control Protocol for Real-time Database Systems,"Transaction priority is a critical feature for real-time database systems. This prioritization mechanism is widely implemented, particularly in locking-based concurrency control protocols. Plor is a non-real-time concurrency control protocol based on the 2-phase locking protocol. Plor utilizes the Wound-Wait scheme, a timestamps-based scheme for deadlock prevention, as it provides lower tail latency. One problem of Plor is that it requires transactions to fetch timestamps from a single centralized atomic counter. This paper evaluates the implementation of the Fair Thread ID method (FairTID), in which transactions use the thread IDs as their timestamps instead of fetching timestamps in 2-phase locking protocols and maintain the prioritization inherited from the Wound-Wait scheme. The FairTID method shows up to 1.5 times throughput improvement while reducing latency by 1.6 times and deadline-miss ratio by 1.67 times over the baseline protocol. The method displayed significant improvements in performance while the level of contention was not high.  © 2023 IEEE.",24-30,10.1109/CANDARW60564.2023.00013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185722155&doi=10.1109%2fCANDARW60564.2023.00013&partnerID=40&md5=9a67527aeaf0702a31abf7b1be50927d,2024
Research on Computer Information Hotel Front-End Storage Database Integrated Management System,"Firstly, this paper uses SQLServer2008 database to store data to design and implement the hotel integrated management system. Then this paper introduces a data structure to realize the continuous storage of multiple books and multi-channel data at the front end of the hotel, and gives the definition of the data structure and related algorithms. Finally, this paper implements modules including main process, protocol processing, event driver and so on, which is used to build a complete hotel front-end data cache system under Linux environment. Experiments show that the virtual core performance of the hotel front-end data stream cache is improved by 9.4% on average under the configuration of the front-end width of 4 instructions and the capacity of the instruction window of 512. Program performance improved by up to 28%. © 2023 IEEE.",70-73,10.1109/CIPAE60493.2023.00019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182335461&doi=10.1109%2fCIPAE60493.2023.00019&partnerID=40&md5=2ac3f02f8bb56f4a1e983669c9f179cf,2022
An Empirical Study of Model Errors and User Error Discovery and Repair Strategies in Natural Language Database Queries,"Recent advances in machine learning (ML) and natural language processing (NLP) have led to significant improvement in natural language interfaces for structured databases (NL2SQL). Despite the great strides, the overall accuracy of NL2SQL models is still far from being perfect (∼75% on the Spider benchmark). In practice, this requires users to discern incorrect SQL queries generated by a model and manually fix them when using NL2SQL models. Currently, there is a lack of comprehensive understanding about the common errors in auto-generated SQLs and the effective strategies to recognize and fix such errors. To bridge the gap, we (1) performed an in-depth analysis of errors made by three state-of-the-art NL2SQL models; (2) distilled a taxonomy of NL2SQL model errors; and (3) conducted a within-subjects user study with 26 participants to investigate the effectiveness of three representative interactive mechanisms for error discovery and repair in NL2SQL. Findings from this paper shed light on the design of future error discovery and repair strategies for natural language data query interfaces.  © 2023 ACM.",633-649,10.1145/3581641.3584067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152141469&doi=10.1145%2f3581641.3584067&partnerID=40&md5=210d3d7a4638ef32412e215607b78fc7,2022
Development of ADL Motion Database to Promote Co-creative Care Support,"This paper introduces fundamental development of a new database of activities of daily living (ADL) for the purpose of supporting independent care for the elderly or disabilities with adaptable AI-enable robots. The main concept of AI-robots is briefly illustrated. According to the concept of AI-robots, a simple framework to deal with ADL in aspect of both subjective and objective is demonstrated to achieve goal-oriented care support. The patient centric support is expected to prevent excessive care by the AI-robots. In order to perform the care support, the activities or motions defined as Doing-ADL that elderly people usually perform are digitalized as motion data. A method of utilizing a database system that accumulates such motion data is conceptually shown. Furthermore, the operation of the system as a digital platform for inclusive social care is proposed from the viewpoint of co-creation among care recipients, care workers and the AI-enabled robots.  © 2023 Society of Instrument and Control Engineers - SICE.",313-318,10.23919/SICE59929.2023.10354252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182588454&doi=10.23919%2fSICE59929.2023.10354252&partnerID=40&md5=a58689d5a20783b10e6f16824de7b0df,2021
Database-driven model predictive control system for online adaptation of an autonomous excavator to environmental conditions,"This paper presents the design of a database-driven model predictive control (DD-MPC) system for the online adaptation of autonomous excavators to environmental conditions. Control systems for autonomous excavators should consider environmental conditions as these affect their performance for a given excavation operation. Moreover, these conditions may change during operation. MPC was performed using an excavator-environment interaction model, which was estimated online using DD-Modeling to represent changes in environmental conditions. The target excavation trajectory was modified by predicting excavation motion using MPC and decision based on the prediction to complete a given excavation operation regardless of the environmental conditions. The proposed system was experimentally verified using a radio-controlled excavator, and it was confirmed that a given operation could be completed by adapting to environmental conditions. © 2024 The Author(s)",-,10.1016/j.conengprac.2024.105843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183094265&doi=10.1016%2fj.conengprac.2024.105843&partnerID=40&md5=bafdf47620548d81a54559b1159141e7,2021
Application of nonparametric quantifiers for online handwritten signature verification: A statistical learning approach,"This work explores the use of nonparametric quantifiers in the signature verification problem of handwritten signatures. We used the MCYT-100 (MCYT Fingerprint subcorpus) database, widely used in signature verification problems. The discrete-time sequence positions in the x -axis and y-axis provided in the database are preprocessed, and time causal information based on nonparametric quantifiers such as entropy, complexity, Fisher information, and trend are employed. The study also proposes to evaluate these quantifiers with the time series obtained, applying the first and second derivatives of each sequence position to evaluate the dynamic behavior by looking at their velocity and acceleration regimes, respectively. The signatures in the MCYT-100 database are classified via Logistic Regression, Support Vector Machines (SVM), Random Forest, and Extreme Gradient Boosting (XGBoost). The quantifiers were used as input features to train the classifiers. To assess the ability and impact of nonparametric quantifiers to distinguish forgery and genuine signatures, we used variable selection criteria, such as: information gain, analysis of variance, and variance inflation factor. The performance of classifiers was evaluated by measures of classification error such as specificity and area under the curve. The results show that the SVM and XGBoost classifiers present the best performance. © 2024 The Authors. Statistical Analysis and Data Mining: The ASA Data Science Journal published by Wiley Periodicals LLC.",-,10.1002/sam.11673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188859838&doi=10.1002%2fsam.11673&partnerID=40&md5=06b6ba93f1d4e3e88d07d7cb2eaff325,2024
COMPASS: Cardinal Orientation Manipulation and Pattern-Aware Spatial Search,"The Spatial Pattern Matching paradigm offers a promising direction for searching with incomplete or imperfect information, but it is badly constrained by dependence on graph-based representations and computationally intensive search algorithms like subgraph matching and constraint satisfaction. To address these limitations, we present COMPASS, a suite of data structures and algorithms that enable pattern-based search by encoding the directional relationships between objects in abstracted matrix representations rather than graph structures. We provide a series of recursive search algorithms that leverage our matrix representations to enable spatial search queries with directional constraints, which are typically too dense for previous graph-based approaches. Our search methods find matches even when the query pattern is not aligned to the global coordinate system, resulting in perfect recall in our evaluation. Computationally, our search methods scale with the number of query objects times the number of database objects squared in the worst case, which is significantly better than previous methods. Our empirical measurements show that the performance is typically even better, approaching logarithmic in the number of query terms.  © 2023 Owner/Author(s).",29-36,10.1145/3615890.3628537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181802473&doi=10.1145%2f3615890.3628537&partnerID=40&md5=49543d3176330f33d464a176cfb7ab98,2021
Hybrid Genetic Algorithms for Order Assignment and Batching in Picking System: A Systematic Literature Review,"The Order-Picking System (OPS) is vital for inbound logistics, ensuring efficient customer order fulfillment and minimizing costs. Efficient execution and implementation of OPS are critical to meeting customer demands and reducing dissatisfaction, necessitating a thorough examination of the process of order picking features. The order picking, a cornerstone of warehouse operations, involves meticulous selection and gathering of items from designated storage locations, unfolding through stages like order assignment and order batching. In order assignment, specific orders are methodically delegated to pickers or teams, considering factors like urgency, order size, item location, and picker availability. The overarching goal is to optimize resource utilization and simultaneously reduce the time needed for order fulfillment, ensuring a streamlined and efficient approach. Conversely, order batching strategically groups multiple orders for concurrent picking, aiming to minimize trips and enhance overall efficiency and productivity. Throughout the order-picking process, pickers utilize tools like pick lists, barcode scanners, and automated storage and retrieval systems (AS/RS) for precise item location and retrieval. Post-collection, items are transported to a dedicated packing area for meticulous preparations before shipping to the customer. Orchestrating the order-picking process requires careful planning, coordination, and execution for punctual and precise customer order fulfillment. This paper highlighted a systematic reviewing process which analyzed relevant research papers, with a primary focus on the problems of order assignment and batching-a key area within the order-picking process. The objective was to provide a comprehensive overview of hybrid Genetic Algorithm solutions for these challenges, achieved through a systematic review from 2018 to 2023 using Web of Science and Scopus databases. After screening, the relevant references were selected, focusing on terms like storage assignment problems. A thorough examination delved into various subcategories, encompassing recent approaches of genetic algorithms and openly accessible datasets. The resulting review offers a concise summary, highlighting key findings, challenges, and potential directions associated with hybrid genetic algorithms, specifically in relation to storage assignment, storage location assignment problems, and order batching issues.  © 2013 IEEE.",23029-23042,10.1109/ACCESS.2024.3357689,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183979489&doi=10.1109%2fACCESS.2024.3357689&partnerID=40&md5=1e47c809d2cab3f7fbd90311c3c87da3,2024
GOMAD: A Goal Oriented Approach for Modeling and Analyzing Database Deployment,"Database deployment automation involves dealing with the diversity of deployment platforms and the panoply of options to improve the efficiency of query workload. Designers must compare different scenarios and alternatives to find a good physical configuration. This requires hardware experimentation on a target platform or a simulation during the physical design phase. We aim to provide a solution that allows designers to reason up the causality of a given choice at the level of conceptual design and realize the selection of a physical configuration in a transparent way for the designers. In order to tackle this problem, we propose an approach based on goaloriented modeling which helps database designers to automatize and ease the traditional workflow dedicated to choosing an appropriate physical configuration. This latter captures a physical design vocabulary at a higher level of abstraction in order to focus on requirement user and application constraints and not on low level implementation details. Our Goal Model with its generator quickly produces the script related to the database materialization and speeds up the database creation process. We demonstrate the feasibility of our approach presenting a prototype tool.  © 2023 IEEE.",-,10.1109/IISEC59749.2023.10391023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184658614&doi=10.1109%2fIISEC59749.2023.10391023&partnerID=40&md5=5bea8ff483bb259b13775b89ac4f5899,2024
A Comparative Study on Machine Learning-Aided Flow Traffic Generators,"In recent years, machine-learning algorithms have been developed for application-level traffic generators, but have yet to be implemented in flow-level traffic generators. Since flow-level traffic data is a time series traffic data, it could be used for predicting subsequent network traffic. Its prediction ability can be utilized to generate traffic when the database cannot provide the data. This is an improvement from our previous work that uses exact-database methods which only generate traffic data based on the database. A comparison is made between traditional machine-learning methods (Linear Regression, Support Vector Regression, and Gaussian Process Regression) and a deep-learning method (Long-Short Term Memory) to determine how well the Artificial intelligence (AI) algorithm performs when applied to a flow-level traffic generator. These methods enable traffic generators to continue generating traffic while the database is unavailable. Among all the methods, the Long-Short Term Memory (LSTM) algorithm performs the best because it has the lowest Root Mean Square Error (RMSE) value which is 0.13. Visually, some traditional machine learning methods still manage to imitate real traffic, despite their high RMSE values. © 2023 IEEE.",101-106,10.1109/ICRAMET60171.2023.10366632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182728876&doi=10.1109%2fICRAMET60171.2023.10366632&partnerID=40&md5=71bfad81987fda1abc87b8404518246b,2021
3P-ECLAT: mining partial periodic patterns in columnar temporal databases,"Partial periodic pattern (3P) mining is a vital data mining technique that aims to discover all interesting patterns that have exhibited partial periodic behavior in temporal databases. Previous studies have primarily focused on identifying 3Ps only in row temporal databases. One can not ignore the existence of 3Ps in columnar temporal databases as many real-world applications, such as Facebook and Adobe, employ them to store their big data. This paper proposes an efficient single database scan algorithm, Partial Periodic Pattern-Equivalence Class Transformation (3P-ECLAT), to identify all 3Ps in a columnar temporal database. The proposed algorithm compresses the given database into a novel list-based data structure and mines it recursively to find all 3Ps. The 3P-ECLAT leverages the “downward closure property” and “depth-first search technique” to reduce the search space and the computational cost. Extensive experiments have been conducted on synthetic and real-world databases to demonstrate the efficiency of the 3P-ECLAT algorithm. The memory and runtime results show that 3P-ECLAT outperforms its competitor considerably. Furthermore, 3P-ECLAT is highly scalable and is superior to the previous approach in handling large databases. Finally, to demonstrate the practical utility of our algorithm, we provide two real-world case studies, one on analyzing traffic congestion during disasters and another on identifying the highly polluted areas in Japan. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",657-679,10.1007/s10489-023-05172-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179934057&doi=10.1007%2fs10489-023-05172-5&partnerID=40&md5=24fd0096ffe2b4f966433c2ab311be1e,2024
Multivariate probability distributions for index and mechanical clay parameters in Shenzhen,"Probabilistic site characterization is the cornerstone of data-centric geotechnics, which underpins the digitalization in geotechnical engineering. The essential aspect of probabilistic site characterization is to reasonably infer the probability density distributions of geotechnical parameters, based on field and laboratory data. In this paper, a clay property database, SZ-CLAY/11/5130, was established by collecting geotechnical investigation reports from 52 engineering projects in Shenzhen city. This SZ-CLAY/11/5130 database was first compared with the global clay database CLAY/10/7490 and the Shanghai clay database SH-CLAY/11/4051, in terms of their sample ranges, correlations, and scatter plots. Then, the multivariate probability distribution of eight clay parameters was established using the training records selected from SZ-CLAY/11/5130, to characterize the generic correlations between multiple clay parameters. Based on Bayesian theory, the posterior distributions of clay mechanical parameters from a construction site in Shenzhen city were updated using both the site-specific correlations and the generic correlations given by SZ-CLAY/11/5130. The constructed multivariate probability density distribution is necessary for this updating step. © 2023 Elsevier Ltd",-,10.1016/j.compgeo.2023.105934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177563485&doi=10.1016%2fj.compgeo.2023.105934&partnerID=40&md5=6267cb141be89eb88be3fb0e4c091fd0,2024
A Visual Analytic System for Ranking Multi-attribute Data Using Multi-level Pareto Frontier,"Ranking data items is a core step of decision making in many scenarios, such as investigating companies in finance and evaluating players in sports. While sorting by a single attribute is often trivial, ranking based on multi-attribute is often fuzzy, meaning that the goals, constraints, and weights of factors are not well-defined. Existing techniques use aggregation or dimension reduction to map the data along a single axis, which destroys the high-dimensional structure of data. In this paper, we aim to reduce the scope of data instead of the dimensionality in ranking tasks. In this way, users make decisions among a few data items based on complete information, instead of ranking many items based on inaccurate information. We introduce the concept of Pareto frontier for partitioning the data into multiple groups. A visual analytic system with two coordinated views is designed for users to rank data in an individual group or compare items in multiple groups. We evaluate the effectiveness of the proposed system through case studies and usability through a user study.  © 2023 ACM.",-,10.1145/3615522.3615529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178378372&doi=10.1145%2f3615522.3615529&partnerID=40&md5=bb3d7dde268eed593a395ca6b8b38d58,2022
An ontology-based secure design framework for graph-based databases,"Graph-based databases are concerned with performance and flexibility. Most of the existing approaches used to design secure NoSQL databases are limited to the final implementation stage, and do not involve the design of security and access control issues at higher abstraction levels. Ensuring security and access control for Graph-based databases is difficult, as each approach differs significantly depending on the technology employed. In this paper, we propose the first technology-ascetic framework with which to design secure Graph-based databases. Our proposal raises the abstraction level by using ontologies to simultaneously model database and security requirements together. This is supported by the TITAN framework, which facilitates the way in which both aspects are dealt with. The great advantages of our approach are, therefore, that it: allows database designers to focus on the simultaneous protection of security and data while ignoring the implementation details; facilitates the secure design and rapid migration of security rules by deriving specific security measures for each underlying technology, and enables database designers to employ ontology reasoning in order to verify whether the security rules are consistent. We show the applicability of our proposal by applying it to a case study based on a hospital data access control. © 2023 The Author(s)",-,10.1016/j.csi.2023.103801,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175322685&doi=10.1016%2fj.csi.2023.103801&partnerID=40&md5=51314722f059cad325fc3e759dd406b2,2022
PassViz: An Interactive Visualisation System for Analysing Leaked Passwords,"Passwords remain the most widely used form of user authentication, despite advancements in other methods. However, their limitations, such as susceptibility to attacks, especially weak passwords defined by human users, are well-documented. The existence of weak human-defined passwords has led to repeated password leaks from websites, many of which are of large scale. While such password leaks are unfortunate security incidents, they provide security researchers and practitioners with good opportunities to learn valuable insights from such leaked passwords, in order to identify ways to improve password policies and other security controls on passwords. Researchers have proposed different data visualisation techniques to help analyse leaked passwords. However, many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs. This paper reports PassViz, a novel method that combines the edit distance with the t-SNE (t-distributed stochastic neighbour embedding) dimensionality reduction algorithm [19] for visualising and analysing leaked passwords in a 2-D space. We implemented PassViz as an easy-to-use command-line tool for visualising large-scale password databases, and also as a graphical user interface (GUI) to support interactive visual analytics of small password databases. Using the '000webhost' leaked database as an example, we show how PassViz can be used to visually analyse different aspects of leaked passwords and to facilitate the discovery of previously unknown password patterns. Overall, our approach empowers researchers and practitioners to gain valuable insights and improve password security through effective data visualisation and analysis.  © 2023 IEEE.",33-42,10.1109/VizSec60606.2023.00011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186670442&doi=10.1109%2fVizSec60606.2023.00011&partnerID=40&md5=35b3db651ca8d8cc97eda04a8ef3a3a8,2023
A Novel QR Code Based Smart Attendance Tracking System,"This work proposes a novel attendance tracking system based on QR code. The implementation of a QR-based attendance system in educational institutions represents a technological advancement which ensures the accuracy of attendance tracking. The process begins with students registering with the institutional organization. Following registration, each student is assigned a unique QR code, which is subsequently printed on their institutional ID card. This QR code serves as a personalized identifier for attendance tracking purposes. The operational procedure involves students scanning their QR codes using installed cameras upon entering and leaving the college premises. The system captures this data and calculates attendance percentages automatically. The recorded information includes not only the attendance status but also the precise entry and exit timestamps. This comprehensive dataset is then stored in the student database, organized according to the academic calendar. The utilization of QR codes in this context simplifies the overall system operation, reducing the chances of errors in attendance tracking. The implementation of technology into the attendance recording process enhances efficiency and minimizes the load on administrative staff.  © 2023 IEEE.",-,10.1109/IEMENTech60402.2023.10423485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186270423&doi=10.1109%2fIEMENTech60402.2023.10423485&partnerID=40&md5=aa873dee146ef4b40061bfe36d0ee22e,2023
A systematic analysis of read-across within REACH registration dossiers,"Read-across is a well-established data-gap filling technique used within analogue or category approaches. Acceptance remains an issue, mainly due to the difficulties of addressing residual uncertainties associated with a read-across prediction and because assessments are expert-driven. Frameworks to develop, assess and document read-across may help reduce variability in read-across results. Data-driven read-across approaches such as Generalised Read-Across (GenRA) include quantification of uncertainties and performance. GenRA also offers opportunities on how New Approach Method (NAM) data can be systematically incorporated to support the read-across hypothesis. Herein, a systematic investigation of differences in expert-driven read-across with data-driven approaches was pursued in terms of building scientific confidence in the use of read-across. A dataset of expert-driven read-across assessments that made use of registration data as disseminated in the public International Uniform Chemical Information Database (IUCLID) (version 6) of Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) Study Results were compiled. A dataset of ∼5000 read-across cases pertaining to repeated dose and developmental toxicity was extracted and mapped to content within EPA's Distributed Structure Searchable Toxicity database (DSSTox) to retrieve chemical name and structural identification information. Content could be mapped to ∼3600 cases which when filtered for unique cases with curated quantitative structure–activity relationship-ready SMILES resulted in 389 target-source analogue pairs. The similarity between target and the source analogues on the basis of different contexts – from structural similarity using chemical fingerprints to metabolic similarity using predicted metabolic information was evaluated. An attempt was also made to quantify the relative contribution each similarity context played relative to the target-source analogue pairs by deriving a model which predicted known analogue pairs. Finally, point of departure values (PODs) were predicted using the GenRA approach underpinned by data extracted from the EPA's Toxicity Values Database (ToxValDB). The GenRA predicted PODs were compared with those reported within the REACH dossiers themselves. This study offers generalisable insights on how read-across is already applied for regulatory submissions and expectations on the levels of similarity necessary to make decisions. © 2024",-,10.1016/j.comtox.2024.100304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187208184&doi=10.1016%2fj.comtox.2024.100304&partnerID=40&md5=d0fbc75b48f0cb662e577cc8eab199e2,2021
AutoQUBO v2: Towards Efficient and Effective QUBO Formulations for Ising Machines,"The QUBO framework provides a way to model, in principle, any combinatorial optimization problem and enables the use of Ising machines to solve it. Ising machines are devices designed to quickly find good solutions to QUBO problems. In previous work, AutoQUBO was designed to automatically generate QUBO formulations from a high-level problem description. We address two shortcomings of this method. It only works on a per-instance basis, making repeated formulations for similar problems inefficient, and relies on the user to specify penalty weights. This work introduces symbolic sampling, which provides QUBO formulations for entire problem classes. We demonstrate the speedup that can be achieved with this approach using instances of the maximum clique problem. Additionally, we use proven methods to compute valid penalty weights automatically to simplify the translation process. By providing a user-friendly way to generate QUBO formulations in an efficient manner, both in terms of time and problem difficulty, we enable more people to use Ising machines for combinatorial optimization. © 2023 Copyright held by the owner/author(s).",227-230,10.1145/3583133.3590662,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168996309&doi=10.1145%2f3583133.3590662&partnerID=40&md5=0771783376d296b98670f9c3c75e2263,2021
DeepGANTT: A Scalable Deep Learning Scheduler for Backscatter Networks,"Novel backscatter communication techniques enable battery-free sensor tags to interoperate with unmodified standard IoT devices, extending a sensor network's capabilities in a scalable manner. Without requiring additional dedicated infrastructure, the battery-free tags harvest energy from the environment, while the IoT devices provide them with the unmodulated carrier they need to communicate. A schedule coordinates the provision of carriers for the communications of battery-free devices with IoT nodes. Optimal carrier scheduling is an NP-hard problem that limits the scalability of network deployments. Thus, existing solutions waste energy and other valuable resources by scheduling the carriers suboptimally. We present DeepGANTT, a deep learning scheduler that leverages graph neural networks to efficiently provide near-optimal carrier scheduling. We train our scheduler with optimal schedules of relatively small networks obtained from a constraint optimization solver, achieving a performance within 3% of the optimum. Without the need to retrain, our scheduler generalizes to networks 6 × larger in the number of nodes and 10 × larger in the number of tags than those used for training. DeepGANTT breaks the scalability limitations of the optimal scheduler and reduces carrier utilization by up to compared to the state-of-the-art heuristic. As a consequence, our scheduler efficiently reduces energy and spectrum utilization in backscatter networks.  © 2023 Owner/Author.",163-176,10.1145/3583120.3586957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160025874&doi=10.1145%2f3583120.3586957&partnerID=40&md5=ddaceff71f80658cb78b5e12ec5fc690,2023
PIRAT - Tool for Automated Cyber-risk Assessment of PLC Components & Systems Deploying NVD CVE & MITRE ATT&CK Databases,"Programmable Logic Controllers (PLCs) are the backbone of modern-day Industrial Control Systems (ICSs), and as such play a key role in many critical infrastructure sectors (e.g., water and water-waste management, power distribution, transportation, food and agriculture, critical manufacturing, etc.). Given the important functions that PLCs carry out within many critical infrastructures, a cyber-compromise of even a single PLC device can have far-reaching impact and consequences, ranging from distribution-system outages, environmental pollution, mass water and food poisoning, to outright loss of human life. The objective of this work-in-progress is to develop a free open source tool, named PIRAT, for cyber-risk assessment of individual PLC components, as well as more complex PLC systems. The tool synthesizes the user-provided PLC component/system information with the readily available data from the National Vulnerability Database (NVD) and MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK) database. The output of the tool is an aggregate risk scores for the given PLC component/system. The risk score is derived not only based on the known PLC vulnerabilities, but also based on the presence and capabilities of advance persistent threat (APT) groups potentially targeting the given PLC component/system and/or targeting the respective critical infrastructure industry. © ICCPS 2023. All rights reserved.",237-238,10.1145/3576841.3589614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167865905&doi=10.1145%2f3576841.3589614&partnerID=40&md5=c4cb0fb85559cad49cf52e98954aa369,2020
An Optical Signal Simulator for the Characterization of Photoplethysmographic Devices,"(1) Background: An optical simulator able to provide a repeatable signal with desired characteristics as an input to a photoplethysmographic (PPG) device is presented in order to compare the performance of different PPG devices and also to test the devices with PPG signals available in online databases. (2) Methods: The optical simulator consists of an electronic board containing a photodiode and LEDs at different wavelengths in order to simulate light reflected by the body; the PPG signal taken from the chosen database is reproduced by the electronic board, and the board is used to test a wearable PPG medical device in the form of earbuds. (3) Results: The PPG device response to different average and peak-to-peak signal amplitudes is shown in order to assess the device sensitivity, and the fidelity in tracking the actual heart rate is also investigated. (4) Conclusions: The developed optical simulator promises to be an affordable, flexible, and reliable solution to test PPG devices in the lab, allowing the testing of their actual performances thanks to the possibility of using PPG databases, thus gaining useful and significant information before on-the-field clinical trials. © 2024 by the authors.",-,10.3390/s24031008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184668098&doi=10.3390%2fs24031008&partnerID=40&md5=692980ac41110012634858fd2cc94e99,2021
Workshop on Simplicity in Management of Data (SiMoD),"At a first glance, database systems today are complex, with various components, tuning knobs and delicate design decisions, leading to hundreds of thousands lines of code (if not more). However, at their core - as researchers and practitioners have been observing at least anecdotally - are simple ideas that work well in practice. Meanwhile, it also often takes a tremendous amount of experience to propose such ideas that are simple but not trivial. SiMoD is a new workshop dedicated to promoting and documenting such ideas as they are often ""buried""in details as part of a full paper or product, or only shared anecdotally among experienced practitioners, creating barriers for newcomers to the field. The workshop will also be a great venue for junior researchers and PhD students to learn about the importance of simple but effective ideas, and get feedback about their on-going work.  © 2023 Owner/Author.",301-302,10.1145/3555041.3590817,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162891790&doi=10.1145%2f3555041.3590817&partnerID=40&md5=95995a06e18b57b526ad687286a9d140,2023
Development of IoT Based Weather Reporting System,"Weather reporting is important in our daily lives because it provides vital information that allows us to make informed decisions about how to plan our activities and protect ourselves from potential hazards. It is also important in agriculture, where weather conditions like temperature, rainfall, and humidity can have a big impact on crop yields and harvests. In this project, a hardware module comprising of ESP-32 controller and sensors is developed to capture the various weather parameters and to upload these parameters to the cloud database MYSQL. Weather parameters over past 21 years are also collected from the source Data Access Viewer (DAV) website. The cloud database comprising of the historical data from 1st January 2001 to 1st January 2022 is created. This database is updated periodically with the sensor data captured using hardware module in real time on hourly basis. Machine learning algorithms are implemented to predict the weather parameters for the duration specified by the users. Also possible crops that can be grown is suggested. HTML web page with user interface is created for collecting user queries, to display predicted weather parameters and also to suggest the crops that could be cultivated in the region.  © 2023 IEEE.",-,10.1109/NKCon59507.2023.10396396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184997563&doi=10.1109%2fNKCon59507.2023.10396396&partnerID=40&md5=b666ba3696d4bb6c2f4efb67371d4086,2020
An optimized fuzzy based FP-growth algorithm for mining temporal data,"Data mining is one of the emerging technologies used in many applications such as Market analysis and Machine learning. Temporal data mining is used to get a clear knowledge about current trend and to predict the upcoming future. The rudimentary challenge in introducing a data mining procedure is, processing time and memory consumption are highly increasing while trying to improve the accuracy, precision or recall. As well as, while trying to reduce the processing time or memory consumption, accuracy, precision and recall values are reducing significantly. So, for improving the performance of the system and to preserve the memory and processing time, Three-Dimensional Fuzzy FP-Tree (TDFFPT) is proposed for Temporal data mining. Three functional modules namely, Three-Dimensional Temporal data FP-Tree (TTDFPT), Fuzzy Logic based Temporal Data Tree Analyzer (FTDTA) and Temporal Data Frequent Itemset Miner (TDFIM) are integrated in the proposed method. This algorithm scans the database and generates frequent patterns as per the business need. Every time a client purchases a new item, it gets stored in the recent database layer instead of rescanning the entire records which are placed in the old layer. The results obtained shows that the performance of the proposed model is more efficient than that of the existing algorithm in terms of overall accuracy, processing time, reduction in the memory utilization, and the number of databases scans. In addition, the proposed model also provides improved decision making and accurate pattern prediction in the time series data. © 2024 - IOS Press. All rights reserved.",41-51,10.3233/JIFS-223030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182526036&doi=10.3233%2fJIFS-223030&partnerID=40&md5=6cb27f7860ed7b8893c948173c218b94,2021
Bibliometric Study on E-Banking as ICT Solutions,"Bibliometric analysis is a quantitative method for analyzing and tracking the current state of research in a specific field of academic interest. Numerous studies have been conducted on e-banking over the last three decades. The main aim of this study is to investigate the existing literature available on e-banking through bibliometric analysis. This research has utilized bibliometric analysis to evaluate research papers in the field of e-banking. Articles published up to 19th September 2022 were considered for bibliometric analysis and were retrieved from the Scopus database. 226 articles related to e-banking were analyzed using biblioshiny and VOSviewer techniques. The study identifies the most productive countries and authors that are deeply engaged in the research area of e-banking. Further, the study's findings displayed the most repeated keywords, top-cited articles, top-productive journals and chronological publications of articles. The study's findings are based on secondary data, and no primary data were collected for the research. The research limits document retrieval to a single database (Scopus) and extraction of the documents was based on specific keywords from the Business Management and Accounting disciplines. As a result, the outcomes could not be generalized. Academicians interested in exploring the topic of e-banking can benefit from this bibliometric research. It provides information on this field's prominent journals, authors, and nations.  © 2023 IEEE.",-,10.1109/ICTBIG59752.2023.10456144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189241966&doi=10.1109%2fICTBIG59752.2023.10456144&partnerID=40&md5=765695431ae7405196a20d3e681e5656,2023
Towards Contactless Fingerprint Presentation Attack Detection using Algorithms from the Contact-based Domain,"In this work, we investigate whether contact-based fingerprint Presentation Attack Detection (PAD) methods can generalize to the contactless domain. To this end, we selected a state-of-the-art patch-based fingerprint PAD algorithm which achieved high detection performance in the contact-based domain and adapted it for contactless fingerprints. We train and test the method using three contactless fingerprint databases and evaluate its generalization capabilities using Leave-One-Out (LOO) pro-tocols. Further, we acquired a new PAD database and use it in a cross-database evaluation. The adopted method shows low error rates in most scenarios and can generalize to unseen contactless presentation attacks.  © 2023 IEEE.",-,10.1109/BIOSIG58226.2023.10345977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182406591&doi=10.1109%2fBIOSIG58226.2023.10345977&partnerID=40&md5=517928b5c8fbe2b162862bda09619ec7,2023
COLFIPAD: A Presentation Attack Detection Benchmark for Contactless Fingerprint Recognition,"Contactless fingerprint recognition is an emerging biometric technology and Presentation Attack Detection (PAD) methods are crucial to preserve system security. Convolutional Neural Networks (CNNs) represent the state-of the-art of PAD algorithms for many contactless captured biometric characteristics and various research groups proposed specialized CNN-based PAD methods or used general purpose CNNs to detect Presentation Attacks (PAs). In this work, we compare nine CNN-based PAD methods for contactless fingerprint PAD: five general purpose algorithms, and four dedicated PAD methods designed for various biometric characteristics. To achieve this, we combine the COLFISPOOF database with three bona fide databases: the HDA database and both versions of the ISPFD database. We set up our experiments using a baseline evaluation protocol and four Leave-One-Out (LOO) protocols, to benchmark the generalization capabilities to unseen data. The results reported by using the Attack Presentation Classification Error Rate (APCER) vs. Bona fide Presentation Classification Error Rate (BPCER) and the Detection Equal Error Rate (D-EER). Further, we discuss the achieved results in detail and give recommendations for real-world implementations. Our results show that established PAD algorithms for other biometric characteristics can accurately detect PAs on contactless fingerprints. While strong deviations between the considered PAD algorithms are observed, the best performing method shows a D-EER between 0.01% and 0.08% (depending on the LOO partition) and a APCER of 0.00% at a BPCER of 1.00%. © 2023 IEEE.",-,10.1109/IJCB57857.2023.10448552,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182398751&doi=10.1109%2fIJCB57857.2023.10448552&partnerID=40&md5=d795246bc7fb9700a3469977b605f04e,2021
MCLFIQ: Mobile Contactless Fingerprint Image Quality,"We propose MCLFIQ: Mobile Contactless Fingerprint Image Quality, the first quality assessment algorithm for mobile contactless fingerprint samples. To this end, we re-trained the NIST Fingerprint Image Quality (NFIQ) 2 method, which was originally designed for contact-based fingerprints, with a synthetic contactless fingerprint database. We evaluate the predictive performance of the resulting MCLFIQ model in terms of Error-vs.-Discard Characteristic (EDC) curves on three real-world contactless fingerprint databases using three recognition algorithms. In experiments, the MCLFIQ method is compared against the original NFIQ 2 method, a sharpness-based quality assessment algorithm developed for contactless fingerprint images and the general purpose image quality assessment method BRISQUE. Furthermore, benchmarks on four contact-based fingerprint datasets are also conducted. Obtained results show that the fine-tuning of NFIQ 2 on synthetic contactless fingerprints is a viable alternative to training on real databases. Moreover, the evaluation shows that our MCLFIQ method works more accurately and is more robust compared to all baseline methods on contactless fingerprints. We suggest considering the proposed MCLFIQ method as a starting point for the development of a new standard algorithm for contactless fingerprint quality assessment. Authors",1-1,10.1109/TBIOM.2024.3377686,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188472023&doi=10.1109%2fTBIOM.2024.3377686&partnerID=40&md5=6cfec52239e202147c6fe32cf14ca7f4,2021
REC CONNECT: A Comprehensive Android File Transfer Application,"The REC CONNECT Android app simplifies file sharing among devices by allowing users to exchange files via QR codes or IP addresses. The application was created in Android Studio, which is integrated with Firebase database for performing optimal storage and transfers. Its easy-to-use interface allows users to generate QR codes or enter recipient IP addresses. This adaptability accommodates to a wide range of user preferences and configurations. Receivers initiate quick file transfers by scanning the sender's QR code or utilizing IP addresses, ensuring cross-platform interoperability. The app's functionality is supported by Firebase integration to ensure secure file storage and transfer. The real-time database handles metadata, while Firebase storage safeguards the actual file data. REC CONNECT holds the potential for expansion, with prospects like robust file management, varied format compatibility, and cloud integration. The app's reliability and privacy-concerned design address the data security concerns. The developed REC CONNECT application reshapes file sharing and increases the efficiency, and security. © 2023 Elsevier B.V.. All rights reserved.",903-913,10.1016/j.procs.2023.12.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184122993&doi=10.1016%2fj.procs.2023.12.042&partnerID=40&md5=7923d64ea29e1d03beec0b955a587ca4,2023
Intelligent Network Device Identification based on Active TCP/IP Stack Probing,"With the continuous development of network devices, there are increasingly types and quantities of network devices. Accurate identification of device types helps proactively protect potentially vulnerable devices exposed on the Internet. Among the network device identification methods, the TCP/IP stack active detection method is an important kind since it does not require many open ports of target devices. However, its performance is limited by the rule/fingerprint database quality. Maintaining such a database requires a lot of expert effort, making the method difficult to scale up. To solve the scalability problem, our insight in this paper is to use machine learning methods to generate network device classifiers without needing expert effort. However, generating labeled datasets, extracting features, and selecting features without expert effort is non-trivial. We propose <italic>IntelliNDI</italic>, an intelligent and active network device identification method. <italic>IntelliNDI</italic> collects network device type information from multiple cyberspace search engines and filter out the network devices with different types on different search engines. We regard the approximately consistent network device types as the ground truth network device types. As for feature extraction, we use the same attributes employed in the well-known Nmap protocol stack detection to avoid the requirements for constructing features. Finally, we select features with basic ML methods. We implement <italic>IntelliNDI</italic> for several kinds of typical network devices. The trained classifiers in our experiments can achieve 90 percent accuracy. IEEE",1-1,10.1109/MNET.2024.3374080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187343825&doi=10.1109%2fMNET.2024.3374080&partnerID=40&md5=7681fd56fb8d9ae8a333ab1b9349cd58,2021
"Bibliometric Analysis on Identifying Plant, Crop Diseases Using Machine Learning and Deep Learning","This paper is intended to explore the research done on identifying the diseased plants and crops using Machine Learning (ML) and Deep Learning (DL) techniques during last 10 years using bibliometric methods. In this study, we used Scopus database to analyze on 'Plant disease' or 'Crop disease' using 'Machine Learning' or 'Deep Learning' or 'Neural Networks'. This paper focuses on the importance of ML and DL techniques in identifying plant or crop diseases. The database collected from the Scopus is analyzed using VOSviewer software of version 1.6.16. The study is limited to publications from conferences, journals with subject areas are limited to Computer Science, Engineering and languages limited to English and Chinese. Scopus search outputs 824 articles on Plant or Crop diseases with ML, DL and Neural Networks covering conference papers and journal articles. Statistics showed that more articles were published during the last five years and major contributions were from India. By analyzing database on Authors, Subject area, Keywords, Affiliation, Source type it is evident that there is plenty of research scope in this area. Network analysis on diverse parameters specifies that there is a good scope to do research in this topic using advanced deep learning techniques. © 2023 The authors and IOS Press.",113-118,10.3233/ATDE221245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149493757&doi=10.3233%2fATDE221245&partnerID=40&md5=450956a86b46582de5680a3b64efdc90,2020
Image recognition based attendance system with spoof detection,"In recent times numerous face recognition algorithms were used for the identification and authentication of a person to a system. The objective of this design is to recognize the human faces and forecast whether the detected faces are real or fake. Our design is mainly useful for institutional purpose to enhance and modernize the present attendance system into more effective and efficient than before. In this design, human face database is dynamically created by new user registration and this data is pumped into the recognizer algorithm. In this design we have added a spoofing discovery technology that’s used to judge whether the face in front of the machine is real or fake. The face presented by other media can be defined as false face, including hardcopy photographs, display screen of electronic products, etc. The dataset which are dynamically created while a new user is registered is stored in the database and whenever a new face is detected on the camera, it compares with the images that are already present in the database to mark the attendance. Also, the result image is generated with the chance of spoofing can be attained from the dataset pushed into the algorithm. Our proposed system has accuracy of above 97.6% and true positive rate is 99.7% in high precision model. © 2024, CRC Press/Balkema. All rights reserved.",482-486,10.1201/9781032684994-76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186768973&doi=10.1201%2f9781032684994-76&partnerID=40&md5=2acf8a192f83645e43f1b4ea2077e04f,2024
"A Comprehensive Analysis of Cloud Migration Strategies: Efficiency Comparison of Trickle, Big Bang, Refactoring, Lift and Shift, Replatforming Approaches","To resolve the poor efficiency issue encountered during database migration and ensure that the complete database is effectively transferred with less amount of time using Trickle Approach over Lift and Shift approach, Big Bang Approach, Refactoring technique, Replatforming technique. For thorough experimentation, we employ the trickle migration approach and the four traditional techniques. This experiment has a sample size of 10. The IBM Employee Dataset is used as a sample dataset, and it contains 1,059 rows of data. In this performed experimental analysis, the trickle migration approach is more efficient than Lift and Shift approach, Big Bang Approach, Refactoring technique, Replatforming technique in all aspects such as cost, time and quality of the data. The experiment shows the performance of Trickle migration approach with efficiency of (85.76%) when compared to the Big Bang Migration approach (77.49%), Refactoring technique (78.71%), Lift and Shift (76.72%), Replatforming Technique (76.96%). A significance test was performed on the five groups, resulting in a p-value of two-tailed is 0.008 for Big Bang, 0.016 for Refactoring, 0.004 for Lift and shift, 0.001 for Replatform, which is lesser than 0.05 (p<0.05) it is evident that there is an existence of statistically significant differences between these groups. The Trickle Migration approach is found to be more effective than the four traditional techniques in this research based on the data obtained. © 2023 IEEE.",-,10.1109/ICSSS58085.2023.10407074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185004808&doi=10.1109%2fICSSS58085.2023.10407074&partnerID=40&md5=196f34a893505f84133b149523d075e4,2023
2PLSF: Two-Phase Locking with Starvation-Freedom,"Invented more than 40 years ago, the two-phase locking concurrency control (2PL) is capable of providing opaque transactions over multiple records. However, classic 2PL can suffer from live-lock and its scalability is low when applied to workloads with a high number of non-disjoint read accesses. In this paper we propose a new 2PL algorithm (2PLSF) which, by utilizing a novel reader-writer lock, provides highly scalable transactions with starvation-freedom guarantees. Our 2PLSF concurrency control can be applied to records, metadata and to indexing data structures used in database management systems (DBMS). In our experiments we show that 2PLSF is often superior to classical 2PL and can surpass concurrency controls with optimistic reads, simultaneously providing high throughput and low latency for disjoint and non-disjoint workloads.  © 2023 ACM.",39-51,10.1145/3572848.3577433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149321298&doi=10.1145%2f3572848.3577433&partnerID=40&md5=689592766bfc2820c1543de2ed1e3388,2021
A Tool for Database Masking and Anonymization of PostgreSQL,"Managing and securing sensitive data in a production database can be a complex and challenging task, especially when creating a staging database for testing and development purposes. This is why we have designed a tool that can automate the process of creating a staging database from a backup of the production database while anonymizing critical data based on user input. Our tool is specifically designed for PostgreSQL databases and can be easily integrated into the database management workflow. It allows users to select which tables and fields in the database should be anonymized, with options for different anonymization techniques based on the data type. This ensures that sensitive data is properly protected and complies with data privacy regulations. To ensure the success of the anonymization process, the tool utilizes machine learning algorithms that analyze past user input to predict which fields are likely to be selected for anonymization. This helps reduce the risk of errors and ensures that all critical data is properly anonymized with high accuracy. In the future, we plan to enhance the tool with more advanced anonymization techniques and incorporate additional machine-learning algorithms to improve the accuracy of the prediction models. This will help further streamline the database management process and ensure that sensitive data is always properly secured. © 2023 IEEE.",-,10.1109/ICDSAAI59313.2023.10452571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187794873&doi=10.1109%2fICDSAAI59313.2023.10452571&partnerID=40&md5=07ffdc4fec9bd6bca093e31fcacd02cf,2022
The UDC California Division Members Database: Using Spatial Analysis to Shed new Light on the Formation of Civil War Memory in California,"The United Daughters of the Confederacy (UDC) in California played a pivotal role in advancing the ""Lost Cause""narrative in the state, including erecting Confederate Memorials. Their activities molded a Californian public perception that aligned more with the Southern viewpoint of the Civil War. Given the substantial influence California holds in U.S. culture and politics, understanding this shift is crucial. While there's a wealth of research on the national UDC and its operations in the South, the California branch remains under-explored. In this paper, I present a detailed digital database of all the 6637 organization's members and officers from roughly half the years between 1914 to 1979, with additional summarized data for the years 1900-1914. The database is a result of digitization of membership rolls, including role and addresses, from several dozens yearbooks: scanning, transcribing, extracting structured data from the transcription, curating and finally geolocating all addresses. This detailed database enables first of a kind research on this organization: locating the exact addresses on a map not only enables analyzing the geospatial distribution of the organization with its ebbs and flows over time, but also tracking the immediate surroundings of member concentrations, enabling innovative insights into path of influence the organization had on Californian, and indeed American,  © 2023 Owner/Author.",31-39,10.1145/3615887.3627753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180122700&doi=10.1145%2f3615887.3627753&partnerID=40&md5=3082688ca3909abdb45a502db12422ea,2023
AOCL-Compression - A High Performance Optimized Lossless Data Compression Library,"Data compression is the process of encoding (or compressing) information using fewer bits than originally present in the data stream or signal. Depending upon whether this process is invertible or not, data compression can be lossless or lossy. While there exists various popular implementations of different lossless data compression methods, they are unable to completely meet the performance requirements demanded by the ever-increasing data usage of the applications. In this paper, we present a comparative survey of different loss less compression methods and introduce a high performance compression library called AOCL-Compression optimized for x86 architecture in general and AMD's 'Zen'-based processors in particular. This library supports LZ4, LZ4HC, ZLIB, ZSTD, LZMA, BZIP2, and Snappy based compression methods. This paper discusses the design features of the new library framework and the algorithmic optimizations implemented for the different compression methods. Results highlighting the performance benefits of this new library implementation over the reference implementations of the respective compression methods are also presented.  © 2023 IEEE.",-,10.1109/HPEC58863.2023.10363590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182607473&doi=10.1109%2fHPEC58863.2023.10363590&partnerID=40&md5=d155e61be22352f946cdebd50020bae8,2024
A novel decomposition-based architecture for multilingual speech emotion recognition,"Multilingual speech emotion recognition (MLSER) is a significant and demanding research domain to improve the utility of human–computer interaction systems. Identifying the emotions from the spoken sentence is one of the most challenging tasks due to the dependency of the MLSER system on spoken languages. This study proposes a novel decomposition-based architecture for MLSER. The architecture includes silence removal, mode tuning, signal reconstruction, feature extraction, feature optimization and classification. In preprocessing, the silence part is removed using short-time energy and spectral centroid. After that, variational mode decomposition is applied for signal decomposition, where the improved Bhattacharyya distance is explored for the decomposition mode tuning. The tuned modes are examined for noise removal, and the signal is reconstructed using denoised modes. The spectral and prosodic features are computed from the reconstructed signal. The optimized features are obtained from the extracted features using the ReliefF algorithm. Finally, the fine k-nearest neighbor classifier is explored with optimized features to identify the emotions. For the experiment, three publicly available emotion databases, namely the English language-based Ryerson audio–visual database (RAVDESS), German language-based emotional speech Berlin database (Emo-DB) and Italian emotional speech database (EMOVO), are used. The proposed method yielded 90.7%, 94% and 91.1% accuracy for English, German and Italian language-based database, respectively. A multilingual database is created with these three databases, and the proposed method yields 93.4% accuracy for this database. The proposed framework provides more efficient and minimum language dependency compared to available traditional and deep learning-based approaches. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",-,10.1007/s00521-024-09577-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186938943&doi=10.1007%2fs00521-024-09577-2&partnerID=40&md5=475e73320acc470d02fd121536e98aba,2021
An Efficient Approach for Indoor Facility Location Selection,"The advancement of indoor location-aware technologies enables a wide range of location based services in indoor spaces. In this paper, we formulate a novel Indoor Facility Location Selection (IFLS) query that finds the optimal location for placing a new facility (e.g., a coffee station) in an indoor venue (e.g., a university building) such that the maximum distance of all clients (e.g., staffs/students) to their nearest facility is minimized. To the best of our knowledge we are the first to address this problem in an indoor setting. We first adapt the state-of-the-art solution in road networks for indoor settings, which exposes the limitations of existing approaches to solve our problem in an indoor space. Therefore, we propose an efficient approach which prunes the search space in terms of the number of clients considered, and the total number of facilities retrieved from the database, thus reducing the total number of indoor distance calculations required. The key idea of our approach is to use a single pass on a state-of-the-art index for an indoor space, and reuse the nearest neighbor computation of clients to prune irrelevant facilities and clients. We evaluate the performance of both approaches on four indoor datasets. Our approach achieves a speedup from 2.84× to 71.29× for synthetic data and 97.74× for real data over the baseline. © 2023 Copyright held by the owner/author(s)",632-644,10.48786/edbt.2023.53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165067003&doi=10.48786%2fedbt.2023.53&partnerID=40&md5=7b681de49f9f4109ec4ea11777718fc1,2023
Face Database Protection Using Encryption Technique,"Biometric is being extensively used for security applications. The major concern with biometric features is, there are limited features for a single person. Compromise of these features will lead to compromising of security and it will be impossible to replace these features as they are unique to every person. For securing biometric database various methods have been proposed like cancelable biometric and biometric cryptosystem. The issues arising from these are matching template in transform domain, authenticity etc. To overcome this problem, encryption-based method is proposed for database present in the server. The transfer of database from server to terminal for authentication is done in encrypted domain along with storage. For authentication, the database is decrypted at the terminal side. The cipher output and security parameters shown in the paper verifies that the algorithm is successful in securing facial database.  © 2023 IEEE.",-,10.1109/PuneCon58714.2023.10450006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187798442&doi=10.1109%2fPuneCon58714.2023.10450006&partnerID=40&md5=a790e76d3d03ae490346428d24d7f49b,2023
QRS Detector Performance Evaluation Aware of Temporal Accuracy and Presence of Noise,"Algorithms for QRS detection are fundamental in the ECG interpretive processing chain. They must meet several challenges, such as high reliability, high temporal accuracy, high immunity to noise, and low computational complexity. Unfortunately, the accuracy expressed by missed or redundant events statistics is often the only parameter used to evaluate the detector’s performance. In this paper, we first notice that statistics of true positive detections rely on researchers’ arbitrary selection of time tolerance between QRS detector output and the database reference. Next, we propose a multidimensional algorithm evaluation method and present its use on four example QRS detectors. The dimensions are (a) influence of detection temporal tolerance, tested for values between 8.33 and 164 ms; (b) noise immunity, tested with an ECG signal with an added muscular noise pattern and signal-to-noise ratio to the effect of “no added noise”, 15, 7, 3 dB; and (c) influence of QRS morphology, tested on the six most frequently represented morphology types in the MIT-BIH Arrhythmia Database. The multidimensional evaluation, as proposed in this paper, allows an in-depth comparison of QRS detection algorithms removing the limitations of existing one-dimensional methods. The method enables the assessment of the QRS detection algorithms according to the medical device application area and corresponding requirements of temporal accuracy, immunity to noise, and QRS morphology types. The analysis shows also that, for some algorithms, adding muscular noise to the ECG signal improves algorithm accuracy results. © 2024 by the authors.",-,10.3390/s24051698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187446824&doi=10.3390%2fs24051698&partnerID=40&md5=6b772d2ba8286d3c647aee54d2b598e4,2021
A Generalization of the Chomsky-Halle Phonetic Representation using Real Numbers for Robust Speech Recognition in Noisy Environments,"Speech recognition is difficult when the speech signal is weak or occurs in a noisy environment. This paper presents an efficient and robust method that can reconstruct the standard pronunciation of English phonemes and words given a weak or noisy signal. The reconstruction is based on a novel representation of the reconstruction task as a problem of data retrieval from a database in two different cases: (1) when the phonemes are represented in the database as binary tuples and the input is also a binary tuple from which deletion errors occur, and (2) when the phonemes are represented in the database and in the input as tuples of real values ranging between 0 and 1. In the latter case, the input phoneme could contain both a higher or lower value than the standard phoneme in the database that is intended by the speaker. For case (2) a theorem is proven regarding when the data retrieval can be expected to be reliable.  © 2023 Owner/Author.",156-160,10.1145/3589462.3589488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161368446&doi=10.1145%2f3589462.3589488&partnerID=40&md5=902b658595b39eeff21760f774f6874f,2023
Bridging the Gap between Data Lakes and RDBMSs Efficient Query Processing with Parquet,"In the age of massive data, databases are getting less convenient for data exploration tasks due to the costly loading phase. Still, the highly optimized query engines of database systems are greatly beneficial for the performance of data analysis tasks. With our research, we want to bridge this gap and provide paramount analytical performance without the need of static data loading. Our approach enables the integration of Parquet files - one of the most used columnar file format in the data lake context - into the data processing pipeline of a database system in a convenient way. We allow end-users to benefit from the database system performance without a costly and time-consuming loading phase. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",-,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188508794&partnerID=40&md5=f476dc2f29bf7d1ab07eb600e9c95b27,2024
Candidate ferroelectrics via ab initio high-throughput screening of polar materials,"Ferroelectrics are a class of polar and switchable functional materials with diverse applications, from microelectronics to energy conversion. Computational searches for new ferroelectric materials have been constrained by accurate prediction of the polarization and switchability with electric field, properties that, in principle, require a comparison with a nonpolar phase whose atomic-scale unit cell is continuously deformable from the polar ground state. For most polar materials, such a higher-symmetry nonpolar phase does not exist or is unknown. Here, we introduce a general high-throughput workflow that screens polar materials as potential ferroelectrics. We demonstrate our workflow on 1978 polar structures in the Materials Project database, for which we automatically generate a nonpolar reference structure using pseudosymmetries, and then compute the polarization difference and energy barrier between polar and nonpolar phases, comparing the predicted values to known ferroelectrics. Focusing on a subset of 182 potential ferroelectrics, we implement a systematic ranking strategy that prioritizes candidates with large polarization and small polar-nonpolar energy differences. To assess stability and synthesizability, we combine information including the computed formation energy above the convex hull, the Inorganic Crystal Structure Database id number, a previously reported machine learning-based synthesizability score, and ab initio phonon band structures. To distinguish between previously reported ferroelectrics, materials known for alternative applications, and lesser-known materials, we combine this ranking with a survey of the existing literature on these candidates through Google Scholar and Scopus databases, revealing ~130 promising materials uninvestigated as ferroelectric. Our workflow and large-scale high-throughput screening lays the groundwork for the discovery of novel ferroelectrics, revealing numerous candidates materials for future experimental and theoretical endeavors. © 2024, The Author(s).",-,10.1038/s41524-023-01193-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182594979&doi=10.1038%2fs41524-023-01193-3&partnerID=40&md5=47e1d9d0e9cd8d898b29df875b5e79ba,2022
How Do Centrality Measures Choose the Root of Trees?,"Centrality measures are widely used to assign importance to graph-structured data. Recently, understanding the principles of such measures has attracted a lot of attention. Given that measures are diverse, this research has usually focused on classes of centrality measures. In this work, we provide a different approach by focusing on classes of graphs instead of classes of measures to understand the underlying principles among various measures. More precisely, we study the class of trees. We observe that even in the case of trees, there is no consensus on which node should be selected as the most central. To analyze the behavior of centrality measures on trees, we introduce a property of tree rooting that states a measure selects one or two adjacent nodes as the most important, and the importance decreases from them in all directions. This property is satisfied by closeness centrality but violated by PageRank. We show that, for several centrality measures that root trees, the comparison of adjacent nodes can be inferred by potential functions that assess the quality of trees. We use these functions to give fundamental insights on rooting and derive a characterization explaining why some measure root trees. Moreover, we provide an almost linear time algorithm to compute the root of a graph by using potential functions. Finally, using a family of potential functions, we show that many ways of tree rooting exist with desirable properties. © Cristian Riveros, Jorge Salas, and Oskar Skibski; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150701662&doi=10.4230%2fLIPIcs.ICDT.2023.12&partnerID=40&md5=a74bb5bfc8949257384329cfc55442b1,2019
Some Vignettes on Subgraph Counting Using Graph Orientations,"Subgraph counting is a fundamental problem that spans many areas in computer science: database theory, logic, network science, data mining, and complexity theory. Given a large input graph G and a small pattern graph H, we wish to count the number of occurrences of H in G. In recent times, there has been a resurgence on using an old (maybe overlooked?) technique of orienting the edges of G and H, and then using a combination of brute-force enumeration and indexing. These orientation techniques appear to give the best of both worlds. There is a rigorous theoretical explanation behind these techniques, and they also have excellent empirical behavior (on large real-world graphs). Time and again, graph orientations help solve subgraph counting problems in various computational models, be it sampling, streaming, distributed, etc. In this paper, we give some short vignettes on how the orientation technique solves a variety of algorithmic problems. © C. Seshadhri; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150692217&doi=10.4230%2fLIPIcs.ICDT.2023.3&partnerID=40&md5=4803bf25e023c52e89b8ae8215570526,2020
Generalizing Greenwald-Khanna Streaming Quantile Summaries for Weighted Inputs,"Estimating quantiles, like the median or percentiles, is a fundamental task in data mining and data science. A (streaming) quantile summary is a data structure that can process a set S of n elements in a streaming fashion and at the end, for any ϕ ∈ (0, 1], return a ϕ-quantile of S up to an ε error, i.e., return a ϕ′-quantile with ϕ′ = ϕ ± ε. We are particularly interested in comparison-based summaries that only compare elements of the universe under a total ordering and are otherwise completely oblivious of the universe. The best known deterministic quantile summary is the 20-year old Greenwald-Khanna (GK) summary that uses O((1/ε) log (εn)) space [SIGMOD'01]. This bound was recently proved to be optimal for all deterministic comparison-based summaries by Cormode and Vesleý [PODS'20]. In this paper, we study weighted quantiles, a generalization of the quantiles problem, where each element arrives with a positive integer weight which denotes the number of copies of that element being inserted. The only known method of handling weighted inputs via GK summaries is the naive approach of breaking each weighted element into multiple unweighted items, and feeding them one by one to the summary, which results in a prohibitively large update time (proportional to the maximum weight of input elements). We give the first non-trivial extension of GK summaries for weighted inputs and show that it takes O((1/ε) log (εn)) space and O(log(1/ε) + log log(εn)) update time per element to process a stream of length n (under some quite mild assumptions on the range of weights and ε). En route to this, we also simplify the original GK summaries for unweighted quantiles. 2012 ACM Subject Classification Theory of computation → Streaming, sublinear and near linear time algorithms; Theory of computation → Approximation algorithms analysis; Theory of computation → Data structures design and analysis. © Sepehr Assadi, Nirmit Joshi, Milind Prabhu, and Vihan Shah; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150707536&doi=10.4230%2fLIPIcs.ICDT.2023.19&partnerID=40&md5=e666b523eff645f55df81ab92613ff06,2021
Probabilistic Query Evaluation with Bag Semantics,"We initiate the study of probabilistic query evaluation under bag semantics where tuples are allowed to be present with duplicates. We focus on self-join free conjunctive queries, and probabilistic databases where occurrences of different facts are independent, which is the natural generalization of tuple-independent probabilistic databases to the bag semantics setting. For set semantics, the data complexity of this problem is well understood, even for the more general class of unions of conjunctive queries: it is either in polynomial time, or #P-hard, depending on the query (Dalvi & Suciu, JACM 2012). Due to potentially unbounded multiplicities, the bag probabilistic databases we discuss are no longer finite objects, which requires a treatment of representation mechanisms. Moreover, the answer to a Boolean query is a probability distribution over non-negative integers, rather than a probability distribution over {true, false}. Therefore, we discuss two flavors of probabilistic query evaluation: computing expectations of answer tuple multiplicities, and computing the probability that a tuple is contained in the answer at most k times for some parameter k. Subject to mild technical assumptions on the representation systems, it turns out that expectations are easy to compute, even for unions of conjunctive queries. For query answer probabilities, we obtain a dichotomy between solvability in polynomial time and #P-hardness for self-join free conjunctive queries. © Martin Grohe, Peter Lindner, and Christoph Standke; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150727304&doi=10.4230%2fLIPIcs.ICDT.2023.20&partnerID=40&md5=7445abcbe811e72e409e9c040dc99d16,2022
Diversity of Answers to Conjunctive Queries,"Enumeration problems aim at outputting, without repetition, the set of solutions to a given problem instance. However, outputting the entire solution set may be prohibitively expensive if it is too big. In this case, outputting a small, sufficiently diverse subset of the solutions would be preferable. This leads to the Diverse-version of the original enumeration problem, where the goal is to achieve a certain level d of diversity by selecting k solutions. In this paper, we look at the Diverse-version of the query answering problem for Conjunctive Queries and extensions thereof. That is, we study the problem if it is possible to achieve a certain level d of diversity by selecting k answers to the given query and, in the positive case, to actually compute such k answers. © Timo Camillo Merkl, Reinhard Pichler, and Sebastian Skritek; licensed under Creative Commons License CC-BY 4.0 26th International Conference on Database Theory (ICDT 2023)",-,10.4230/LIPIcs.ICDT.2023.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150736584&doi=10.4230%2fLIPIcs.ICDT.2023.10&partnerID=40&md5=bbb183a7c3d3268c7ef894b483be27ae,2023
A new tree-based approach to mine sequential patterns[Formula presented],"Generic sequential pattern mining problem aims to mine the set of sequential patterns from a sequential database that satisfies a minimum support or occurrence threshold constraint. The main challenges that affect the efficiency of a solution lie in reducing the pattern search space, early detecting the infrequent patterns, representing the database in an efficient format, etc. Also, additional challenges get included when the problem environment transitions from static to incremental database leading to not to re-mine but efficiently tracking the effect of the incremental portion over the complete updated database. In this article, we introduce a new tree-based solution to the sequential pattern mining problem, including two sets of novel solutions for static and incremental sequential databases. We propose two new structures, SP-Tree and IncSP-Tree, and design two efficient algorithms, Tree-Miner and IncTree-Miner to mine the complete set of sequential patterns from static and incremental databases respectively. The proposed novel structures provide an efficient manner to store the complete sequential database maintaining “build-once-mine-many” property and giving scope to perform interactive mining. Additionally, we also design a new breath-first based support counting technique to efficiently identify the infrequent patterns at early stages and a new heuristic pruning strategy to reduce pattern search space. We also design a new pattern storage structure BPFSP-Tree to store the frequent patterns during successive iterations in incremental mining to reduce the number of database scans and to remove the infrequent patterns efficiently. A novel structure named Sequence Summarizer is also introduced to efficiently calculate and update the co-occurrence information of the items, especially in an incremental environment. Experimental results from various real-life and synthetic datasets demonstrate the efficiency of our work in comparison with the related state-of-the-art approaches. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.122754,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178662860&doi=10.1016%2fj.eswa.2023.122754&partnerID=40&md5=e2051f6eae46b5b790f66a15d4e7b5c7,2023
National Brazilian judicial database (DataJud): transparency and better administration of justice,"The use of data about judicial procedures is crucial to improve the overall administration of justice, define strategies for justice institutions, promote transparency, broaden access, and enhance confidence in the justice system. Nevertheless, data collected worldwide has been sparse and inconsistent as most countries have been late to start steadily adopting information technology in the Justice System. Brazil is an exception, as the country has been collecting digital judicial data systematically since 2009. Despite its enormous geographical and population dimensions and its heterogeneity of legal systems among the states, Brazil managed to develop and implement the National Database of the Brazilian Judiciary (DataJud) as a result of 15 years of studies and experience coordinated by the National Council of Justice (CNJ) and with the participation of the Brazilian Judiciary actors. The paper presents DataJud, the main previous events that favoured its creation, its characteristics, and two results and firsts benefit of the platform: Dashboard of Statistics of the Judiciary and Dashboard of Major Litigants.  © 2023 Owner/Author.",417-419,10.1145/3614321.3614381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180123097&doi=10.1145%2f3614321.3614381&partnerID=40&md5=bb3488525f90fe13a2418dcd8eca41a6,2024
Estimating hydrocarbon recovery factor at reservoir scale via machine learning: Database-dependent accuracy and reliability,"This study aims to estimate recovery factor (RF), a key property for exploration, from other reservoir characteristics, such as porosity, permeability, pressure, and water saturation via machine learning (ML). The database dependence of ML algorithms in the estimation of the hydrocarbon RF at the reservoir scale, however, has not yet been addressed. We, therefore, used various combinations of three databases and applied three regression-based models including the extreme gradient boosting (XGBoost), support vector machine (SVM), and stepwise multiple linear regression (MLR) to construct the ML models and estimate the oil and/or gas RF. Using two databases and the cross-validation method, we evaluated the performance of the ML models. The third independent database was then used to further assess the constructed models. We found that the XGBoost model estimated the oil and gas RF for the train and test datasets more accurately than the SVM and MLR models. In the estimation of oil RF and for the testing dataset in the largest database, we found RMSE = 0.111 for the XGBoost model, while RMSE = 0.130 and 0.134, respectively, for the SVM and MLR models. However, the performance of all the models were unsatisfactory for the independent databases. Results demonstrated that the ML algorithms were highly dependent and sensitive to the databases based on which they were trained. Statistical tests revealed that such unsatisfactory performances were because the distributions of input features and target variables in the train datasets were significantly different from those in the independent databases (p-value <0.05). © 2023",-,10.1016/j.engappai.2023.107500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177849955&doi=10.1016%2fj.engappai.2023.107500&partnerID=40&md5=389eb9b31e813376ad7479bd53540e82,2024
Drops in a bucket: Contributions of the IAEA Lise Meitner Library to the INIS database,"No library can have it all. It is the job of the Inter Library Loan/Document Delivery Service to get elsewhere what is not in a Library's collection. That is also the case in the IAEA Lise Meitner Library (ILML). Most of the topics the ILML deals with are nuclear related (albeit not only), and so are the ""known""or specific items requested to the ILL/DD Service. The ILML is therefore a heavy user of the INIS database, among other nuclear related databases. Many relevant documents are to be found there. In any case, it is a source that must be checked. Then again, it also happens that a given item is not in INIS. Realizing that, the ILML ILL/DD service took the initiative to report those ""missing drops""to the INIS proverbial bucket. (ILML and INIS are two different units within IAEA's Nuclear Information Section). One may wonder if a drop in a bucket is worth it. It is in this case because INIS, as other topical databases/repositories, aims for the wider possible coverage within its topical scope. Since 1972, records in INIS come from member states contributions, IAEA contributions, and yet, some documents escape the coverage, something learned by the experience of looking for requested materials like old conference papers, unpublished thesis, reports from nuclear agencies and institutions, etc. As a relatively cheap by-product of the search of an item, every month or so, a report in RIS format with items found elsewhere than INIS is sent to INIS, including basic bibliographic information, report ID, searches performed in INIS (not finding it), and location of the material: URL of a site, library catalogue, publisher, or agency. Occasionally, a reported item may lead to the inclusion of many others, a conference paper to the proceedings, a chapter to a book, a paper to a journal, etc. The reporting is easily replicable: any other library that stumbles upon an item not covered by INIS is welcome to inform about it. These are items that escaped prior systematic coverage, added to INIS because some user needed it, and ILL/DD located it elsewhere. Reporting to INIS is a way of compensating the relatively high cost of looking for a specific item, hopefully avoiding others looking for it again, but finding it in INIS.  © TextRelease, 2024.",90-98,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188330830&partnerID=40&md5=b58443bcbf97c379ec379a7d2fa6b411,2024
Two-Stage Encryption for Strengthening Data Security in Web-Based Databases: AES-256 and RSA Integration,"The research introduces a two-stage encryption approach, integrating AES-256 and RSA algorithms, to bolster the security of web-based databases. Focusing specifically on databases, the study's primary objective was to fortify data protection against SQL Injection attacks. Employing a PHP-native model with CRUD functions, the study meticulously implemented and tested the integration of AES-256 and RSA encryption and decryption procedures. The results showcased successful encryption of sensitive information, rendering the targeted columns impervious to unauthorized access. However, challenges surfaced, particularly concerning potential impacts on web response times, especially with extensive datasets. Performance concerns associated with creating public-private key pairs for each data storage operation were evident. Query performance, especially with encrypted attributes, posed significant hurdles. In the realm of further research, this study underscores the delicate equilibrium between security and efficiency in web-based database systems. Future investigations should scrutinize the cost implications and resource utilization in large-scale implementations. Optimizing the methodology's performance, elucidating processes executed once versus those repeated for each data storage action, and exploring the incorporation of indexes are vital avenues for exploration. Additionally, practical implications regarding the real-world implementation of encryption techniques demand in-depth analysis. Addressing these aspects will not only enhance data security but also guide the development of more efficient and practical web-based database protection strategies.  © 2023 IEEE.",486-492,10.1109/COMNETSAT59769.2023.10420796,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186124653&doi=10.1109%2fCOMNETSAT59769.2023.10420796&partnerID=40&md5=52b86a2b6c267f3c7e1d1bf7d9ee2d81,2022
CBIR-ACHS: compressed domain content-based image retrieval through auto-correloblock in HEVC standard,"Because the complete decompression of images encoded through the HEVC standard is time-consuming, image retrieval encoded through the HEVC standard is one of the most challenging topics in existing database management. Our proposed method is a content-based image retrieval method in the compressed domain, leveraging feature vectors constructed from the I-frame information encoded in HEVC. We introduce a novel concept called auto-correloblock to form these feature vectors. These feature vectors, which represent HEVC coded images in the compressed domain, are based on block sizes and intra-prediction modes. In our method, we utilize the feature vector of the query image to generate a histogram, which is then compared to the feature vectors of images stored in the database. To evaluate our approach, we conduct experiments using the INRIA HOLIDAY database. The results demonstrate the effectiveness of our proposed method, achieving an ANMRR (Average Normalized Modified Retrieval Rank) of 0.23 in coded image retrieval and outperforms its counterparts in this domain. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11042-024-18488-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185109605&doi=10.1007%2fs11042-024-18488-2&partnerID=40&md5=28382df7df1c3f3c6c83db01e8b852b0,2021
Abnormal Heart Sound Detection using Time-Frequency Analysis and Machine Learning Techniques,"Phonocardiogram (PCG) signals contains valuable information pertaining to heart valve functionality, rendering them potentially useful for early detection of cardiovascular diseases. Automated classification of heart sounds harbors great promise for identifying cardiac pathologies. This paper introduces a novel automated approach to classify normal and abnormal heart sounds. Our methodology involves partitioning heart sounds into four segments: S1, S2, systolic, and diastolic, followed by extraction of time–frequency and time-statistical features. Prior to data classification, we employ two techniques - particle Swarm optimization (PSO) and Sequential Forward Feature Selection (SFFS) - for efficient feature selection. We assess the performance of the proposed method on the Physionet Challenge 2016 database, utilizing the 10-fold cross-validation method. To address the issue of dataset imbalance, we apply the synthetic minority over-sampling technique (SMOTE) to create balanced datasets. Our approach surpasses existing methods in the literature, as evidenced by its superior accuracy, sensitivity, and specificity metrics. Specifically, our method achieves an accuracy of 98.03%, a sensitivity of 97.64%, and a specificity of 98.43% in distinguishing normal from abnormal heart sounds on the Physionet database. These findings outperform the results obtained by previously established methods evaluated on the Physionet 2016 challenge database. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105899,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182022887&doi=10.1016%2fj.bspc.2023.105899&partnerID=40&md5=270d6b54c140c734e967449920bfeec6,2021
Al–Ni–Ti thermodynamic database from first-principles calculations,"First-principles calculations have increasingly become an essential tool for providing additional thermodynamic data for assessing thermodynamic databases using the CALPHAD methodology. As computational power increases and becomes easily accessible, first-principles calculation results have become more presented along with the experimental procedures to determine the thermochemical properties of the phases within a target system. This can help accelerate the alloy development process, even in complex multi-component systems. By using only first-principles calculation data for the both thermodynamic description and interaction parameters of end-members, an Al–Ni–Ti ternary thermodynamic database is constructed. By relying on the existing materials database for ground state stability of the known compounds, the constructed Al–Ni–Ti thermodynamic database can mostly capture all the features related to the solid-state phases appear in the ternary phase diagram comparable to that of the published database from an experimental data assessment. © 2023 Elsevier Ltd",-,10.1016/j.calphad.2023.102658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181838295&doi=10.1016%2fj.calphad.2023.102658&partnerID=40&md5=bac60a8fc259593a8e4115882d4062de,2024
Evidence for the druggability of aldosterone targets in heart failure: A bioinformatics and data science-driven decision-making approach,"Background: Aldosterone plays a key role in the neurohormonal drive of heart failure. Systematic prioritization of drug targets using bioinformatics and database-driven decision-making can provide a competitive advantage in therapeutic R&D. This study investigated the evidence on the druggability of these aldosterone targets in heart failure. Methods: The target disease predictability of mineralocorticoid receptors (MR) and aldosterone synthase (AS) in cardiac failure was evaluated using Open Targets target-disease association scores. The Open Targets database collections were downloaded to MongoDB and queried according to the desired aggregation level, and the results were retrieved from the Europe PMC (data type: text mining), ChEMBL (data type: drugs), Open Targets Genetics Portal (data type: genetic associations), and IMPC (data type: genetic associations) databases. The target tractability of MR and AS in the cardiovascular system was investigated by computing activity scores in a curated ChEMBL database using supervised machine learning. Results: The medians of the association scores of the MR and AS groups were similar, indicating a comparable predictability of the target disease. The median of the MR activity scores group was significantly lower than that of AS, indicating that AS has higher target tractability than MR [Hodges-Lehmann difference 0.62 (95%CI 0.53–0.70, p < 0.0001]. The cumulative distributions of the overall multiplatform association scores of cardiac diseases with MR were considerably higher than with AS, indicating more advanced investigations on a wider range of disorders evaluated for MR (Kolmogorov-Smirnov D = 0.36, p = 0.0009). In curated ChEMBL, MR had a higher cumulative distribution of activity scores in experimental cardiovascular assays than AS (Kolmogorov-Smirnov D = 0.23, p < 0.0001). Documented clinical trials for MR in heart failures surfaced in database searches, none for AS. Conclusions: Although its clinical development has lagged behind that of MR, our findings indicate that AS is a promising therapeutic target for the treatment of cardiac failure. The multiplatform-integrated identification used in this study allowed us to comprehensively explore the available scientific evidence on MR and AS for heart failure therapy. © 2024 Elsevier Ltd",-,10.1016/j.compbiomed.2024.108124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186263536&doi=10.1016%2fj.compbiomed.2024.108124&partnerID=40&md5=f71937ffa8b5af7e0f10261227245cbd,2023
Measuring Meaning Similarity Using TF/IDF and Term Synonym ID,"This work proposes a novel alternative for measuring the meaning similarity between two sentences in Bahasa Indonesia. Two sentences are considered to have the same meaning if each word in those sentences has the same synonym. In this regard, a synonym database is needed. In this study, the synonym database was developed by categorizing words with the same meaning. Each word group was given a unique ID. The first scenario was performed without stemming, while the second was performed with stemming during the pre-processing. Each word in the sentence was matched to the synonym database using the synonym ID reference. The synonym ID was weighed using TF/IDF, and the vector distance was measured using Cosine similarity. The most optimal test result was noticed in the first scenario with a 89.72% similarity score out of 80 data tests, higher than the second scenario with an 76.24% similarity score.  © 2023 IEEE.",206-211,10.1109/ICOIACT59844.2023.10455894,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187803266&doi=10.1109%2fICOIACT59844.2023.10455894&partnerID=40&md5=3c427484dedd71bc66dbd593baf75fd7,2023
Insights into dietary phytochemicals targeting Parkinson's disease key genes and pathways: A network pharmacology approach,"Parkinson's disease (PD) is a complex neurological disease associated with the degeneration of dopaminergic neurons. Oxidative stress is a key player in instigating apoptosis in dopaminergic neurons. To improve the survival of neurons many dietary phytochemicals have gathered significant attention recently. Thus, the present study implements a comprehensive network pharmacology approach to unravel the mechanisms of action of dietary phytochemicals that benefit disease management. A literature search was performed to identify ligands (i.e., comprising dietary phytochemicals and Food and Drug Administration pre-approved PD drugs) in the PubMed database. Targets associated with selected ligands were extracted from the search tool for interactions of chemicals (STITCH) database. Then, the construction of a gene-gene interaction (GGI) network, analysis of hub-gene, functional and pathway enrichment, associated transcription factors, miRNAs, ligand-target interaction network, docking were performed using various bioinformatics tools together with molecular dynamics (MD) simulations. The database search resulted in 69 ligands and 144 unique targets. GGI and subsequent topological measures indicate histone acetyltransferase p300 (EP300), mitogen-activated protein kinase 1 (MAPK1) or extracellular signal-regulated kinase (ERK)2, and CREB-binding protein (CREBBP) as hub genes. Neurodegeneration, MAPK signaling, apoptosis, and zinc binding are key pathways and gene ontology terms. hsa-miR-5692a and SCNA gene-associated transcription factors interact with all the 3 hub genes. Ligand-target interaction (LTI) network analysis suggest rasagiline and baicalein as candidate ligands targeting MAPK1. Rasagiline and baicalein form stable complexes with the Y205, K330, and V173 residues of MAPK1. Computational molecular insights suggest that baicalein and rasagiline are promising preclinical candidates for PD management. © 2024",-,10.1016/j.compbiomed.2024.108195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186957894&doi=10.1016%2fj.compbiomed.2024.108195&partnerID=40&md5=792a9f6b0d0f321990f84634bb4f6cba,2022
Application of a new multi-binary classification strategy in the Weld Defects Images,"The conventional techniques for identifying flaws in welds lack accuracy. However, Non-Destructive Testing (NDT) is vital in assessing weld quality. This paper addresses the limitations of conventional weld defect detection methods by proposing an automatic detection method based on convolutional neural networks (CNNs). We enhance the Non-Destructive Testing (NDT) approach by incorporating artificial intelligence. They construct a database of X-ray images of pipeline welds, including sixty-seven patches of manually annotated weld defect images from the GDXray database. We utilize radiography images to classify weld defects into five distinct classes. We employ a multi-binary classification strategy (MBC) using the annotated database to detect and classify weld defects automatically. This novel approach offers improved accuracy and efficiency in weld defect detection. By combining NDT with artificial intelligence, specifically CNNs, we introduce a promising detection strategy for assessing weld quality. It demonstrates the potential for enhancing weld defect identification in various industries and applications.  © 2023 IEEE.",-,10.1109/IW_MSS59200.2023.10368597,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182939927&doi=10.1109%2fIW_MSS59200.2023.10368597&partnerID=40&md5=233fb0fe8ee26458394baa44a832cce4,2022
Exploiting Access Pattern Characteristics for Join Reordering,"With increasing main memory sizes, data processing has significantly shifted from secondary storage to main memory. However, choosing a good join order is still very important for efficient query execution in modern DBMS. This choice bases mainly on cardinality estimates for intermediate join results. However, the memory access pattern, e.g., sequential or random, on the intermediate state is an often neglected performance factor. In this paper, we examine this impact on join query performance by evaluating the execution time, and cache misses for n-ary foreign-key joins. Based on this analysis, we propose a novel join reordering algorithm that detects the memory access pattern (using machine learning on hardware performance counters) and adapts the join order accordingly at runtime. By considering the access pattern, our evaluation shows that our adaptive reorder algorithm converges quickly to a good join order and reaches improvements of up to a factor of 5.7×. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",10-18,10.1145/3592980.3595304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163711760&doi=10.1145%2f3592980.3595304&partnerID=40&md5=14bb70e9970496769a72f94ecc2b3f5d,2023
Why Your Experimental Results Might Be Wrong,"Research projects in the database community are often evaluated based on experimental results. A typical evaluation setup looks as follows: Multiple methods to compare with each other are embedded in a single shared benchmarking codebase. In this codebase, all methods execute an identical workload to collect the individual execution times. This seems reasonable: Since the only difference between individual test runs are the methods themselves, any observed time difference can be attributed to these methods. Also, such a benchmarking codebase can be used for gradual optimization: If one method runs slowly, its code can be optimized and re-evaluated. If its performance improves, this improvement can be attributed to the particular optimization. Unfortunately, we had to learn the hard way that it is not that simple. The reason for this lies in a component that sits right between our benchmarking codebase and the produced experimental results — the compiler. As we will see in the following case study, this black-box component has the power to completely ruin any meaningful comparison between methods, even if we setup our experiments as equal and fair as possible. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",94-97,10.1145/3592980.3595317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163748803&doi=10.1145%2f3592980.3595317&partnerID=40&md5=85f98bc5d5ad90339e33a45ba56ff2fc,2023
Satellite imagery retrieval based on adaptive Gaussian–Markov random field model with Bayes deep convolutional neural network,"This paper introduces a novel method for satellite colour imagery retrieval, based on an adaptive Gaussian–Markov random field (AGMRF) model with the Bayes-driven deep convolutional neural network (AGMRF–BDCNN). The given input imagery is segregated into the structure, microstructure, and texture components, and the AGMRF-driven features and statistical features are extracted from the segregated components and are formulated as a feature vector of the query imagery. Cosine direction and Bhattacharyya distance measures are deployed to match the feature vector with the feature vector of the feature vector database. If the query imagery features match the feature-vector database's features, then the reference imagery in the database is marked and indexed. The indexed imageries are retrieved. Three different benchmark data sets, SceneSat, PatternNet, and UC Merced, have been used to validate the proposed AGMRF–BDCNN method. For the SceneSat data set, the AGMRF–BDCNN method results in 0.2319 scores for ANMRR and 0.7156 scores for mAP; for the UC Merced data set, it yields 0.2316 scores for ANMRR and 0.7816 scores for mAP; for PatternNet data set, it achieves 0.2405 scores for ANMRR and 0.6979 scores for mAP. The obtained results are comparable to state-of-the-art methods. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",661-684,10.1007/s00500-023-09418-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180188827&doi=10.1007%2fs00500-023-09418-9&partnerID=40&md5=bd378a27809849bde8aa7610908ab93b,2022
A control architecture for fixed-wing aircraft based on the convolutional neural networks,"This paper develops a nonlinear architecture to control different fixed-wing aircraft. This architecture has inner and outer loops. The inner loops, designed based on the convolutional neural networks, control the internal dynamics of the aircraft, and the outer loops, which use linear controllers, are designed to control the kinematic states, which are the same for all aircraft. So, the inner loops are designed to have adaptation mechanisms based on the convolutional neural networks to control the internal dynamics of different aircraft. The networks are trained offline based on a generated database to avoid time-consuming online procedures. The database is created by simulating simple linear training models. Then, the input-output data of these training models are preprocessed and mapped to an appropriate form to be fed to convolutional neural networks. After that, an appropriate network structure is selected, and the networks are trained based on the mapped database. These trained networks, along with linear controllers in a cascade form, are applied to control nonlinear simulations of 15 different fixed-wing aircraft. Then, the performance of the represented controller is compared to two adaptive controllers. The promising results show that the controller can sufficiently be utilized in controlling fixed-wing aircraft, even in slow or sudden changes in aircraft dynamics. © 2024 The Franklin Institute",-,10.1016/j.jfranklin.2024.106664,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186666330&doi=10.1016%2fj.jfranklin.2024.106664&partnerID=40&md5=6735656e2332ca3450e12980b3a0a95d,2021
Prediction of Clients Based on Google Analytics Income Using Support Vector Machines,"This study aims to deploy Support Vector Machines (SVMs) to classify clients within the user base of the IMOLKO company website, predicated on the analysis of clickstream behavior. The study conducted several experiments using Monte Carlo cross-validation, encompassing diverse training and testing data proportions. Model performance was evaluated using parameters such as accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and F1 score. The results indicate that the SVMs consistently performs well across multiple runs, as evidenced by the low standard deviations associated with the evaluation metrics. It suggests that the results are reliable and not strongly influenced by random variations. The findings indicate that SVMs is an acceptable classification technique for predicting client status in the context of IMOLKO C.A. However, it is worth noting that although the model effectively predicts non-customers, the possibility of false positives exists, which reduces the percentage of F1 scores. The imbalance in the database, with a significantly higher number of non-clients compared to clients, may be impacting the method's efficiency. A balanced database, where each class has a similar number of examples, is desirable in classification tasks to avoid biases towards a dominant client class and ensure accurate decisions for all client classes. In conclusion, SVMs show promise as a reliable classification technique for predicting client status in the IMOLKO C.A. context. However, addressing database imbalance and conducting further research are imperative to enhance the performance of the models. © 2023 IEEE.",-,10.1109/C358072.2023.10436245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186767737&doi=10.1109%2fC358072.2023.10436245&partnerID=40&md5=712c6800190edbda9dee9521790ee6e5,2021
A Study of Subjective and Objective Quality Assessment of HDR Videos,"As compared to standard dynamic range (SDR) videos, high dynamic range (HDR) content is able to represent and display much wider and more accurate ranges of brightness and color, leading to more engaging and enjoyable visual experiences. HDR also implies increases in data volume, further challenging existing limits on bandwidth consumption and on the quality of delivered content. Perceptual quality models are used to monitor and control the compression of streamed SDR content. A similar strategy should be useful for HDR content, yet there has been limited work on building HDR video quality assessment (VQA) algorithms. One reason for this is a scarcity of high-quality HDR VQA databases representative of contemporary HDR standards. Towards filling this gap, we created the first publicly available HDR VQA database dedicated to HDR10 videos, called the Laboratory for Image and Video Engineering (LIVE) HDR Database. It comprises 310 videos from 31 distinct source sequences processed by ten different compression and resolution combinations, simulating bitrate ladders used by the streaming industry. We used this data to conduct a subjective quality study, gathering more than 20,000 human quality judgments under two different illumination conditions. To demonstrate the usefulness of this new psychometric data resource, we also designed a new framework for creating HDR quality sensitive features, using a nonlinear transform to emphasize distortions occurring in spatial portions of videos that are enhanced by HDR, e.g., having darker blacks and brighter whites. We apply this new method, which we call HDRMAX, to modify the widely-deployed Video Multimethod Assessment Fusion (VMAF) model. We show that VMAF+HDRMAX provides significantly elevated performance on both HDR and SDR videos, exceeding prior state-of-the-art model performance. The database is now accessible at: https://live.ece.utexas.edu/research/LIVEHDR/LIVEHDR_index.html. The model will be made available at a later date at: https://live.ece.utexas.edu//research/Quality/index_algorithms.htm.  © 1992-2012 IEEE.",42-57,10.1109/TIP.2023.3333217,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177998036&doi=10.1109%2fTIP.2023.3333217&partnerID=40&md5=5c8180dc6f9ec643d4b6790f24b2169d,2024
The construction of visual aesthetic element system in graphic design based on big data,"Various exquisite graphic designs that can be seen everywhere beautify people's vision and influence people's visual aesthetics subtly. This paper starts with the visual aesthetic elements of graphic design and discusses the construction of visual aesthetic element systems in graphic design by analyzing the composition elements and aesthetic characteristics. Secondly, it proposes to use the biological visual perceptual machine model to process information through two visual pathways in a hierarchical manner. The features of graphic design images are expressed, stored, and extracted using the visual information model. Finally, the effectiveness of the bio-visual perceptual model applied to the construction of visual aesthetic elements of graphic design is verified by the Caltech5 database, Caltech256 database, and Scene15 database. The results show that the classification accuracy of the bio-visual perceptron model is 94.28%, 91.36%, and 99.75% in Caltech5, Caltech256, and Scene15 databases, respectively, which is 23.12% higher than the accuracy of other neural network models on average. The bio-visual perceptron model proposed in this paper can accurately detect the elemental features in complex graphic design, which provides a new idea and method for rapid detection and recognition for graphic design workers. © 2023 Jing Shen et al., published by Sciendo.",-,10.2478/amns.2023.2.00315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175416124&doi=10.2478%2famns.2023.2.00315&partnerID=40&md5=0e4064ae36c59dfd41318f86bd7d2dd5,2021
Content-based medical image retrieval using deep learning-based features and hybrid meta-heuristic optimization,"Medical imaging is essential to the medical profession because it gives physicians access to crucial data on interior body structures for clinical analysis and treatment decisions that help them identify and cure a wide range of illnesses. A significant collection of medical photos has been created as a result of the rapid increase in medical diagnoses, yet it can be difficult to locate similar medical images within such a large database. This article describes a technique for deep learning-based convolutional neural network (CNN) -based Content-Based Medical Image Retrieval (CBMIR) to deal with this problem as well as Modified Cosine Similarity (MCS)-based matching. The aim of this approach is to enhance the accuracy and efficiency of CBMIR by utilizing the power of deep learning and advanced optimization techniques. The proposed model includes two major phases: (a) the training stage, and (b) the testing stage. In the training stage, the pre-processing, feature extraction, and optimal feature selection process take place. The database images are pre-processed using the Gaussian filter, Contrast Limited Adaptive Histogram Equalization (CLAHE), and Gaussian smoothing. Then, the deep features of database images are extracted using the Inception V3 CNN model and VGG19, respectively. The extracted features are combined, and the optimal features are selected from them. This selection is done through the new Coyote-Moth Optimization Algorithm (CMOA). This CMOA model is the conceptual amalgamation of the standard Moth-flame optimization (MFO) and coyote optimization Algorithm (COA). © 2024",-,10.1016/j.bspc.2024.106069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185718639&doi=10.1016%2fj.bspc.2024.106069&partnerID=40&md5=353033536b6c33f79952fd7c415cc679,2022
Batch-Less Stochastic Gradient Descent for Compressive Learning of Deep Regularization for Image Denoising,"We consider the problem of denoising with the help of prior information taken from a database of clean signals or images. Denoising with variational methods is very efficient if a regularizer well-adapted to the nature of the data is available. Thanks to the maximum a posteriori Bayesian framework, such regularizer can be systematically linked with the distribution of the data. With deep neural networks (DNN), complex distributions can be recovered from a large training database. To reduce the computational burden of this task, we adapt the compressive learning framework to the learning of regularizers parametrized by DNN. We propose two variants of stochastic gradient descent (SGD) for the recovery of deep regularization parameters from a heavily compressed database. These algorithms outperform the initially proposed method that was limited to low-dimensional signals, each iteration using information from the whole database. They also benefit from classical SGD convergence guarantees. Thanks to these improvements we show that this method can be applied for patch-based image denoising. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s10851-024-01178-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187639847&doi=10.1007%2fs10851-024-01178-x&partnerID=40&md5=dd473a644fc11397b896d338b876fa68,2021
Uncovering CWE-CVE-CPE Relations with Threat Knowledge Graphs,"Security assessment relies on public information about products, vulnerabilities, and weaknesses. So far, databases in these categories have rarely been analyzed in combination. Yet, doing so could help predict unreported vulnerabilities and identify common threat patterns. In this article, we propose a methodology for producing and optimizing a knowledge graph that aggregates knowledge from common threat databases (CVE, CWE, and CPE). We apply the threat knowledge graph to predict associations between threat databases, specifically between products, vulnerabilities, and weaknesses. We evaluate the prediction performance both in closed world with associations from the knowledge graph and in open world with associations revealed afterward. Using rank-based metrics (i.e., Mean Rank, Mean Reciprocal Rank, and Hits@N scores), we demonstrate the ability of the threat knowledge graph to uncover many associations that are currently unknown but will be revealed in the future, which remains useful over different time periods. We propose approaches to optimize the knowledge graph and show that they indeed help in further uncovering associations. We have made the artifacts of our work publicly available.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",-,10.1145/3641819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185218585&doi=10.1145%2f3641819&partnerID=40&md5=d23831b871aa43d01d4740d514f13f7d,2024
Development of Structure of the Users’ Motivational Intentions Database of the Web-Community for Supply and Demand of Educational Services and Evaluation of Its Information Content Quality,"The aim of this article is to develop the structure of the users’ motivational intentions database of the web community for supply and demand of educational services and evaluation the quality of its information content. This makes it possible to effectively communicate and satisfaction the needs of users who are looking for a necessary training course or offer their services in acquiring the necessary knowledge, skills and abilities. The article establishes that the keywords of thematic posts that define the relevant thematic sections of the web community for supply and demand of educational services coincide with the motivational intentions of its users. In accordance with this, the structure of the database includes such relations as ""Indication MI"", ""Motivational Intentions"", ""EduSection"", ""Keywords"". The development of an indicator of the importance of keywords in thematic posts of specialized community’s users, which evaluates the keywords in the post and reduces the importance of commonly used words, is the task of this paper. The article establishes that the content of the users' motivational intentions database is an information product. That is why the paper applies metrics according to the ISO/IEC-25010 standard to evaluate the quality of the information content of the users’ motivational intentions database of the web community for supply and demand of educational services. The analysis of metrics that correspond to the characteristics/sub-characteristics of software quality according to the ISO/IEC-25010 standard shows that such characteristics/sub-characteristics as Functional suitability (Accuracy) and Operability (Helpfulness) are suitable for evaluating the quality of information content of the web community for supply and demand of educational services. The article contains an analysis of the quality of information content of the specialized web community for supply and demand according to the proposed metrics. The results of the study are used and can be used for effective moderation of specialized web communities for supply and demand of educational services, as well as facilitate communication between their participants. © 2023 CEUR-WS. All rights reserved.",345-354,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182936450&partnerID=40&md5=760a10646803053e693b175fcf000667,2022
Database for storing and accessing diabetes related data in a standardized way,"Real-world data has a major importance in diabetes related research, especially considering the widespread applications of machine learning algorithms. There are several existing datasets of real-world data in the literature; however, they all have their specific formats, applied devices and file structures. The different charachteristics make it cumbersome to use multiple datasets for research purposes. We developed a pipeline for efficiently storing and accessing diabetes related data in a standardized way. We defined a standardized JSON format for the records and stored them in a MongoDB database. The pipeline is capable of uploading data from various sources by implementing a corresponding transformer script; also it is possible to extend it with algorithms which are called automatically for new entries in the database. The source code for constructing the pipeline is given in https://github.com/NeuroDiab/CINTI2023Diabdatabase.  © 2023 IEEE.",285-290,10.1109/CINTI59972.2023.10381957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183867838&doi=10.1109%2fCINTI59972.2023.10381957&partnerID=40&md5=92d2a242f58595c6509487f34c2d562f,2023
Low-Cost Remote Monitoring Solutions for Sustainable Water Management in Madagascar,This paper describes a low-cost automated approach for logging data from remote sites in Madagascar directly to a cloud-based repository where it can be accessed by end-users through an online dashboard. SMS alerts can also be triggered to warn operators of conditions that require immediate attention. The system integrates commercial sensors and telemetry with alternative cellular providers and custom back-end processing to automatically format and upload data to the database. The database itself conforms to the Observations Data Model standard. Early results and interpretation of recorded data are provided as an example.  © 2023 IEEE.,414-418,10.1109/GHTC56179.2023.10354568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182744377&doi=10.1109%2fGHTC56179.2023.10354568&partnerID=40&md5=8e02c9bd84e8d9e19051e087119b280d,2023
A Clustering-Based Computational Model to Group Students With Similar Programming Skills From Automatic Source Code Analysis Using Novel Features,"Throughout a programming course, students develop various source code tasks. Using these tasks to track students' progress can provide clues to the strengths and weaknesses found in each learning topic. This practice allows the teacher to intervene in learning in the first few weeks of class and maximize student gains. However, the biggest challenge is to overcome the amount of work required of the teacher in the manual analysis of all tasks. In this context, our main research objective is to automatically group students with similar programming skills based on the analysis of their submitted source codes. Our research is applied and uses an experimental procedure. First, we prepared the database, with more than 700 real-world source code tasks written in C language, and distributed it in five different learning topics. Afterward, we define a set of features to be extracted from each learning topic. We defined and extracted 23 features from the source code for five learning topics. Then, we preprocess our database and extract the proposed features. Finally, we grouped the students. After performing the grouping, we obtained four groups of students, which were analyzed using a cluster midpoint calculation. Our results support the monitoring of students throughout the term, offering the teacher the freedom to create new exercises and waiving the obligation of any specific programming environment. We believe that these results can support the teacher in pedagogical decisions closer to the needs of each group of students. © 2008-2011 IEEE.",428-444,10.1109/TLT.2023.3273926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159842898&doi=10.1109%2fTLT.2023.3273926&partnerID=40&md5=5ee10953b1abdcbc5a5581f29e4f9c8f,2021
Load Balancing Beam Selection Alogorithm using Fingerprint DB in UAV Supported Cellular Systems,"Unmanned Aerial Vehicle (UAV) is attracting attention as a key element of the New Radio (NR) system as it can exceed the limits of the ground network to prepare for the rapidly increasing data usage and create new application services. However, high-altitude UAVs have a high Line-Of-Sight (LOS) probability, resulting in interference problems between adjacent cells in multi-cell networks. Therefore, this paper proposes an algorithm that selects the optimal beam to reduce the impact of interference and ensure high transmission efficiency. Specifically, the proposed algorithm consists of the process of building a Fingerprint Database (fingerprint database) and the process of selecting a load balancing beam. Simulations were conducted based on the downlink system for performance analysis, and signal-to-interference-plus-noise cumulative distribution function (SINR CDF) is used as performance analysis indicators.  © 2024 IEEE.",-,10.1109/ICEIC61013.2024.10457244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189244647&doi=10.1109%2fICEIC61013.2024.10457244&partnerID=40&md5=4a86ce6a0f4bf87fa1496c8057ae4712,2021
Power Spectral Analysis of Heart Rate Variability using Compressed ECG Sensing for Energy-Constrained Fast Health Monitoring,"Heart rate variability (HRV) denotes the natural variation in the time gaps between the consecutive heartbeats due to physiological changes. The HRV analysis is useful in the diagnosis of different clinical and functional conditions. Computation of HRV parameters from high-resolution electrocardiogram (ECG) signals on resource-constrained wearable internet of things (IoT) devices poses challenges due to the substantial computational load and increased energy consumption. Compressed ECG sensing for R-peak detection emerges as an approach for fast processing. Frequency domain HRV (FDHRV) analysis based on the power spectrum is capable for quantifying various clinical and functional conditions through the examination of ultra-short-term (1 minute) segments of ECG data. This paper presents a comparative study of FDHRV analysis using compressed ECG sensing for R-peak detection and FDHRV analysis through conventional high resolution R-peak detection. For this comparative study, 48 ECG records of MIT-BIH arrhythmia database and 18 ECG records of MIT-BIH normal sinus rhythm (NSR) database are used. During comparative FDHRV analysis, we have observed an average deviation of 0.003 Hz in respective band-wise peak frequencies. For both databases, the observed peak-frequency locations are 99.25% and 99.23% similar, respectively. Also, compared methods have provided relative power with 99.64% and 94.68% similar values for both databases, respectively. These findings support the suitability of compressed ECG sensing based FDHRV analysis for fast health monitoring using an ultra-short duration ECG signal.  © 2023 IEEE.",6-11,10.1109/ICSIMA59853.2023.10373533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183462750&doi=10.1109%2fICSIMA59853.2023.10373533&partnerID=40&md5=0841399819f29ea4c86ed4d125dda808,2023
Deep Learning-Based Medical Image Classification Segmented with Particle Swarm Optimization Technique,"Medical image segmentation and classification is a prior task in medical image analysis. Therefore, a generalized method is required to process these images efficiently. Objective of our work is to use a nature inspired metaheuristic algorithm, particle swarm optimization (PSO). The algorithms is applied on three types of databases available in repositories. These databases include breast cancer ultrasound, chest-X- Rays, and carotid ultrasound images. After performing PSO clustering, medical images are classified into binary class using state of the art deep learning models VGG16, GoogleNet and ResNet50 models. Out of these DL models GoogleNet performs better classification performance for all types of database. The PSO-based approach effectively produces distinct ROI clusters for cluster 3, while the cluster 4-based method highlights variations in ROI tissue density. In summary, the PSO-based method stands out as a robust tool for delineating regions of interest in medical images. © 2023 IEEE.",617-621,10.1109/OCIT59427.2023.10430516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186651488&doi=10.1109%2fOCIT59427.2023.10430516&partnerID=40&md5=2bfce1e06fba32ab0b9f80d5f7b2c235,2023
Personalized Career-Path Recommendation Model for Information Technology Students in Indonesia,"One of the challenging decisions for students is taking a job specialization. To make their decisions, they use subjective perceptions of friends or family due to the lack of guidance and limited resources. This increases the risk of dissatisfaction with the work environments. To address these drawbacks, this study presents a personalized career-path recommendation model (CPRM) to provide guidance and help college students choose information technology jobs. The design of the CPRM is based on the personalized Na&#x00EF;ve Bayes (p-NB) algorithm with three primary sources: job profiles, personality types, and subjects. The association between personality type and college students was established using samples of 104 computer science students enrolled in private universities in Indonesia. CPRM was implemented as a web-based application. This study evaluated the model by measuring the quality of the recommended items to determine whether the proposed model is well accepted by users. The model considers educational data mining grounded theory (EDM-GT) data integration and hierarchically related concepts. CPRM has been validated by Information Technology (IT) professionals and three psychologists in Indonesia through focus group discussions. The evaluation results showed that more than 83% of respondents were satisfied with the recommendation model. Hence, CPRM can provide automatic academic advisors and guidance to computer science students interested in pursuing careers in IT jobs. The result shows that CPRM is the first career path recommendation model based on EDM-GT to target the computer science community in Indonesia. Authors",1-1,10.1109/ACCESS.2024.3381032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188899134&doi=10.1109%2fACCESS.2024.3381032&partnerID=40&md5=0d151fc1ddd8398d3884ac1858cacc72,2022
Website Monitoring for Disaster Victim Search and Rescue,"The disaster area can cover a very large area so that the process of finding and gathering information regarding disaster victims will be difficult or the process will run slowly. One solution to overcome these problems is to utilize the use of monitoring websites to assist the process of gathering information regarding disaster victims in the disaster area. The monitoring website is able to present the data that has been collected through the sensor system in the disaster area properly. This research was conducted with the aim of producing a monitoring website that can be used to display data on the location of victims in a disaster area. Several studies were carried out to consider how communication techniques were used and how the design of the monitoring website would be built. In this research, the design of the monitoring system is discussed starting from the data collection techniques carried out by the sensor system in the field, the technique of sending data from the sensor system to the database and the technique of presenting information that has been successfully stored in the database into the monitoring website. After the monitoring system design has been successfully implemented, testing is carried out by experimenting with sending data that has been obtained by the sensor system to the database, then the data is continued to be displayed on the monitoring website. After the experiment was carried out, the final results were obtained where in the database there were two tables, each of which served to store sensor system monitoring data and victim position data as a result of processing from the sensor system. In this research the data is stored in the MySQL database. The results of the monitoring website design have succeeded in displaying information on the detection position of the sensor system and the position of the victim on a digital map, then the victim's position is also displayed in the form of a list containing the coordinates of the victim in latitude and longitude.  © 2023 IEEE.",-,10.1109/ICoSNIKOM60230.2023.10364398,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182924806&doi=10.1109%2fICoSNIKOM60230.2023.10364398&partnerID=40&md5=01927f4e2f40085f6ddacd28d39d13f6,2023
Enhancing image-based facial expression recognition through muscle activation-based facial feature extraction,"This article introduces a non-intrusive method to estimate facial muscle activity from images, diverging from conventional electrode-based approaches. Our methodology capitalizes on an inclusive set of features encompassing a diverse range of facial muscles, often overlooked in research, thus significantly expanding the scope of analyzing muscle activity within facial expressions. Our method is based on the standard 68-point face landmark and extends it by identifying the interactions of these muscles when a person performs a specific facial expression. These interactions are recorded in a feature vector, which is used by three classifiers, Linear Discriminant (LD), Support Vector Machine (SVM) and Multi-layer Perceptron (MLP) to classify six facial expressions (anger, disgust, fear, happiness, neutrality, and sadness). The method's validation is conducted with three databases: FACES, KDEF, and JAFFE. These databases present different challenges; the first contains faces of individuals from three age ranges and both genders displaying facial expressions. On one hand, the KDEF database contains images of both genders but only within the range of young adults, whereas JAFFE comprises solely female faces. Additionally, JAFFE presents another drawback, representing only 10% of the images contained in FACES. In all these cases, our method yields excellent results, occasionally achieving 100% classification, especially in the young category. It is also noteworthy that the best results were obtained in the classification of facial expressions in female faces, suggesting that women tend to be more expressive. The method was thoroughly evaluated, and the results demonstrate the robustness of the approach, showcasing its good performance across three different databases and also for faces of individuals across various age groups. Numerical metrics, including accuracy, precision, recall, and F1 score, are presented along with confusion matrices. © 2024 Elsevier Inc.",-,10.1016/j.cviu.2024.103927,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182406878&doi=10.1016%2fj.cviu.2024.103927&partnerID=40&md5=7b75bc6a14258e481cf31635e2fadb53,2023
Hardening Password-Based Credential Databases,"We propose a protection mechanism for password-based credential databases maintained by service providers against leakage, dubbed PCDL. In PCDL, each authentication credential is derived from a user's password and a salt, where a service provider employs a set of key servers to share the salt in a threshold way. With PCDL, an external adversary cannot derive any information about the underlying passwords from a compromised credential database, even if he can compromise some of the key servers. The most prominent manifestation of PCDL is transparency: integrating PCDL with existing password-based authentication schemes does not require users to perform any additional operation (and thereby does not change users' interaction patterns), yet enhances the security guarantee significantly. PCDL serves as an independent component only deployed on the service provider side to harden the credential database. As such, PCDL is well compatible with existing password-based authentication schemes. We analyze the security of PCDL and conduct a performance evaluation, which shows that PCDL is secure and efficient.  © 2023 IEEE.",469-484,10.1109/TIFS.2023.3324326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174833785&doi=10.1109%2fTIFS.2023.3324326&partnerID=40&md5=0d40dbb98ca8399f42fff399cb40e547,2021
Identifying biases in a multicenter MRI database for Parkinson&#x0027;s disease classification: Is the disease classifier a secret site classifier&#x003F;,"Sharing multicenter imaging datasets can be advantageous to increase data diversity and size but may lead to spurious correlations between site-related biological and non-biological image features and target labels, which machine learning (ML) models may exploit as shortcuts. To date, studies analyzing how and if deep learning models may use such effects as a shortcut are scarce. Thus, the aim of this work was to investigate if site-related effects are encoded in the feature space of an established deep learning model designed for Parkinson&#x0027;s disease (PD) classification based on T1-weighted MRI datasets. Therefore, all layers of the PD classifier were frozen, except for the last layer of the network, which was replaced by a linear layer that was exclusively re-trained to predict three potential bias types (biological sex, scanner type, and originating site). Our findings based on a large database consisting of 1880 MRI scans collected across 41 centers show that the feature space of the established PD model (74&#x0025; accuracy) can be used to classify sex (75&#x0025; accuracy), scanner type (79&#x0025; accuracy), and site location (71&#x0025; accuracy) with high accuracies despite this information never being explicitly provided to the PD model during original training. Overall, the results of this study suggest that trained image-based classifiers may use unwanted shortcuts that are not meaningful for the actual clinical task at hand. This finding may explain why many image-based deep learning models do not perform well when applied to data from centers not contributing to the training set Authors",1-8,10.1109/JBHI.2024.3352513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182375298&doi=10.1109%2fJBHI.2024.3352513&partnerID=40&md5=378479406ba97790dfe526b457b9087f,2021
A residual utility-based concept for high-utility itemset mining,"Knowledge discovery in databases aims at finding useful information for decision-making. The problem of high-utility itemset mining (HUIM) has specifically garnered huge research attention, as it aims to find relevant information on patterns in a database, which conform to a user-defined utility function. The mined patterns are used for making data-backed decisions in the fields of healthcare, e-commerce, web analytics, etc. Various algorithms exist in the literature related to mining the high-utility items from the databases; however, most of them require multiple database scans, or deploy complex data structures. The utility-list is an efficient list-based data structure that is being widely adopted in the design of HUIM algorithms. The existing utility-list-based algorithms, however, suffer from some drawbacks like extensive use of inefficient join operations, multiple definitions of join operations, etc. Though the HUIM is an important research area, yet very little research has been directed towards improving the design of data structures used for the mining process. In this paper, we introduce the concept of residual utility to design two new data structures, called residue-map and master-map. Using these two data structures, a new algorithm, called R-Miner, is introduced for mining the high-utility items. In order to further optimise the mining process, the cumulative utility value is used as an upper bound and additional pruning conditions are also discussed. Several experiments are carried out on both real and synthetic datasets to compare the performance of R-Miner with the existing list-based algorithms. The experimental results show that the R-Miner improves the performance by up to the order of 2 as compared to the list-based algorithms: EFIM, H-Miner, HUI-Miner, FHM, and ULB-Miner. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",211-235,10.1007/s10115-023-01948-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168620177&doi=10.1007%2fs10115-023-01948-w&partnerID=40&md5=7cc40ed68a1f6e66e6247886eac8b6a8,2021
Performance evaluation of convolution neural network models for detection of abnormal and ventricular ectopic beat cardiac episodes,"The fast and accurate detection of abnormal cardiac episodes is essential for quick diagnosis high-risk patients prone to irregular cardiac arrhythmias. Although various algorithms have been proposed in the literature, there is a need to develop artificial intelligence driven computer aided intelligent models with high detection rate. This specific study focuses on the application of Convolution Neural Network (CNN) approach to classify abnormal and ventricular ectopic beat (VEB) episodes from normal ECG signals. A cross-database approach comprised of open source and a proprietary database has been deployed for training–testing strategies. Single-channel ECG time series was converted into a derivative image stack before applying it to CNN. The current study has proposed extracting derivative image features using a pretrained network and classification using the SVM classifier. The proposed study was tested on 18 pretrained networks such as Resnet18, Resnet50, Resnet101, Densnet201, InceptionV3, Googlenet, Squeeznet, Vgg19, Vgg16, Alexnet, InceptionResnetV2, Shufflenet, MobilenetV2, NASnet-Mobile, Darknet-19, Draknet-53, Xception, and NASnet-Large for optimization. Among the chosen pretrained models, Mobilenetv2 has shown the highest classification accuracy of 92.73% & 99.29%, with relative performance of 1.24 & 0.95 for the normal-abnormal and normal-VEB cardiac episode classifications of opensource datasets. The highest accuracy of 73.66% (using xception) and 99.92% (using Inceptionv3) was achieved for abnormal and VEB cardiac episodes detection from proprietary datasets. In addition, the proposed method showed the highest classification accuracy of 90.14% with Inceptionv3 and 99.34% with darknet19 using the proprietary database for normal-abnormal & N-V cardiac episode classification. Further, inceptionv3 & darknet19 trained models using the proprietary database showed the highest detection rate of 81.67% & 94.66% for abnormal & ventricular ectopic beat cardiac episodes of the open-source database. The study has concluded that the abnormal and VEB cardiac episode detection using CNN-based models could help diagnose and treat cardiac patients. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",-,10.1007/s11042-023-17997-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182215095&doi=10.1007%2fs11042-023-17997-w&partnerID=40&md5=e10bb1f4d1b6b344b6186c3f764a8fa2,2020
Detection of COVID-19 Using Convolutional Neural Networks,"A highly contagious illness caused by the COVID-19 pandemic is proven to havoc with people's health and well-being all over the globe. Chest radiography is one of the most crucial databases for applying detection techniques. The respiratory system is contaminated by COVID-19, which also affects the alveoli and replicates itself. Conventional approaches such as RT - PCR tests, rapid antigen tests, serological tests, etc. are generally used for the detection of COVID came out to be costly and time-consuming. There have been several suggested artificial intelligence (AI)-based models for the detection of COVID-19 in contaminated individuals' using lung ultrasound images, voice patterns, chest sounds, etc. The proposed model shows how disease cases could be identified using features and variations in chest radiography images. A deep convolutional neural network (CNN) with ResNet50 modified configuration has been used for the identification of COVID-19 cases from the chest radiography image dataset. The suggested model also depicts the comparison of the proposed technique with the Resnet 101 model. Chest radiography dataset containing Covid infected, normal, and pneumonia-infected images. Additionally, the suggested model can identify covid-19 patients' current conditions in real-time by identifying coronavirus diseases from CT scan pictures. The database has the ability to monitor detected patients and keep their databases in order to improve the training and model's accuracy. The proposed model provides approximately 96.73% of accuracy and shows explicit competency with ResNet 101 and other existing models.  © 2023 IEEE.",-,10.1109/CISCT57197.2023.10351356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182938144&doi=10.1109%2fCISCT57197.2023.10351356&partnerID=40&md5=0d53fc7e43667bbbe4dd0358df5f497f,2023
Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation,"Analyzing keystroke dynamics (KD) for biometric verification has several advantages: it is among the most discriminative behavioral traits; keyboards are among the most common human-computer interfaces, being the primary means for users to enter textual data; its acquisition does not require additional hardware, and its processing is relatively lightweight; and it allows for transparently recognizing subjects. However, the heterogeneity of experimental protocols and metrics, and the limited size of the databases adopted in the literature impede direct comparisons between different systems, thus representing an obstacle in the advancement of keystroke biometrics. To alleviate this aspect, we present a new experimental framework to benchmark KD-based biometric verification performance and fairness based on tweet -long sequences of variable transcript text from over 185,000 subjects, acquired through desktop and mobile keyboards, extracted from the Aalto Keystroke Databases. The framework runs on CodaLab in the form of the Keystroke Verification Challenge (KVC). Moreover, we also introduce a novel fairness metric, the Skewed Impostor Ratio (SIR), to capture inter - and intra -demographic group bias patterns in the verification scores. We demonstrate the usefulness of the proposed framework by employing two state-of-the-art keystroke verification systems, TypeNet and TypeFormer, to compare different sets of input features, achieving a less privacy-invasive system, by discarding the analysis of text content (ASCII codes of the keys pressed) in favor of extended features in the time domain. Our experiments show that this approach allows to maintain satisfactory performance.  © 2023 The Authors.",1102-1116,10.1109/ACCESS.2023.3345452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181580552&doi=10.1109%2fACCESS.2023.3345452&partnerID=40&md5=d10c481c15036580f920e0f4b91f722b,2023
Splendor: Static Detection of Stored XSS in Modern Web Applications,"In modern websites, stored Cross-Site Scripting (XSS) is the most dangerous XSS vulnerability, which can store payloads in the web system and be triggered directly by the victim. Database (DB) as the most commonly used storage medium for data on websites is therefore also the most common place where stored XSS occurs. Due to the modularity of modern programming architectures, the complex underlying database operations will often be encapsulated and abstracted as a Data Access Layer (DAL) to provide unified data access services to the business layer. The heavy use of Object-Oriented (OO) and dynamic language features involved in the encapsulation makes it increasingly challenging for static taint analysis tools to understand how tainted data flows between the source code and the exact locations in database. In this paper, we propose the first static analysis framework for detecting stored XSS in modern web applications using DAL and implement a prototype Splendor for PHP code analysis. The highlight in the framework is the design of a heuristic but precise token-matching method to locate the flows of taint data between database and source code. The precisions of the identified DB read and write (R/W) locations are 91.3% and 82.6%, respectively. With the identified R/W locations, the disconnected taint paths can be statically stitched to obtain a complete taint propagation path of stored XSS. Comparisons with existing works on 5 real-world applications and large-scale experiments on PHP web applications in Github show that Splendor significantly outperforms both the state-of-the-art static and dynamic approaches on stored-XSS detection, and detects 17 zero-day vulnerabilities.  © 2023 ACM.",1043-1054,10.1145/3597926.3598116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167668003&doi=10.1145%2f3597926.3598116&partnerID=40&md5=e6bca5a6b6af1355557bd52b61edd759,2021
Going the Extra Mile in Face Image Quality Assessment: A Novel Database and Model,"An accurate computational model for image quality assessment (IQA) benefits many vision applications, such as image filtering, image processing, and image generation. Although the study of face images is an important subfield in computer vision research, the lack of face IQA data and models limits the precision of current IQA metrics on face image processing tasks such as face superresolution, face enhancement, and face editing. To narrow this gap, in this article, we first introduce the largest annotated IQA database developed to date, which contains 20,000 human faces - an order of magnitude larger than all existing rated datasets of faces - of diverse individuals in highly varied circumstances. Based on the database, we further propose a novel deep learning model to accurately predict face image quality, which, for the first time, explores the use of generative priors for IQA. By taking advantage of rich statistics encoded in well pretrained off-the-shelf generative models, we obtain generative prior information and use it as latent references to facilitate blind IQA. The experimental results demonstrate both the value of the proposed dataset for face IQA and the superior performance of the proposed model.  © 1999-2012 IEEE.",2671-2685,10.1109/TMM.2023.3301276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166766844&doi=10.1109%2fTMM.2023.3301276&partnerID=40&md5=58a205627469ace9a55e0aaeb54956e5,2021
Learning Autoregressive Model in LSM-Tree based Store,"Database-native machine learning operators are highly desired for efficient I/O and computation costs. While most existing machine learning algorithms assume the time series data fully available and readily ordered by timestamps, it is not the case in practice. Commodity time series databases store the data in pages with possibly overlapping time ranges, known as LSM-Tree based storage. Data points in a page could be incomplete, owing to either missing values or out-of-order arrivals, which may be inserted by the imputed or delayed points in the following pages. Likewise, data points in a page could also be updated by others in another page, for dirty data repairing or re-transmission. A straightforward idea is thus to first merge and order the data points by timestamps, and then apply the existing learning algorithms. It is not only costly in I/O but also prevents pre-computation of model learning. In this paper, we propose to offline learn the AR models locally in each page on incomplete data, and online aggregate the stored models in different pages with the consideration of the aforesaid inserted and updated data points. Remarkably, the proposed method has been deployed and included as a function in an open source time series database, Apache IoTDB. Extensive experiments in the system demonstrate that our proposal LSMAR shows up to one order-of-magnitude improvement in learning time cost. It needs only about 10s of milliseconds for learning over 1 million data points.  © 2023 Owner/Author.",2061-2071,10.1145/3580305.3599405,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171367024&doi=10.1145%2f3580305.3599405&partnerID=40&md5=dc49ca2a2ab916597d4414fefdd68c24,2021
ECenterDet: An Efficient Anchor-free Detector,"Current networks rely on custom operators to achieve high performance, but these new operators are difficult to obtain hardware acceleration quickly. Anchor-based object detection algorithms rely on a large number of anchors, and the post-processing of these anchors also limits the speed and performance of the detection task. In this work, we propose an efficient deployment-friendly detector, and name it as ECenterDet. Our proposed model uses an anchor-free paradigm and treats the detection problem as a key point estimation problem, the backbone comes equipped with a efficient cross stage partial network(ECSPNet), an adaptive feature pyramid (AFP) with both high speed and accuracy. We try to improve the training efficiency by using all pixel points in the whole Gaussian region as training samples, and further optimize the loss calculation in order to balance the number of objects of different sizes, which can significantly improve the performance of the model without bringing additional computation and parameters. In addition, we develop the s/m/l/x model for different application scenarios. The proposed ECenterDet shows very competitive performance compared with existing methods, and our ECenterDet-x achieves 51.6% mAP on the MS COCO dataset, outperforming YOLOv4-Tiny and YOLOv5s with faster inference speed. © 2023 Copyright held by the owner/author(s).",793-799,10.1145/3594315.3594407,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168250509&doi=10.1145%2f3594315.3594407&partnerID=40&md5=93ff96b65a7d5a966f61a359fedc7831,2023
Design and Implementation of Cloud Computing-Based API for Mobile Application Tookar,"The mobile application Tookar serves as a barter platform that enables users to exchange goods with one another. While the application holds significant potential, it faces challenges in terms of scalability and efficiency. To enhance its performance, this integrates Cloud Computing technology, specifically Vertex AI for Image Classification utilizing AutoML models, and App Engine for deploying the API to the backend database. The design and implementation include the Backend Database API and Image Classification features within the Tookar mobile application. Additionally, Google Cloud Platform, as a Platform as a Service, is utilized for Deployment and Monitoring, involving model evaluation and latency measurements. The main outcomes include test parameters such as Average Prediction, Precision, Recall, Confusion Matrix, Response Latency, and Prediction Latency. Testing is conducted using Cloud Logging and Model Evaluate from Vertex AI. The results obtained using the Google Cloud Platform server include a Highest Response Latency is only 9,719 ms, while highest Prediction Latency only 400 ms, an Average Prediction ranging from 0 to 1 at 0.891, precision at 85.5%, recall at 84.1%, and the highest value in the Confusion Matrix being only 19%. The utilization of Cloud Computing services from the Google Cloud Platform positively contributes to addressing scalability and efficiency challenges in the design and implementation of the API in the Tookar application. © 2023 IEEE.",490-495,10.1109/ICICyTA60173.2023.10428947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186748994&doi=10.1109%2fICICyTA60173.2023.10428947&partnerID=40&md5=4b5dd4309d0c6945c047d5fe9a6290e6,2023
SQL queries over encrypted databases: a survey,"Limited by the local storage resource, data users have to encrypt their data and outsource the encrypted databases to cloud servers to enjoy low-cost, professional data management services, which promotes the rapid development of outsourcing database technology. Despite this, the complex underlying setting and loosely coupled database architecture lead to various security risks and performance bottlenecks, while there is currently no work to achieve a comprehensive evaluation of existing encrypted database solutions from the aspects of underlying settings, security levels, functions, etc. In this work, we first propose an evaluation model to assess SQL functionalities and security from multiple dimensions. Secondly, we categorise the existing SQL query schemes into three categories: software-based construction, hardware-based construction, and hybrid-based construction, that is, a combination of software and hardware components. On this basis, we analyse the framework, advantages, and limitations of classic and state-of-the-art schemes. Finally, we summarise the software-based and hardware-based approaches from dimensions of SQL functionality, security, and efficiency, thus clarifying their ideal application scenarios. Notably, SQL query schemes that exhibit minimal equality of pair leakage and support strong obliviousness can achieve higher levels of security. In addition, hardware-based solutions can achieve more complex SQL queries and superior performance without designing complex and functionally-limited cryptographic tools. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",-,10.1080/09540091.2024.2323059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186758448&doi=10.1080%2f09540091.2024.2323059&partnerID=40&md5=02ec5a66072ba15d95c12732586ecefc,2024
SN-RNSP: Mining self-adaptive nonoverlapping repetitive negative sequential patterns in transaction sequences,"Negative sequential patterns (NSP) focus on non-occurring events and play a role that cannot be replaced by positive sequential patterns (PSP). Considering the repetitive occurrence of sequential patterns in a sequence, repetitive NSP (RNSP) mining captures frequent NSP across different sequences from a database. Those patterns benefit many tasks of transaction services, e.g., fraud detection and medical diagnosis. However, limited studies focusing on mining RNSP are proposed, e.g., e-RNSP and ONP-Miner, and they are devised under strict constraints and are inefficient in practice. To address these issues, this paper proposes a Self-adaptive Nonoverlapping RNSP mining method SN-RNSP to mine nonoverlapping RNSP with the self-adaptive gap between successive elements from transaction sequences, which requires that each element cannot be reused at the same position in occurrences, and the gap value does not need to be specified in advance. First, this paper develops a method that maintains occurrences of pattern candidates via the bitmap structure to capture all repetitive PSP (RPSP), which utilizes the bitmap-based operation to calculate support efficiently. Second, SN-RNSP leverages bitmaps to record the locations of RPSP and RNSP in the database and query the repetition times of corresponding RPSP for the support calculation of RNSP. Conducted on real-world and synthetic datasets, extensive experiments demonstrate that SN-RNSP can discover more patterns with better mining performance than the state-of-the-art RNSP mining algorithms in transaction sequence databases. © 2024 Elsevier B.V.",-,10.1016/j.knosys.2024.111449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184510583&doi=10.1016%2fj.knosys.2024.111449&partnerID=40&md5=89cce2ddb86c36d39f6f2b6f4c05f0c5,2022
ZJUT-EIFD: A Synchronously Collected External and Internal Fingerprint Database,"External fingerprints (EFs) based only on epidermal information are vulnerable to spoofing attacks and non-ideal skin conditions. To solve such shortcomings, internal fingerprints (IFs) collected using optical coherence tomography (OCT) have been proposed and widely researched. However, the development of IF is limited by the lack of in-depth researches on the IF and the EF-IF interoperability, which is partially caused by the lack of public OCT database. The obvious gap in the applications of EF and IF recognition motivated us to design and publish a comprehensive fingerprint database containing both traditional EFs and OCT IFs, denoted as ZJUT-EIFD. To the best of our knowledge, ZJUT-EIFD is the first public database that combines OCT and total internal reflection (TIR) via synchronous acquisition, with 399 different fingers from 60 subjects. In this article, the composition of the database, the quality of EFs and IFs, and the verification performance of different types of fingerprints were detailed. In addition, potential application directions of ZJUT-EIFD were demonstrated. ZJUT-EIFD can serve benchmarks and interoperability tests for EF-IF research, which will promote the research and development of EF and IF.  © 1979-2012 IEEE.",2267-2284,10.1109/TPAMI.2023.3334760,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151089037&doi=10.1109%2fTPAMI.2023.3334760&partnerID=40&md5=92699a36c123b0323bb741c116277bb0,2022
Digital Regeneration and Database Construction of Hunan Embroidery Needlework under the Perspective of Artificial Intelligence,"In order to explore the development path of Hunan embroidery under the vision of artificial intelligence, promote the digital regeneration and database construction of Hunan embroidery stitches, the communication, and interaction between Hunan embroidery brands and the public, and create more possibilities for revitalizing the culture and industrial development of non-heritage Hunan embroidery. In this paper, a mechanics model of Hunan embroidery stitch is established based on the finite element idea under the view of artificial intelligence. The single yarn in the yarn is regarded as a frictionless articulation of some rows of elastic rods with a circular cross-section. The elastic rod can only be subjected to axial force without a moment, and it is a uniform, continuous, and completely elastic isotropic body. Using the displacement method, the displacement of the unit node is taken as the basic unknown quantity, the displacement in the unit is assumed to be linearly distributed, and the displacement of any node in the unit is obtained by linear interpolation. The strain, stress, and stiffness matrices of the elastic rod unit are derived, the equilibrium equations are given, and a database is established. The results of the study showed that consumers of all age levels thought that the patterns representing Hunan embroidery mainly include Hunan characteristic landscapes, portraits of Hunan great men, traditional flowers, birds and animals, and totems of Chu culture, etc., among which Hunan characteristic landscape accounted for 54% of the largest proportion. It provides a development direction for the inheritance and protection of Hunan embroidery skills. © 2023 Shunyao Sun, Yuxuan Tang, Wei Wang, and Ying Xiao, published by Sciendo.",-,10.2478/amns.2023.1.00093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158121675&doi=10.2478%2famns.2023.1.00093&partnerID=40&md5=80da848cc73ceb330ffe52e38381278d,2023
Video question answering via traffic knowledge database and question classification,"Video question answering (VideoQA) is a task that involves answering questions related to videos. The main idea is to understand the content of the video and to combine it with the relevant semantic context to answer various types of questions. Existing methods typically analyze the spatiotemporal correlations of the entire video to answer questions. However, for some simple questions, the answer is related to only a specific frame of the video, and analyzing the entire video undoubtedly increases the learning cost. For some complex questions, the information contained in the video is limited, and these methods are not sufficient to fully answer such questions. Therefore, we proposes a VideoQA model based on question classification and a traffic knowledge database. The model starts from the perspective of the question and classifies the questions into general scene questions and causal questions using different methods to process these two types of questions. For general scene questions, we first extract the key frames of the video to convert it into a simpler image question-answering task and then we use top–down and bottom–up attention mechanisms to process it. For causal questions, we design a lightweight traffic knowledge database that provides relevant traffic knowledge not originally present in VideoQA datasets, to help model reasoning. Then, we use a question and knowledge-guided aggregation graph attention network to process causal questions. The experimental results show that while greatly reducing resource costs, our model performs better on the TrafficQA dataset than do models utilizing millions of external data for pretraining. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",-,10.1007/s00530-023-01240-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182472938&doi=10.1007%2fs00530-023-01240-5&partnerID=40&md5=4998c69285967eaab5282a3778967720,2022
Fingerprint-based Indoor Localization via Deep Learning,"Deep learning (DL) application is proven helpful in a vast research field. One recent trend is to employ DL in radio frequency (RF)-based indoor localization. The fingerprint technique is the most used indoor localization technique known for its accuracy and performance. However, the fingerprint technique pays a high cost and effort in offline database construction, while its performance solely depends on the database density. Moreover, to apply deep learning, we also need a large dataset for it to learn efficiently. We propose to implement a DL-based fingerprint technique to tackle both problems of dataset scarcity and localization performance. We propose the DL's discriminative model, i.e., multilayer perceptron (MLP), for classification tasks. For the fingerprint database augmentation, we employed the generative model, i.e., Generative adversarial networks (GANs). We considered using a received signal strength indicator (RSSI) from a measurement campaign based on Wi-Fi devices for the database. The total area of interest is 25 m2 inside the typical classroom environment, and we consider the 25 fingerprint locations as labels. We have a dataset of 1,250 rows x 8 columns (from 8 reference points). From the results, by using only 50% of actual data combined with the 125 synthetic data, we can improve the accuracy by more than 200% compared to only using 50% of actual data and show a 60% improvement in the loss. The combination of 100% actual data and 125 synthetic data gives the best accuracy and loss performance of 0.76 and 0.85, respectively. It gives an improvement of 144% in accuracy and 200% loss performance. By implementing deep learning for fingerprint techniques for data augmentation and classification, we can achieve good performance and reduce the workload of fingerprint database construction.  © 2023 ACM.",146-152,10.1145/3592307.3592330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169806874&doi=10.1145%2f3592307.3592330&partnerID=40&md5=e669a13c2ec703b978126349a90cb567,2023
Tourism destinations' digital marketing: Current trends and issues,"Purpose: The purpose of this study is to systematically analyze research references related to digital marketing in the tourism sector, particularly from the perspective of tourist destinations, over the last 30 years, or from 1992 to 2022. This study will specifically look at the development trends of digital marketing research in the tourism sector in data from Google Scholar. Design/methodology/approach: In this study, a descriptive qualitative approach was used in conjunction with the literature review method. This research collected data on 287 international publications from a total of 319 identified articles in an exploration. A total of 32 articles were not used as data because they did not meet the criteria for reliability based. The keyword used is ""tourism digital marketing,""which was sourced from the Google Scholar database via web scraping. The sample used is only articles in English based on these keywords. As a result, this study did not consider digital marketing research in the tourism industry as a whole, considering that many languages other than English are published through the Google Scholar database. Findings: According to the findings of this study, the highest growth in publications on the topic of digital marketing in the tourism sector on the Google scholar database occurred from 2020 to 2022, with a total of 170 articles. With a total of four articles, the Journal of Pengabdian Kepada Masyarakat (community service) is one of the journals that frequently publishes this issue. This review also categorizes digital marketing research trends in the tourism sector into three major issues: research locations, research approaches, and scope or topic research. Furthermore, this review organizes three major perspectives that are commonly used when discussing digital marketing in the tourism sector, including those related to social; business, management, and marketing; and information and technology. Research limitations/implications: The research is limited by the scope of the articles used, which are sourced from a single Google Scholar database. In the future, in addition to research with broader literature sources, the recommendations in this study can be used as hypotheses and in-depth follow-up research. The findings of this study are intended to represent as a resource for academics and researchers interested in studying digital marketing in the tourism sector, particularly in terms of tourist destinations.  © 2023 ACM.",87-95,10.1145/3616712.3616791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180413807&doi=10.1145%2f3616712.3616791&partnerID=40&md5=3efba05aaa5b1cf21434aaa88edf2f59,2023
Computer Vision in Smart City Application: A Mapping Review,"Research interest in the application of Computer Vision (CV) in the context of smart cities has grown significantly in recent years. However, finding relevant studies in databases can be challenging due to the use of non-explicit or varying terms. To address this issue, this research aims to conduct a literature review by collecting papers that discuss the application of CV for smart city needs. The method used is a mapping review, with the following stages: 1) Establishing research questions; 2) Selecting relevant reference sources based on relevant databases; 3) Identifying appropriate keywords; 4) Conducting screening based on criteria; and 5) Extracting and synthesizing relevant data. For this research, we selected a total of 35 papers as the primary review. Our findings show that research trends and interests vary widely, with the most prevalent being in smart mobility components, which on average come from the IEEE Xplore Digital Library database. YOLO and CNN algorithms are also widely used to solve problems. Furthermore, we conclude that there are many benefits to be gained from the application of this technology to support the sustainable development of smart cities, including ensuring the quality of life of their inhabitants.  © 2023 IEEE.",-,10.1109/ACIIS59385.2023.10367332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182937452&doi=10.1109%2fACIIS59385.2023.10367332&partnerID=40&md5=c0111f4983181581dc24290bc8b3e624,2023
Abeto: An automated benchmarking tool to manage heterogeneous IP core databases,"System-level design makes use of building blocks, known as soft IP cores, to build complex developments. The usage of these IP cores allows to reduce design and verification time, and also to save costs. However, the use of third-party IP cores tends to present difficulties because of a lack of standardization in their organization, distribution and management, which derive in heterogeneous databases. Most of the time, system developers need to describe some additional code to enable the integration, verification and validation of the IP core, which is not available as part of their distribution. This implies acquiring a deep knowledge of each IP core, often with a large learning curve. In this work Abeto is presented, a new software tool for IP core databases management. It allows to easily integrate and use a heterogeneous group of IP cores, described in VHDL, with a unified set of instructions or commands. In order to do so, Abeto requires from every IP core some side information about its packaging and how to operate with the IP. Currently, Abeto provides support for a set of well-known EDA tools and has been successfully applied to the European Space Agency portfolio of IP cores for benchmarking purposes. To demonstrate its performance, mapping results for these IP cores on the novel NanoXplore BRAVE FPGA family are provided. © 2023 The Author(s)",-,10.1016/j.micpro.2023.104987,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181750444&doi=10.1016%2fj.micpro.2023.104987&partnerID=40&md5=01294b9752fee9ec09835cfe6534e2f5,2022
Data-driven hierarchical Bayesian model for predicting wall deflections in deep excavations in clay,"The current paper presents a data-driven hierarchical Bayesian model (HBM) for predicting maximum lateral wall deflections in deep excavations in clay. The presented approach can address a number of challenges, including handling missing input parameters, incorporating the observational method, and accounting for site uniqueness. A new database, EXCA-CLAY/11/901, comprising 302 excavation sites worldwide, is compiled to train the HBM. The trained HBM is applied to a real case study in downtown Shanghai. The HBM can be applied to predict the maximum lateral wall deflection at each excavation stage independent of measured deflections from past stages (non-observational approach) or the more typical observational approach that includes past deflection measurements. Extensive cross-validation analysis validates the performance of the HBM and compares it with three existing regression models. The results show that the regression models exhibit slightly better performance within their applicable range when compared to the non-observational HBM prediction, but are outperformed by the observational HBM. The HBM is shown to offer several advantages over the conventional regression models, including the ability to produce 95% confidence intervals, handle missing input parameters, accommodate a diverse database, and continuously refine predictions as new information becomes available. © 2024 Elsevier Ltd",-,10.1016/j.compgeo.2024.106135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184517326&doi=10.1016%2fj.compgeo.2024.106135&partnerID=40&md5=d2703de734261ad7f22eceac5ed87dd6,2022
Query Execution Plans and Semantic Errors: Usability and Educational Opportunities,"Syntax errors are typically separated from semantic errors in query formulation, the former being detected by the database management system (DBMS), and the latter seemingly not. On the other hand, query execution plans are typically utilized in query optimization, and not interconnected with syntax errors, as a syntactically invalid query produces no execution plan. In this study, we show and argue for breaking the confound between execution plans and error messages for better query formulation usability and education. We show how several popular DBMSs detect semantic errors and complications in queries, yet often do not inform the user of such problems. This study is a demonstration of how decades old technology could be used more effectively in novel contexts of usability and software engineering education with little effort by showing query writers not merely syntax errors, but also semantic errors and complications detected by DBMSs. © 2023 Owner/Author.",-,10.1145/3544549.3585794,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158114517&doi=10.1145%2f3544549.3585794&partnerID=40&md5=ad94b43a23552312040e50e52d7d7197,2022
"Vector database management systems: Fundamental concepts, use-cases, and current challenges","Vector database management systems have emerged as an important component in modern data management, driven by the growing importance for the need to computationally describe rich data such as texts, images and video in various domains such as recommender systems, similarity search, and chatbots. These data descriptions are captured as numerical vectors that are computationally inexpensive to store and compare. However, the unique characteristics of vectorized data, including high dimensionality and sparsity, demand specialized solutions for efficient storage, retrieval, and processing. This narrative literature review provides an accessible introduction to the fundamental concepts, use-cases, and current challenges associated with vector database management systems, offering an overview for researchers and practitioners seeking to facilitate effective vector data management. © 2024 The Author",-,10.1016/j.cogsys.2024.101216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185410189&doi=10.1016%2fj.cogsys.2024.101216&partnerID=40&md5=e266a48f26c78c0a4243133083ad7551,2024
Innovative Application of Visual Communication of Yi Lacquer Art under Internet Technology,"Based on archaeological and historical data, this paper firstly researches and explains the source and flow of Yi lacquer art in Liangshan and proposes the visual communication innovation strategy of Yi lacquer art. Secondly, it uses Internet technology to collect images of Yi lacquer art, grayscale them on the basis of obtaining images of Yi lacquer art, and then facilitates the adaptive threshold method to realize the conversion of the bitmap into a vector map. Then, considering that the visual database pattern resource of Yi lacquer art contains not only pattern information but also color information, we constructed the visual communication database of Yi lacquer art images and carried out an example analysis of the visual communication of Yi lacquer art images based on adaptive thresholding method. The results show that the accuracy, precision, and DSC of R-AMRFCM are improved by 0.0978, 0.4537, and 0.5436, respectively, compared with the GSRG algorithm in algorithmic analysis. In the analysis of Yi lacquer art communication behaviors, behavioral cognition (t=-4.108, p<0.01), behavioral habit (t=-3.090, p<0.01), behavioral intention (t =-2.848, p<0.01), emotional experience (t=-2.277p<0.05), subjective criteria (t=-4.241, p<0.01), and attitude towards Yi lacquer art communication (t=-5.268, p<0.01) are all significantly different. This study optimizes the details of Yi lacquer art dissemination in Internet technology and effectively promotes the visual dissemination and high-quality development of Yi lacquer art. © 2023 Ying Tan, published by Sciendo.",-,10.2478/amns.2023.2.01066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175983876&doi=10.2478%2famns.2023.2.01066&partnerID=40&md5=3dd8928e4ae0042e9e0cb938146cd77e,2023
Towards Building the Next Generation Computation Engine,"In this poster, I first briefly introduce several system work in the Database Research Group at Southern University of Science and Technology, (i.e., DBGroup@SUSTech); I next present the key ideas of our on-going project, i.e., architecting the next generation computation engine, which is designed for data-intensive applications on heterogeneous computing environment.  © 2023 Owner/Author.",129-130,10.1145/3603165.3607435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174239235&doi=10.1145%2f3603165.3607435&partnerID=40&md5=68bd48bc9c061ffa23c2b58d3db7c056,2023
Lysosome-related biomarkers in preeclampsia and cancers: Machine learning and bioinformatics analysis,"Background: Lysosomes serve as regulatory hubs, and play a pivotal role in human diseases. However, the precise functions and mechanisms of action of lysosome-related genes remain unclear in preeclampsia and cancers. This study aimed to identify lysosome-related biomarkers in preeclampsia, and further explore the biomarkers shared between preeclampsia and cancers. Materials and methods: We obtained GSE60438 and GSE75010 datasets from the Gene Expression Omnibus database, pre-procesed them and merged them into a training cohort. The limma package in R was used to identify the differentially expressed mRNAs between the preeclampsia and normal control groups. Differentially expressed lysosome-related genes were identified by intersecting the differentially expressed mRNAs and lysosome-related genes obtained from Gene Ontology and GSEA databases. Gene Ontology annotations and Kyoto Encyclopedia of Genes and Genomes enrichment analysis were performed using the DAVID database. The CIBERSORT method was used to analyze immune cell infiltration. Weighted gene co-expression analyses and three machine learning algorithm were used to identify lysosome-related diagnostic biomarkers. Lysosome-related diagnostic biomarkers were further validated in the testing cohort GSE25906. Nomogram diagnostic models for preeclampsia were constructed. In addition, pan-cancer analysis of lysosome-related diagnostic biomarkers were identified by was performed using the TIMER, Sangebox and TISIDB databases. Finally, the Drug-Gene Interaction, TheMarker and DSigDB Databases were used for drug-gene interactions analysis. Results: A total of 11 differentially expressed lysosome-related genes were identified between the preeclampsia and control groups. Three molecular clusters connected to lysosome were identified, and enrichment analysis demonstrated their strong relevance to the development and progression of preeclampsia. Immune infiltration analysis revealed significant immunity heterogeneity among different clusters. GBA, OCRL, TLR7 and HEXB were identified as lysosome-related diagnostic biomarkers with high AUC values, and validated in the testing cohort GSE25906. Nomogram, calibration curve, and decision curve analysis confirmed the accuracy of predicting the occurrence of preeclampsia based on OCRL and HEXB. Pan-cancer analysis showed that GBA, OCRL, TLR7 and HEXB were associated with the prognosis of patients with various tumors and tumor immune cell infiltration. Twelve drugs were identified as potential drugs for the treatment of preeclampsia and cancers. Conclusion: This study identified GBA, OCRL, TLR7 and HEXB as potential lysosome-related diagnostic biomarkers shared between preeclampsia and cancers. © 2024 Elsevier Ltd",-,10.1016/j.compbiomed.2024.108201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186266101&doi=10.1016%2fj.compbiomed.2024.108201&partnerID=40&md5=a9ea01b4c06f346c77703aea75425cd4,2023
Development of computer application system and database testing based on data encryption technology,"In this paper, we use the MapReduce programming framework for the development of a computer application system and design the vision module, control module and transaction module of the system based on MVC architecture. Focusing on the encryption design of the database, the data stored in the database is chunked through the Map function, and all the chunking results are aggregated based on the Reduce function, and synchronized encryption of the database is combined with the Paillier homomorphic encryption algorithm. On this basis, the system environment is configured, and the application system developed in this paper is tested, focusing on exploring the stability of the database. The test results show that the decryption output time is 21s when the Paillier encryption algorithm is 200, and the first set of database peak is 76% of the server load, which is within the tolerable range. Security test on the database: the results show that the probability of an attack on all ports with a database security factor less than 6 is around 0.3. There is no significant difference, and the database test is good.  © 2023 Wei Tang, published by Sciendo.",-,10.2478/amns.2023.2.01665,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331525&doi=10.2478%2famns.2023.2.01665&partnerID=40&md5=c321e2f2a800995f4a6bccfeeecf7238,2022
Research on Image Database Classification Method Based on MHPA,"Due to the image database size expanding, how to effectively classify and retrieve images has become an important research topic. This paper proposes an image database classification method based on a multi-step heuristic partitioning algorithm (MHPA) and convolutional neural network, aiming to improve the accuracy and efficiency of image classification, provide an overview of the principle and process of the MHPA and the methods and techniques for extracting features from images. Furthermore, this paper designs a system based on an MHPA and convolutional neural network, including image acquisition, preprocessing, and image classification modules. This paper describes the structure and parameters of the convolutional neural network model and the model training and optimization process. Finally, this paper verifies the effectiveness and superiority of this method through experiments, analyzes the experimental results, and gives conclusions and prospects.  © 2023 IEEE.",292-298,10.1109/ICEDCS60513.2023.00059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182604785&doi=10.1109%2fICEDCS60513.2023.00059&partnerID=40&md5=0eff879da6e7a4bff6d6337d71464f5d,2021
UltraCytomic: A cross-scale exploration to improve risk estimation of the malignant thyroid nodule,"Initial estimation of the thyroid nodule malignancy mainly relies on the analysis of ultrasound images, but cytology confirmation is always required. Despite clinical decision guides, final criteria evaluation is still prone to subjectivity in both, ultrasound and cytology. Recently, artificial intelligence has been applied to identify malignant thyroid nodules with outstanding results in both image modalities. However, these automatic models have been separately developed either to ultrasound or cytology, ignoring coupled relations which are commonly used by physicians in clinical practice. Malignancy prediction is herein improved by concatenating relevant cytology and ultrasound image features and then used to obtain coupled models. The proposed approach is challenged with 314 patients, coming from two independent databases containing either only ultrasound or coupled ultrasound-cytology images. Basically, the most relevant ultrasound features were selected from the first image database with only ultrasound information (299 patients and 650 thyroid nodules). Afterward, malignancy prediction was improved in a second independent database composed of patient-coupled thyroid/ultrasound images (40 nodules, eight malignant, from 15 patients). The obtained results show that the model trained with coupled information outperformed the one trained with only ultrasound, i.e. accuracy and fl-score were respectively 0.80 and 0.77 when ultrasound and cytology were combined, compared to the respective 0.67 and 0.63 obtained with only ultrasound.  © 2023 IEEE.",-,10.1109/SIPAIM56729.2023.10373523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183463773&doi=10.1109%2fSIPAIM56729.2023.10373523&partnerID=40&md5=3908268deef1fea312912447f6850251,2021
PMechDB: A Public Database of Elementary Polar Reaction Steps,"Most online chemical reaction databases are not publicly accessible or are fully downloadable. These databases tend to contain reactions in noncanonicalized formats and often lack comprehensive information regarding reaction pathways, intermediates, and byproducts. Within the few publicly available databases, reactions are typically stored in the form of unbalanced, overall transformations with minimal interpretability of the underlying chemistry. These limitations present significant obstacles to data-driven applications including the development of machine learning models. As an effort to overcome these challenges, we introduce PMechDB, a publicly accessible platform designed to curate, aggregate, and share polar chemical reaction data in the form of elementary reaction steps. Our initial version of PMechDB consists of over 100,000 such steps. In the PMechDB, all reactions are stored as canonicalized and balanced elementary steps, featuring accurate atom mapping and arrow-pushing mechanisms. As an online interactive database, PMechDB provides multiple interfaces that enable users to search, download, and upload chemical reactions. We anticipate that the public availability of PMechDB and its standardized data representation will prove beneficial for chemoinformatics research and education and the development of data-driven, interpretable models for predicting reactions and pathways. PMechDB platform is accessible online at https://deeprxn.ics.uci.edu/pmechdb. © 2024 The Authors. Published by American Chemical Society",1975-1983,10.1021/acs.jcim.3c01810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187982624&doi=10.1021%2facs.jcim.3c01810&partnerID=40&md5=2d5bb590a340e3cded5e8699edc6f956,2021
Human factors in software development: A study on database systems adoption by developers.,"Ease of use and usability of tools and programming languages used in software development has been a topic of interest of the field of human-computer interaction. Various studies have examined these technologies from the point of view of the user, focusing either in general purpose programming languages, or specific tools. The area of database design and development and the adopted data models and tools is of high importance for most modern information systems, as the explosion of data generation has triggered a rapid development in databases. So, an interesting research question is to investigate the factors that influence practitioners in adopting database models and tools today. Factors to be considered relate to business, organizational and social factors, as well as the usability of the tools and representations used. In this explorative study we investigate (1) choices by professionals in relation to different types of projects (2) the criteria programmers use when selecting technological models; (3) possible reasons for reluctance in adoption; and (4) Perception of different models. The research is based on interviews with 9 professional developers in both their work as independent consultants and in corporate environments. The results revealed a strong influence of background knowledge acquired during university studies and experience as well as other social and organizational factors that influence technology selection by IT professionals concerned. © 2023 ACM.",-,10.1145/3609987.3609994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174495996&doi=10.1145%2f3609987.3609994&partnerID=40&md5=0a8984e019d7dbda06021f4555e19065,2024
Automated face recognition system for smart attendance application using convolutional neural networks,"In this paper, a touch less automated face recognition system for smart attendance application was designed using convolutional neural network (CNN). The presented touch less smart attendance system is useful for offices and college’s attendance applications with this the spread of covid-19 type viruses can be restrict. The CNN was trained with dedicated database of 1890 faces with different illumination levels and rotate angles of total 30 targeted classes. A CNN performance analysis was done with 9-layer and 11-layer with different activation functions i.e., Step, Sigmoid, Tanh, softmax, and ReLu. An 11-layer CNN with ReLu activation function offers an accuracy of 96.2% for the designed face database. The system is capable to detect multiple faces from test images using Viola Jones algorithm. Eventually, a web application was designed which helps to monitor the attendance and to generate the report. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024.",162-178,10.1007/s41315-023-00310-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181873617&doi=10.1007%2fs41315-023-00310-1&partnerID=40&md5=b98b5e7b5a303c81e8ee73fa6801b8e7,2021
MChemID: Android Application for Chemical Reactions using Speech Recognition,"In the ever-evolving field of chemistry, the need for efficient and user-friendly tools to identify chemical reactions has become crucial. mChemID, a mobile application that leverages speech recognition technology to enable users to input chemical elements verbally and retrieve corresponding reactions from a cloud-based database, streamlining the learning process for students, educators, and researchers. It utilizes speech recognition technology to identify chemical reactions by the speech input as chemical elements. Objective: The primary objective is to develop a user-friendly mobile application, mChemID, that utilizes speech recognition technology to assist individuals in identifying chemical reactions by chemical elements through voice input. The application aims to provide a convenient and efficient tool for users to retrieve chemical symbols and corresponding reactions from a cloud-based database. Methodology: The Android platform's built-in speech recognition functionality is used to verbally input chemical element names. Google's speech-to-text service is used for transcription, ensuring accurate recognition. The application's design and development are carried out using Android Studio, a popular development environment for Android applications. Cloud-based connectivity is integrated into the application to fetch and access the required chemical reaction data from a central database. Outcome: The mChemID offers a user-friendly and efficient means to facilitate and simplify the understanding and study of chemical reactions, contributing to the advancement of chemical knowledge and research.  © 2023 IEEE.",-,10.1109/ICACIC59454.2023.10435349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186759040&doi=10.1109%2fICACIC59454.2023.10435349&partnerID=40&md5=f79e47b359a0a9fa36d4be7045dd7b33,2021
Construction of Digital Case Base System Based on DB2 Database,"DB2 database supports from mainframe to single-user environment, and makes database localization and remote connection transparent, simplifies the operation and maintenance process of database system, and has become the most important platform for data storage and processing of mainframe system. Digital teaching resources simplify the knowledge and teaching difficulties through multimedia devices, making the communication and interaction between teachers and students more convenient. Teaching case base is one of the digital teaching resources, and the construction of digital teaching case base for recitation based on DB2 database meets the needs of teaching reform and development in the information age. The DB2 object architecture and DB2 Viper architecture are analysed, the database design and functional design of digital teaching case base system for recitation are carried out, and the specific application scheme is proposed. © 2023 Elsevier B.V.. All rights reserved.",1187-1194,10.1016/j.procs.2023.11.108,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184135494&doi=10.1016%2fj.procs.2023.11.108&partnerID=40&md5=cecaeff2e465a0ede4ac79e7dd7a2b6b,2023
Transformer 4.0 - A Smart Transformer for a Smarter Living,"Traditional transformers are not equipped with sophisticated monitoring and control features, which limits their ability to operate and maintain efficiently. The suggested system, which can be adapted to the current Distribution transformers in India, has been developed for 2% of the estimated cost of the newer Smart Transformers on the market, which cost a staggering 10 lakhs. This project focuses on turning ordinary transformers into smart devices by integrating the Raspberry Pi 4B, the Arduino Mega 2560, and Heltec ESP32, which includes Wifi, LoRa, and Bluetooth in addition to an OLED panel that provides additional functionality to the task. The three boards are primarily separated into three distinct subsystems, with the Raspberry Pi serving as the primary computer and the other two separate boards / subsystems serving as backup devices in case of unanticipated events. several sensors, including the DHT 11, GPS, Accelerometer for detecting tremors and accidents, the BMP280 for altitude and pressure sensing within the oil chamber, the HCSR04 for monitoring the oil level, and the Camera Module for transmitting data over the internet. The database is managed by an active DBMS that has been combined with an RFID scanner and a biometric fingerprint sensor for security reasons. Wi- Fi and LoRa protocols have been used to allow for distant communication while keeping the system operational round-the-clock. On the Raspberry Pi, a clever method of transmitting Push notifications to all devices, including Smart phones, PCs, tablets, and different Smart devices, has been implemented in order to save costs and eliminate GSM maintenance. For offline data logging, a USB device has been programmed to capture the data on a Raspberry Pi that serves as a 'Blackbox,' and an SD card using SPI communication interfaced with an Arduino MEGA is very effective to understand the working statistics of the Smart Transformer. Additionally, by using a GPS module, it aids in the identification of power theft and transmits the information to local authorities. Last but not least, this job makes use of cutting-edge boards for highly effective, power-efficient, accurate, and dependable working. Cloud services from ThingSpeak and Adafruit have been enabled with certificates that have been encrypted and decoded, as well as the use of the 8883 TCP port for safe Client communication on the MQTT protocol. With better maintenance procedures and less downtime, this makes it possible to proactively identify probable problems and abnormalities.  © 2023 IEEE.",-,10.1109/CISCT57197.2023.10351417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182924021&doi=10.1109%2fCISCT57197.2023.10351417&partnerID=40&md5=bd0098427540aca27b9c789302397adc,2023
A rigorous possibility approach for the geotechnical reliability assessment supported by external database and local experience,"Reliability analyses based on probability theory are widely applied in geotechnical engineering, and several analytical or numerical methods have been built upon the concept of failure occurrence. Nevertheless, common geotechnical engineering real-world problems deal with scarce or sparse information where experimental data are not always available to a sufficient extent and quality to infer a reliable probability distribution function. This paper rigorously combines Fuzzy Clustering and Possibility Theory for deriving a data-driven, quantitative, reliability approach, in addition to fully probability-oriented assessments, when useful but heterogeneous sources of information are available. The proposed non-probabilistic approach is mathematically consistent with the failure probability, when ideal random data are considered. Additionally, it provides a robust tool to account for epistemic uncertainties when data are uncertain, scarce, and sparse. The Average Cumulative Function transformation is used to obtain possibility distributions inferred from the fuzzy clustering of an indirect database. Target Reliability Index Values, consistent with the prescribed values provided by Eurocode 0, are established. Moreover, a Degree of Understanding tier system based on the practitioner's local experience is also proposed. The proposed methodology is detailed and discussed for two numerical examples using national-scale databases, highlighting the potential benefits compared to traditional probabilistic approaches. © 2023 The Author(s)",-,10.1016/j.compgeo.2023.105967,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179121540&doi=10.1016%2fj.compgeo.2023.105967&partnerID=40&md5=fdeb81d972a001026711801d359f0a82,2023
Emotion Recognition to Support Personalized Therapy: An Approach Based on a Hybrid Architecture of CNN and Random Forest,"Emotions play an important role in the life of any human being, directly influencing communication and social interaction. In recent years, personalized medicine has stood out due to its ability to improve medical practice. Through processing patient data, it is possible to create strategies to individualize their care, from diagnosis to therapeutic practices. Focusing on customization of therapeutic practices, we believe that emotion recognition systems through facial expressions can contribute, especially for patients who have difficulties expressing emotions due to several reasons, including pathologies. Identifying wishes and desires during therapy through these systems can help the therapist assertively customize their approach. Aiming to contribute in this field, we developed a hybrid architecture for emotion recognition through facial expressions based on a pre-trained Convolutional Neural Network (CNN) used for feature extraction, and a Random Forest, responsible for classifying emotions. As novelty, our model was trained with a database created by combining four well studied databases (i.e.: FER-2013, Chicago Face Database, KDEF and Yale Face), what we called complete database. The proposed method achieved 70.52% accuracy in the training/validation stage and 82.92% in testing, despite all the variability between images from the different databases. © 2023 IEEE.",-,10.1109/LA-CCI58595.2023.10409408,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185217518&doi=10.1109%2fLA-CCI58595.2023.10409408&partnerID=40&md5=7c5c3966c563a19ad5e724f7bc3d83a3,2023
Efficient Braille Transformation for Secure Password Hashing,"In this work, we propose a novel approach to enhancing the security of passwords before storing them in databases. Our method utilizes Braille transformation to encrypt the password after generating the corresponding hash. The hash is divided into multiple blocks, each representing a character treated as a transformation unit. Each character is then associated with its corresponding Braille code, which consists of 6 digits. To further enhance security, we randomly replace each occurrence of '0' in the generated string with one of the digits 7, 8, or 9. The final string, six times larger than the original hash, is then stored in the database. To evaluate our approach, we conducted several experiments and comparisons. The results demonstrate that Braille transformation is resistant to brute-force attacks, statistical attacks, and differential attacks. These results were justified using various evaluation criteria, such as execution time and memory space occupied. Braille transformation is susceptible to any modification made to the hash or the generated string, further reinforcing its security. Our Braille-based approach offers an effective solution to strengthen the security of database passwords. It provides advantages in terms of protection against different attacks and offers a robust evaluation based on relevant criteria.  © 2013 IEEE.",5212-5221,10.1109/ACCESS.2024.3349487,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181572648&doi=10.1109%2fACCESS.2024.3349487&partnerID=40&md5=b41a2617777aeaae9efeeb2037dea72a,2021
SINGLE BUILDING POINT CLOUD SEGMENTATION: TOWARDS URBAN DATA MODELING AND MANAGEMENT,"To manage urban areas, a key step is the development of a geometric survey and its subsequent analysis and processing in order to provide useful information, and to become a good basis for urban modeling. Surveys of urban areas can be developed with various technologies, such as Aerial Laser Scanning, Unmanned Aerial Systems photogrammetry, and Mobile Mapping Systems. To make the resulting point clouds useful for subsequent steps, it is necessary to segment them into classes representing urban elements. On the other hand, there are 2D land representations that provide a variety of information related to the elements in the urban environment, which are linked to databases that have information content related to them. In this context, the element identified as interesting for urban management of the built heritage is the individual building unit. This paper presents an automated method for using map datasets to segment individual building units on a point cloud of an urban area. A unique number is then assigned to the segmented points, linking them directly to the corresponding element in the map database. The resulting point cloud thus becomes a container of the information in the map database, and a basis for possible city modeling. The method was successfully tested on the historic city of Sabbioneta (northern Italy), using two point clouds, one obtained through the use of a Mobile Mapping System and one obtained with Unmanned Aerial System photogrammetry. Two cartographic databases were used, one opensource (OpenStreetMap) and one provided by the regional authorities (regional cartographic database). © Author(s) 2023. CC BY 4.0 License.",511-516,10.5194/isprs-archives-XLVIII-1-W1-2023-511-2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162166534&doi=10.5194%2fisprs-archives-XLVIII-1-W1-2023-511-2023&partnerID=40&md5=4c27d0ae2b1c12f6621ec2229c764983,2021
"Analysis of chiral oxirane molecules in preparation for next generation telescopes: A review, new analysis, & a chiral molecule database","Human biology has a preference for left-handed chiral molecules and an outstanding question is if this is imposed through astrophysical origins. We aim to evaluate the known information about chiral molecules within astrophysical and astrochemical databases, evaluate chemical modeling accuracy, and use high-level CCSD(T) calculations to characterize propylene oxide and other oxirane variants. By comparing these computational values with past laboratory experiments, we find a 99.9% similarity. We also have put together a new database dedicated to chiral molecules and variants of chiral molecules to assist in answering this question. © 2024 The Author(s)",-,10.1016/j.ascom.2024.100791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184521774&doi=10.1016%2fj.ascom.2024.100791&partnerID=40&md5=8aba9367562c2762de8e63f3eb04cf93,2024
An efficient method for mining High-Utility itemsets from unstable negative profit databases,"The study of High-Utility Itemset Mining (HUIM) and Frequent Itemset Mining (FIM) is crucial since it explains consumer behavior and offers actionable advice to improve business results. HUIM algorithms have been successfully established to identify high-utility itemsets, including those with negative utilities. The problem with these approaches is that they presume incorrectly that items with negative utility across transactions would always be losses. Products with positive profitability may seem negative when combined with other items to increase sales or reduce inventory. Using strict upper-bound approaches, this paper presents strategies for making database scanning more efficient and reducing the number of prospective candidates. We also prove that it is correct to use the proposed upper-bounds for pruning on several types of items in the database. Based on all the proposed solutions, we develop a novel algorithm to solve this problem efficiently. To demonstrate their efficiency, the algorithms are tested against states-of-art HUIM algorithm on diverse datasets with regard to size and characteristics with unstable negative profits. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.121489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171785616&doi=10.1016%2fj.eswa.2023.121489&partnerID=40&md5=edd9f48628bac246d006c2d9ace70a6f,2021
CalD3r and MenD3s: Spontaneous 3D facial expression databases,"In the last couple of decades, the research on 3D facial expression recognition has been fostered by the creation of tailored databases containing prototypical expressions of different individuals and by the advances in cost effective acquisition technologies. Though, most of the currently available databases consist of exaggerated facial expressions, due to the imitation principle which they rely on. This makes these databases only partially employable for real world applications such as human-computer interaction for smart products and environments, health, and industry 4.0, as algorithms learn on these ‘inflated’ data which do not respond to ecological validity requirements. In this work, we present two novel 2D + 3D spontaneous facial expression databases of young adults with different geographical origin, in which emotions have been evoked thanks to affective images of the acknowledged IAPS and GAPED databases, and verified with participants’ self-reports. To the best of our knowledge, these are the first three-dimensional facial databases with emotions elicited by validated affective stimuli. © 2023 Elsevier Inc.",-,10.1016/j.jvcir.2023.104033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181889155&doi=10.1016%2fj.jvcir.2023.104033&partnerID=40&md5=56671bf4a68d80e1903e080d937ea4f2,2021
Detecting Robustness against MVRC for Transaction Programs with Predicate Reads,"The transactional robustness problem revolves around deciding whether, for a given workload, a lower isolation level than Serializable is sufficient to guarantee serializability. The paper presents a new characterization for robustness against isolation level (multi-version) Read Committed. It supports transaction programs with control structures (loops and conditionals) and inserts, deletes, and predicate reads - scenarios that trigger the phantom problem, which is known to be hard to analyze in this context. The characterization is graph-theoretic and not unlike previous decision mechanisms known from the concurrency control literature that database researchers and practicians are comfortable with. We show experimentally that our characterization pushes the frontier in allowing to recognize more and more complex workloads as robust than before. © 2023 Copyright held by the owner/author(s)",565-577,10.48786/edbt.2023.48,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162167654&doi=10.48786%2fedbt.2023.48&partnerID=40&md5=781de78088d0edbd898272cf933f1942,2023
Allocating Isolation Levels to Transactions in a Multiversion Setting,"A serializable concurrency control mechanism ensures consistency for OLTP systems at the expense of a reduced transaction throughput. A DBMS therefore usually offers the possibility to allocate lower isolation levels for some transactions when it is safe to do so. However, such trading of consistency for efficiency does not come with any safety guarantees. In this paper, we study the mixed robustness problem which asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation is always serializable. That is, whether the given allocation is indeed safe. While robustness has already been studied in the literature for the homogeneous setting where all transactions are allocated the same isolation level, the heterogeneous setting that we consider in this paper, despite its practical relevance, has largely been ignored. We focus on multiversion concurrency control and consider the isolation levels that are available in Postgres and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI). We show that the mixed robustness problem can be decided in polynomial time. In addition, we provide a polynomial time algorithm for computing the optimal robust allocation for a given set of transactions, prioritizing lower over higher isolation levels. The present results therefore establish the groundwork to automate isolation level allocation within existing databases supporting multiversion concurrency control.  © 2023 ACM.",69-78,10.1145/3584372.3588672,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164273094&doi=10.1145%2f3584372.3588672&partnerID=40&md5=2858397d7c595887fa3acf3aa659d434,2023
A framework for the prioritization of industry 4.0 and lean manufacturing technologies based on network theory,"Purpose: The purpose of this paper is to present a practical data-based framework for the prioritization of investment in manufacturing technologies, methods and tools, and to demonstrate its applicability and practical relevance through two case studies of manufacturing firms of different industrial segments. Design/methodology/approach: The proposed framework is based on network theory applied on technology adoption. For this, the database of Industry 4.0 maturity assessments of SENAI was used to develop data visualization tools named “Technology Networks”. Thus, this study is descriptive research with correlational design. Besides, the framework was applied in two companies and semi-structured interviews were carried out with domain experts. Findings: The technology networks highlight the technological adoption patterns of six industrial segments, by considering the answers of 863 Brazilian companies. In general, less sophisticated technologies were positioned in the center of the networks, which facilitates the visualization of adoption paths. Moreover, the networks presented a well-balanced adoption scenario of Industry 4.0 related technologies and lean manufacturing methods and tools. Research limitations/implications: Since the database was not built under an experimental design, it is not expected to make statistical inferences about the variables. Furthermore, the decision to use an available database prevented the editing or inclusion of technologies. Besides, it is estimated that the technology networks given have few years for obsolescence due to the fast pace of technological development. Practical implications: The framework is a tool that may be used by practicing manufacturing managers and entrepreneurs for taking assertive decisions regarding the adoption of manufacturing technologies, methods and tools. The proposition of using network theory to support decision making on this topic may lead to further studies, developments and adaptations of the framework. Originality/value: This paper addresses the topics of lean manufacturing and Industry 4.0 in an unprecedented way, by quantifying the adoption of its technologies, methods and tools and presenting it in network visualizations. The main value of this paper is the comprehensive framework that applies the technology networks for supporting decision making regarding technology adoption. © 2023, Emerald Publishing Limited.",95-118,10.1108/JMTM-03-2023-0114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176950851&doi=10.1108%2fJMTM-03-2023-0114&partnerID=40&md5=57573a9b4340b62152bfb2e0728fab86,2021
Interface-Based Search and Automatic Reassembly of CAD Models for Database Expansion and Model Reuse,"This paper introduces a new framework for reassembling CAD models of mechanical parts. The generated CAD assemblies are well-constrained, with collision-free parts, and they are ready-to-use for downstream applications. First, input dead CAD models candidate for the reassembly are sorted following a part-by-part interface-based identification algorithm that is capable of characterizing each part according to the assembly slots and interfaces it offers. The slots are then hashed, and the resulting keys are used for fast search of parts in the database. Thus, the parts that can be assembled are quickly identified, and their assembly can be considered according to various scenarios. To support the reassembly steps and satisfy the constraints associated with the specified interfaces, a collision-free kinematic constraint solver is also proposed. During the reassembly steps, the geometry of the slots can also be automatically modified to adjust their dimensions, in order to ensure a perfect fit and to avoid interference at the level of the interfaces. The resulting database can also be further expanded while modifying the relative positions/orientations of the assembled parts. The approach is illustrated on several test cases covering different exploitation scenarios, ranging from simple model reuse to database expansion for machine learning-based applications. Such an automatic reassembly process is particularly innovative, and it clearly paves the way for smart assembly procedures. © 2023 Elsevier Ltd",-,10.1016/j.cad.2023.103630,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175307335&doi=10.1016%2fj.cad.2023.103630&partnerID=40&md5=6f86ff97c096f72d3a813c07bedea32e,2024
Assertive Wiki: An Experience Report in the Industry on the Redesign of Software Requirements Documentation,"The development of requirements documentation in the software life cycle process is a crucial step in requirements engineering. However, due to excessive documentation, multiple databases for reference, and difficulties in understanding requirements, the testing process can become exhaustive. Existing literature already demonstrates that well-written documentation results in higher-quality product delivery. In summary, the industry focuses on clarity and objectivity in business rules, but does not address the issues of excessive documentation and multiple requirement databases. The aim of this study is to highlight the value of developing usable and useful documentation within the context of SIDIA, an Institute of Science and Technology. To achieve this, the Assertive Wiki was developed, comprising 45 wikis that document requirements based on the application of UX/UI Design and A/B Testing techniques. Through qualitative research conducted with a software testing team at SIDIA, it is demonstrated that 90% of the testers agree that the utilities and databases built in this manner are useful. Thus, it can be concluded that the applied requirements documentation process, through the use of Assertive Wiki, led to testers satisfaction with testing activities. © 2023 ACM.",22-30,10.1145/3634848.3634859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184516729&doi=10.1145%2f3634848.3634859&partnerID=40&md5=3ef44df280b5ad5b7f2728e29b55c238,2024
How to manage massive spatiotemporal dataset from stationary and non-stationary sensors in commercial DBMS?,"The growing diffusion of the latest information and communication technologies in different contexts allowed the constitution of enormous sensing networks that form the underlying texture of smart environments. The amount and the speed at which these environments produce and consume data are starting to challenge current spatial data management technologies. In this work, we report on our experience handling real-world spatiotemporal datasets: a stationary dataset referring to the parking monitoring system and a non-stationary dataset referring to a train-mounted railway monitoring system. In particular, we present the results of an empirical comparison of the retrieval performances achieved by three different off-the-shelf settings to manage spatiotemporal data, namely the well-established combination of PostgreSQL + PostGIS with standard indexing, a clustered version of the same setup, and then a combination of the basic setup with Timescale, a storage extension specialized in handling temporal data. Since the non-stationary dataset has put much pressure on the configurations above, we furtherly investigated the advantages achievable by combining the TSMS setup with state-of-the-art indexing techniques. Results showed that the standard indexing is by far outperformed by the other solutions, which have different trade-offs. This experience may help researchers and practitioners facing similar problems managing these types of data. © The Author(s) 2023.",2063-2088,10.1007/s10115-023-02009-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177027445&doi=10.1007%2fs10115-023-02009-y&partnerID=40&md5=0d0bf3feff3551a6d1ec1eb216767b91,2020
Compact deep neural networks for real-time speech enhancement on resource-limited devices,"In real-time applications, the aim of speech enhancement (SE) is to achieve optimal performance while ensuring computational efficiency and near-instant outputs. Many deep neural models have achieved optimal performance in terms of speech quality and intelligibility. However, formulating efficient and compact deep neural models for real-time processing on resource-limited devices remains a challenge. This study presents a compact neural model designed in a complex frequency domain for speech enhancement, optimized for resource-limited devices. The proposed model combines convolutional encoder–decoder and recurrent architectures to effectively learn complex mappings from noisy speech for real-time speech enhancement, enabling low-latency causal processing. Recurrent architectures such as Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU), and Simple Recurrent Unit (SRU), are incorporated as bottlenecks to capture temporal dependencies and improve the performance of SE. By representing the speech in the complex frequency domain, the proposed model processes both magnitude and phase information. Further, this study extends the proposed models and incorporates attention-gate-based skip connections, enabling the models to focus on relevant information and dynamically weigh the important features. The results show that the proposed models outperform the recent benchmark models and obtain better speech quality and intelligibility. The proposed models show less computational load and deliver better results. This study uses the WSJ0 database where clean sentences from WSJ0 are mixed with different background noises to create noisy mixtures. The results show that STOI and PESQ are improved by 21.1% and 1.25 (41.5%) on the WSJ0 database whereas, on the VoiceBank+DEMAND database, STOI and PESQ are imp4.1% and 1.24 (38.6%) respectively. The extension of the models shows further improvement in STOI and PESQ in seen and unseen noisy conditions. © 2023 Elsevier B.V.",-,10.1016/j.specom.2023.103008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178019636&doi=10.1016%2fj.specom.2023.103008&partnerID=40&md5=e3fb9e47544e3cfe5945abb296b68a2c,2021
SQLDAF: SQL injection attack active defense system based on randomized method pool,"Data is the most valuable resource of the Internet, attackers often use SQL injection attacks to destroy the database in order to obtain important data information in the database, and today's attack scene is complex, dynamic, multi-channel, non-linear, the existing defense detection technology can not cope with unknown attacks, the existing instruction set randomi-zation method may be broken by force. Aiming at the above problems, an active defense system of SQL injection attack based on randomization method pool is proposed. The randomization method pool and parallel executor are introduced to build the system framework. The result is decided whether to forward to the database after the decision maker votes, which no longer depends on prior knowledge. The attacker cannot use the system information obtained before to carry out the next effective attack. The formal representation and experimental results show that this method can effectively defend against SQL injection attacks. © 2023 SPIE.",-,10.1117/12.2683433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188101566&doi=10.1117%2f12.2683433&partnerID=40&md5=817bd4ff7b685cb69126591c3c22be16,2024
Research on Query Task Fragmentation in the Scenario of Storage and Compute Separation,"The cloud-native database is one of the hottest topics in database research. Storage-compute separation architecture with cloud characteristics is designed to process complex and changeable workloads to reduce resource coupling for cloud-native databases. During processing query loads, the computing layer pushes the operator tasks in execution plans generated from loads down to the storage layer through the network. Query optimization techniques such as operator pushdown and parallel computing have been developed and perfected to improve query speed in storage and compute separation architecture. But at the same time, these technologies also increase the complexity of execution plans, leading to an increase in tasks. Moreover, the experiment shows that under the same query load, the cloud-native database executes a significantly greater number of tasks compared to the distributed database. Given the above phenomenon, the concept of query task fragmentation is proposed. From the view of operator pushdown and parallel computing query optimization, the presence of query task fragmentation is verified through experiments. Experimental results based on the TPC-H benchmark demonstrate that, compared to MySQL distributed cluster, the average number of operator tasks in the execution plan, generated through operator pushdown and parallel computing for the cloud-native databases, increases by 20.17% and 39.49%, respectively. Finally, the impact of task fragmentation is analyzed from the perspectives of technical research and the application of cloud-native databases.  © 2023 IEEE.",-,10.1109/ICNGN59831.2023.10396786,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185005860&doi=10.1109%2fICNGN59831.2023.10396786&partnerID=40&md5=5cf6e1c30852127b55c183bd61f1821f,2024
Study on the Construction of Accounting Degree case Base at Home and Abroad in the Metauniverse era,"This paper studies the construction of accounting degree case base at home and abroad in the metauniverse era. Firstly, it analyses the significance of the construction of accounting degree case base. By the end of 2021, the number of master of professional accounting training institutions in China will have increased from 24 to 298. At the same time, it explains the connotation and main technology of meta-universe. Metacosmic technologies include 5G, 6G, cloud computing, edge computing, Internet of things, holographic projection, digital twinning, block chain, VR/AR/MR/XR and other technologies. Secondly, it analyses the development process and characteristics of Chicago case, Harvard case and Ivey case in Canada. The use of Harvard case allows to solve complex management problems compared to Chicago style. The 8-step strategy for case development at Ivey Business School in Canada includes three styles, two parts and seven parts of case writing. In 1998, the case of Ivey Business School Press has been distributed to more than 100 universities and enterprises in 20 countries. This paper analyses the shortcomings of case study courses and case construction of accounting major in China. Thirdly, this paper puts forward the enlightenment of foreign case teaching to the training of accounting talents: Based on the experience of case studies in developed countries, pay attention to the difference between accounting case teaching and accounting case teaching; advocate the general idea of ""four-dimensional""accounting case development. Finally, the paper constructs the technical framework model of the construction of the case database of accounting professional degree in the meta-universe. The research conclusion shows that, under the background of meta-universe, the case database ecology of accounting professional database is built on the basis of meta-universe equipment and meta-universe underlying technology, to construct the ecological map of the original universe accounting case database, and on this basis to carry out the meta-universe accounting case database comprehensive intelligent analysis system, digital mining, case recommendation and other services. Improve the accounting professional database case use accuracy, enhance the sense of experience. The organic combination of meta-universe and accounting case database construction will inject new vitality into the teaching reform of professional degree postgraduates. The deep integration of foreign case study experience and meta-universe new technology will certainly bring about subversive changes in teaching research. Meta-universe can make contributions to accounting case teaching. Metauniverse changes the traditional accounting case teaching mode. Firstly, the application of meta-universe technology in accounting case teaching can enhance students' sense of teaching experience and give them an immersive feeling. Secondly, meta-universe technolog has also promoted the transformation of accounting case teaching from traditional teaching methods to information-based teaching methods. Accounting case teaching can not only simulate the offline teaching mode through metacosmic technology, but also use the characteristics of virtual world to completely change the teaching mode of accounting case teaching. By using meta-universe experiential accounting case teaching, we can improve the teaching effectiveness, solve the shortage of accounting case teachers, and meet the needs of students to learn accounting cases at any time.  © 2023 ACM.",32-40,10.1145/3594441.3594448,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171303499&doi=10.1145%2f3594441.3594448&partnerID=40&md5=349aa3ae1090f140a3869c9ff7387572,2023
Indoor Fingerprint Positioning Method Based on Real 5G Signals,"Indoor positioning services are being used more and more widely. However, existing indoor positioning techniques cannot simultaneously take into account low cost, ease of use, high precision, and seamless switching between indoor and outdoor positioning. With the maturity of 5G techniques, 5G-based indoor positioning is gradually being paid attention to. 5G-based indoor positioning does not require additional equipment, and supports flexible indoor and outdoor switching under the same system. However, the 5G-related information used in existing research on 5G indoor positioning is not open to users. Therefore, in this paper, we propose an indoor fingerprint positioning method based on measured 5G signals. This method first collects 5G signals in the positioning area, and processes them to form a fingerprint database. Then, a machine learning algorithm is used to match the signal to be located with the fingerprint database to obtain the positioning result. Finally, we conduct experiments in real field, and the experimental result demonstrates that the positioning accuracy of our proposed method can reach 96%.  © 2023 ACM.",205-210,10.1145/3583788.3583819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162730935&doi=10.1145%2f3583788.3583819&partnerID=40&md5=d6955b6fe9426776e4a4d77fb8c77bfa,2023
GLIN: A (G)eneric (L)earned (In)dexing Mechanism for Complex Geometries,"Although spatial indexes shorten the query response time, they rely on complex tree structures to narrow down the search space. Such structures in turn yield additional storage overhead and take a toll on index maintenance. Recently, there have been a flurry of efforts attempting to leverage Machine-Learning (ML) models to simplify the index structures. However, existing geospatial indexes can only index point data rather than complex geometries such as polygons and trajectories that are widely available in geospatial data. As a result, they cannot efficiently and correctly answer geometry relationship queries. This paper introduces GLIN, an indexing mechanism for spatial relationship queries on complex geometries. To achieve that, GLIN transforms geometries to Z-address intervals, and then harnesses an existing order-preserving learned index to model the cumulative distribution function between these intervals and the record positions. The lightweight learned index greatly reduces indexing overhead and provides faster or comparable query latency. Most importantly, GLIN augments spatial query windows to support queries exactly for common spatial relationships. Our experiments on real-world and synthetic datasets show that GLIN has 80%-90% lower storage overhead than Quad-Tree and 60% -80% than R-tree and 30% - 70% faster query on medium selectivity. Moreover, GLIN's maintenance throughput is 1.5 times higher on insertion and 3 - 5 times higher on deletion. © 2023 ACM.",1-12,10.1145/3615833.3628590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178648425&doi=10.1145%2f3615833.3628590&partnerID=40&md5=d5819e2f83f653016bf54e19ad181e2c,2023
"Better Cardinality Estimators for HyperLogLog, PCSA, and beyond","Cardinality Estimation (aka Distinct Elements) is a classic problem in sketching with many applications in databases, networking, and security. Although sketching algorithms are fairly simple, analyzing the cardinality estimators is notoriously difficult, and even today the analyses of state-of-The-Art sketches like HyperLogLog and ¶CSA are not very accessible. In this paper we introduce a new class of estimators called τ-Generalized-Remaining-Area estimators, as well as a dramatically simpler approach to analyzing estimators. The estimators of Durand and Flajolet, Flajolet et al., and Lang can be seen as τ-GRA estimators for integer values of τ. By using fractional values of τ we derive improved estimators for HyperLogLog and ¶CSA whose variance comes very close to the Crame r-Rao lower bounds. We also derive τ-GRA-based estimators for the class of Curtain sketches introduced by Pettie, Wang, and Yin, which can be seen as a hybrid of HyperLogLog and ¶CSA with a more attractive simplicity-Accuracy tradeoff than both.  © 2023 ACM.",317-327,10.1145/3584372.3588680,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164257691&doi=10.1145%2f3584372.3588680&partnerID=40&md5=0cc4eb7c58467bedaaae171dc43ffbae,2023
IMSF-Net:An improved multi-scale information fusion network for PPG-based blood pressure estimation,"Recently, deep learning (DL) architectures have been widely used for PPG-based blood pressure (BP) monitoring due to their powerful feature extraction ability. However, the DL methods still suffer from limitations. First, the multi-scale time dependency characteristics of PPG require the DL-based methods to have the ability of sufficient feature representation at various scales. Second, the lack of DL model interpretability can lead to concerns about the reliability and generalizability of prediction. In this study, a new PPG-based end-to-end deep learning method, referred to as the IMSF-Net, is proposed for BP estimation, and its advantages are as follows: (1) the improved multi-scale fusion (IMSF) block is designed to extract different-scale feature in a two-channel way, which can obtain the abstract high-level information while avoiding low-level information loss simultaneously; (2) Permutation importance algorithm is firstly adopted to determine what parts of PPG are most useful for automatic BP estimation. On the University of California, Irvine database, the proposed IMSF-Net achieves mean absolute error (MAE) ± standard deviation (STD) of 2.07 ± 2.32 mmHg for systolic blood pressure (SBP) and 1.21 ± 1.51 mmHg for diastolic blood pressure (DBP). On the healthy database, referred to as the VDB, the method achieves MAE ± STD of 1.75 ± 3.98 mmHg for SBP and 1.29 ± 2.3 mmHg for DBP. Both of the performances outperform related previous works. The model interpretation results show that feature grids comprising of Diastolic 1–3, and Systolic 5, which generally cover the waveform from the peak point to the dicrotic notch, have a strong correlation with BP estimation. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179134723&doi=10.1016%2fj.bspc.2023.105791&partnerID=40&md5=6959f49395a93bda8b169476b69cda65,2023
Semantic-Based SQL Injection Detection Method,"SQL Injection Attack (SQLIA) is one of the main attack vectors against databases, which exploits a vulnerability that user-input data is executed in the database to maliciously concatenate SQL query statements for the purpose of illegally querying the database, posing a significant threat to web security. Due to the fact that there are many similarities between SQL injection statements and normal HTTP requests in terms of numerous identical fields, directly matching keywords by computers can result in a considerable amount of misjudgment and missed judgments. Therefore, this article proposes a semantic-based SQL injection attack detection method that allows computers to determine the semantics of the statement to determine whether it is a normal HTTP request or a SQL injection attack statement. This article analyzes the similarities and differences between attack statements at various stages of SQL injection and normal HTTP requests, and proposes a preprocessing method that can extract effective attack keywords based on this. Additionally, combined with natural language processing, word2vec, and a support vector machine (SVM) classification algorithm, a set of semantic-based SQL injection attack detection methods were constructed. These methods were tested using experiments on a local CMS with known SQL injection vulnerabilities. The experimental results demonstrate that compared to directly using traditional SQL injection attack detection methods based on machine learning algorithms such as TF-IDF or word2vec, this method has significant improvement in the detection results.  © 2023 IEEE.",519-524,10.1109/ICAICA58456.2023.10405528,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185881085&doi=10.1109%2fICAICA58456.2023.10405528&partnerID=40&md5=3125bac923aa6782ee5aace19abb0fcd,2023
ReferPose: Distance Optimization-Based Reference Learning for Human Pose Estimation and Monitoring,"Existing deep learning models for human pose estimation (HPE) have shown satisfactory performance in monitoring human actions. However, they usually face a dilemma between complexity and accuracy. To address this challenge, we propose an effective reference learning method for HPE (namely ReferPose), which is based on a new distance optimization strategy. Specifically, we utilize a reference model for pose learning and representation. The pose representation learned from the entire database is merged into the reference model, providing continuous reference learning guidance for an in-training model. In addition, we design a new cosine annealing-based reference guidance for temporal denoising and further develop a distance optimization strategy to provide joint guidance from pose knowledge, model representation, and temporal experience. Experimental results on two benchmark databases and a human fall monitoring system demonstrate that our ReferPose not only achieves promising accuracy improvement compared with several representative HPE models, but also offers low cost and high efficiency.  © 2005-2012 IEEE.",4440-4450,10.1109/TII.2023.3324940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181568618&doi=10.1109%2fTII.2023.3324940&partnerID=40&md5=eca700e393eccaab6518577538e4bc33,2021
Rising of Retracted Research Works and Challenges in Information Systems: Need New Features for Information Retrieval and Interactions,"This perspective paper analyzes the rising threat of retracted scientific works and the challenges of preventing the continued spreading and use of the retracted science; further, a framework is proposed for research and actions to effectively manage retractions in the information ecosystem. The precipitous increase in retractions of scientific publications is real and the complexity of retracting publications challenges current IR systems and people's information behaviors. Retracting published, especially peer-reviewed, papers in prestigious venues is a complex phenomenon involving various entities through often time-consuming processes. These publications may be accessible from the original venues, digital archives, or free-access databases, but these systems differ in retrievability and output. Many systems do not identify the retractions or reasons for retractions; most systems do not treat the retracted paper and its related notices (retraction or correction) as an integrated entity. Studies found that many retracted publications continue to be cited post-retraction as valid science. A new threat is the widely spread of retracted publications on social media. Retracting invalid scientific publications has serious implications in the real world. Based on current findings, we propose (1) a framework for further research; (2) a DOI resolution to integrate the documents related to retraction/correction; (3) a structured facet taxonomy for representing and indexing the retracted, corrected, or republished publications in databases; (4) a retraction registry or database with personalized AI helper for researchers to tract retracted publications; (5) an approach for understanding how retracted publications are circulated on social media.  © 2023 ACM.",69-82,10.1145/3576840.3578281,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151481252&doi=10.1145%2f3576840.3578281&partnerID=40&md5=e07d1cd6600e8d33a04d562a1ae02192,2023
Half-Xor: A Fully-Dynamic Sketch for Estimating the Number of Distinct Values in Big Tables,"Calculating the number of distinct values (i.e., NDV) in a column of a big table is costly yet fundamental to a variety of database applications such as data compression and profiling. To reduce the high time and space cost, a number of sketch methods (e.g., HyperLogLog) have been proposed, which estimate the NDV from a constructed compact data summary of distinct values. However, these methods fail or are costly to manage fully-dynamic scenarios where data is often inserted into and deleted from the table. To solve this issue, we propose a novel sketch method, <italic>Half-Xor</italic>. Our Half-Xor sketch consists of a compact bit matrix and a small counter array, and it needs to set a few bits and update a counter when handling a data insertion/deletion. Compared with the state-of-the-art mergeable method, our experimental results demonstrate that our method Half-Xor is up to 6.6 times more accurate under the same memory usage and reduces the memory usage by up to 16 times to achieve the same estimation accuracy. IEEE",1-14,10.1109/TKDE.2024.3359710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184312696&doi=10.1109%2fTKDE.2024.3359710&partnerID=40&md5=80300ed12e3ac91483461d982e0058e0,2023
Graph-Diffusion-Based Domain-Invariant Representation Learning for Cross-Domain Facial Expression Recognition,"The precondition that most of the existing facial expression recognition (FER) algorithms have succeeded lies in that the training (source) and test (target) samples are independent of each other and identically distributed. However, it is too strict to satisfy this precondition in the real-world. To this end, we propose a novel graph-diffusion-based domain-invariant representation learning (GDRL) model for the cross-domain FER scenario where there exist distribution shifts between various domains. Specifically, a low-dimensional space mapping strategy is first adopted to diminish the domain mismatch. Then, by skillfully combining the local graph embedding and affinity graph diffusion, the local geometric structures can be effectively modeled and the deeper higher-order relationships of samples from various domains can be captured. In addition, in order to better guide the transfer process and learn a more discriminative and invariant representation, we take into account the label consistency. Experimental results on four laboratory-controlled databases and two in-the-wild databases demonstrate that our proposed model can yield better recognition performance compared with state-of-the-art domain adaptation methods. IEEE",1-12,10.1109/TCSS.2024.3355113,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184320874&doi=10.1109%2fTCSS.2024.3355113&partnerID=40&md5=cd3b0ee8c2b22a3f5c9abc7181117dcd,2021
Join Sampling Under Acyclic Degree Constraints and (Cyclic) Subgraph Sampling,"Given a (natural) join with an acyclic set of degree constraints (the join itself does not need to be acyclic), we show how to draw a uniformly random sample from the join result in O(polymat/max{1, OUT}) expected time (assuming data complexity) after a preprocessing phase of O(IN) expected time, where IN, OUT, and polymat are the join's input size, output size, and polymatroid bound, respectively. This compares favorably with the state of the art (Deng et al. and Kim et al., both in PODS'23), which states that, in the absence of degree constraints, a uniformly random sample can be drawn in Õ(AGM/max{1, OUT}) expected time after a preprocessing phase of Õ(IN) expected time, where AGM is the join's AGM bound and Õ(.) hides a polylog(IN) factor. Our algorithm applies to every join supported by the solutions of Deng et al. and Kim et al. Furthermore, since the polymatroid bound is at most the AGM bound, our performance guarantees are never worse, but can be considerably better, than those of Deng et al. and Kim et al. We then utilize our techniques to tackle directed subgraph sampling, a problem that has extensive database applications and bears close relevance to joins. Let G = (V, E) be a directed data graph where each vertex has an out-degree at most λ, and let P be a directed pattern graph with a constant number of vertices. The objective is to uniformly sample an occurrence of P in G. The problem can be modeled as join sampling with input size IN = Θ(|E|) but, whenever P contains cycles, the converted join has cyclic degree constraints. We show that it is always possible to throw away certain degree constraints such that (i) the remaining constraints are acyclic and (ii) the new join has asymptotically the same polymatroid bound polymat as the old one. Combining this finding with our new join sampling solution yields an algorithm to sample from the original (cyclic) join (thereby yielding a uniformly random occurrence of P) in O(polymat/max{1, OUT}) expected time after O(|E|) expected-time preprocessing, where OUT is the number of occurrences. © Ru Wang and Yufei Tao.",-,10.4230/LIPIcs.ICDT.2024.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188587626&doi=10.4230%2fLIPIcs.ICDT.2024.23&partnerID=40&md5=5909afcef686438e53f99f51a4d4430a,2021
A novel finer soil strength mapping framework based on machine learning and remote sensing images,"Soil strength is an important factor for assessing the vehicle trafficability in the wilds and making reliable off-road path planning. Rating Cone index (RCI) has been widely used as an indicator of soil strength for mobility assessment. Currently, regional RCI are mainly obtained by using Soil Moisture Strength Prediction (SMSP) Model based on the Unified Soil Classification System (USCS) soil types and soil moisture at critical depth. However, USCS classification is inaccessible directly in most soil databases. And current soil moisture products are of coarse spatial resolution and only reliable in soil surface layer. It is costly to arrange ground survey for USCS soil type and the fine soil moisture in deeper layers cannot be obtained directly. To fill this gap, a novel framework is proposed to generate finer RCI-based soil strength map. To gain USCS soil classification in an un-surveyed study site, RF, GBDT, XGBoost and LCE ensemble learning models were trained with data from gSSURGO Database which contains USCS soil types and soil properties, and then make prediction on data from SoilGrids Database by using same properties. Then, to obtain the finer soil moisture of critical depth, these tree-based models were constructed with ground observation data, Sentinel-2 images, topographical data and soil properties with higher spatial resolution. The SMSP model is finally used to generate the finer daily soil strength map for the study site. The proposed framework highly improved the resolution and reliability of existing soil strength map generating methods, and can provide detailed soil strength information. © 2023 The Authors",-,10.1016/j.cageo.2023.105479,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175628125&doi=10.1016%2fj.cageo.2023.105479&partnerID=40&md5=91a8492379e7dbd7d08454903d97308e,2024
APD-229: a textual-visual database for agricultural pests and diseases,"The damage caused by agricultural pests and diseases has brought huge losses to the economy. Rapid recognition and timely treatment can minimize economic losses. Most of the existing image databases are produced in laboratories, where the shooting costs are expensive, and the background of these images are very different from the real farmland environment. Moreover, although the existing recognition systems can locate entities, they cannot provide discriminative evidence which is semantically interpretable, which makes it difficult for them to distinguish entities with very similar appearances. Fortunately, there are text descriptions in professional agricultural control documents that can clearly distinguish similar entities. In this paper, a textual-visual database for agricultural pests and diseases named APD-229 is constructed. The goal of APD-229 is to learn prior knowledge that can distinguish similar entities from the control documents, and to guide the image recognition system to complete the task of fine-grained classification. The database contains two sub-databases: pest set and disease set. A total of 121,213 images and 8,209 text descriptions belong to 229 categories. Furthermore, extensive experiments were carried out on APD-229, results show that in the single-modal image classification task, the accuracy of pest database is 75.15% and the accuracy of disease database is 61.23%. While in the multi-modal image classification task, the accuracy is 78.74% and 71.67% respectively. Compare with the single-model experiment, the accuracy of multi-model is improved by 4.78% and 17% respectively. APD-229 is publicly available at https://github.com/SDUST-MMML/APD-229. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",22189-22220,10.1007/s11042-023-15393-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168123403&doi=10.1007%2fs11042-023-15393-y&partnerID=40&md5=ee1e4f4f9493303f76bb1cc76b21be6c,2022
A multi-module algorithm for heartbeat classification based on unsupervised learning and adaptive feature transfer,"The scarcity of annotated data is a common issue in the realm of heartbeat classification based on deep learning. Transfer learning (TL) has emerged as an effective strategy for addressing this issue. However, current TL techniques in this realm overlook the probability distribution differences between the source domain (SD) and target domain (TD) databases. The motivation of this paper is to address the challenge of labeled data scarcity at the model level while exploring an effective method to eliminate domain discrepancy between SD and TD databases, especially when SD and TD are derived from inconsistent tasks. This study proposes a multi-module heartbeat classification algorithm. Initially, unsupervised feature extractors are designed to extract rich features from unlabeled SD and TD data. Subsequently, a novel adaptive transfer method is proposed to effectively eliminate domain discrepancy between features of SD for pre-training (PTF-SD) and features of TD for fine-tuning (FTF-TD). Finally, the adapted PTF-SD is employed to pre-train a designed classifier, and FTF-TD is used for classifier fine-tuning, with the objective of evaluating the algorithm's performance on the TD task. In our experiments, MNIST-DB serves as the SD database for handwritten digit image classification task, MIT-DB as the TD database for heartbeat classification task. The overall accuracy of classifying heartbeats into normal heartbeats, supraventricular ectopic beats (SVEBs), and ventricular ectopic beats (VEBs) reaches 96.7 %. Specifically, the sensitivity (Sen), positive predictive value (PPV), and F1 score for SVEBs are 0.802, 0.701, and 0.748, respectively. For VEBs, Sen, PPV, and F1 score are 0.976, 0.840, and 0.903, respectively. The results indicate that the proposed multi-module algorithm effectively addresses the challenge labeled data scarcity in heartbeat classification through unsupervised learning and adaptive feature transfer methods. © 2024",-,10.1016/j.compbiomed.2024.108072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183941446&doi=10.1016%2fj.compbiomed.2024.108072&partnerID=40&md5=078990ea91939db32b09cadf2206148a,2024
Learned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN,"Density-based clustering is a commonly used tool in data science. Today many data science works are utilizing high-dimensional neural embeddings. However, traditional density-based clustering techniques like DBSCAN have a degraded performance on high-dimensional data. In this paper, we propose LAF, a generic learned accelerator framework to speed up the original DBSCAN and the sampling-based variants of DBSCAN on high-dimensional data with angular distance metric. This framework consists of a learned cardinality estimator and a post-processing module. The cardinality estimator can fast predict whether a data point is core or not to skip unnecessary range queries, while the post-processing module detects the false negative predictions and merges the falsely separated clusters. The evaluation shows our LAF-enhanced DBSCAN method outperforms the state-of-the-art efficient DBSCAN variants on both efficiency and quality. © 2023 Copyright held by the owner/author(s)",492-498,10.48786/edbt.2023.42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165125025&doi=10.48786%2fedbt.2023.42&partnerID=40&md5=15a9b519fb0983460596e0b47c5310b8,2023
Integrating Non-Volatile Main Memory in a Deterministic Database,"Deterministic databases provide strong serializability while avoiding concurrency-control related aborts by establishing a serial ordering of transactions before their execution. Recent work has shown that they can also handle skewed and contended workloads effectively. These properties are achieved by batching transactions in epochs and then executing the transactions within an epoch concurrently and deterministically. However, the predetermined serial ordering of transactions makes these databases more vulnerable to long-latency transactions. As a result, they have mainly been designed as main-memory databases, which limits the size of the datasets that can be supported. We show how to integrate non-volatile main memory (NVMM) into deterministic databases to support larger datasets at a lower cost per gigabyte and faster failure recovery. We describe a novel dual-version checkpointing scheme that takes advantage of deterministic execution, epoch-based processing and NVMM’s byte addressability to avoid persisting all updates to NVMM. Our approach reduces NVMM accesses, provides better access locality, and reduces garbage collection costs, thus lowering the performance impact of using NVMM. We show that our design enables scaling the dataset size while reducing the impacts of using NVMM, achieving up to 79% of DRAM performance. Our design supports efficient failure recovery and outperforms alternative failure recovery designs, especially under contended workloads, by up to 56%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",672-686,10.1145/3552326.3567494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160206346&doi=10.1145%2f3552326.3567494&partnerID=40&md5=4f92845bd474b5f1b9cdf56e8d16fa47,2022
Quartet: A Query Aware Database Adaptive Compilation Decision System[Formula presented],"The executor is an important component of a database. Typical executors that are applied in modern database systems follow either the VOLCANO model or Compiled model, each of which fits some scenarios but not all. Even the widely employed PostgreSQL (PGSQL) and CockroachDB (CRDB) have to rely on human experts to achieve the optimal execution mode. Nevertheless, the accuracy of these decisions is only 32.8% on average, even with an expert involved. Moreover, due to the exclusive use of rule-based strategies, it is not feasible to reasonably switch between two working modes when confronted with different queries. To solve this problem, we propose a QUery awARe daTabase adaptivE compilaTion decision system (Quartet), which can determine the most suitable execution mode with respect to the current workload at runtime. Quartet generates operator-cost and tree-based vectors by analysing the query execution plan (QEP) and then uses the fully connected neural network (FCNN) and tree-based convolutional neural network (TBCNN) to learn the relationship between the QEP and the optimal execution. Our evaluations show that Quartet can improve execution decision accuracy by 60% on average under TPC-H (under 3 GB) workloads. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.122841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180966976&doi=10.1016%2fj.eswa.2023.122841&partnerID=40&md5=62447c8f3eaff83bcaa36d767b7d80ee,2021
Lightweight Arrhythmia Detection Based on Momentum Contrast Learning,"Arrhythmia disease can be extremely damaging to the heart, and in severe cases can even lead to death. The ECG smart monitoring device is an effective way for detecting arrhythmia disease, and as wearable devices spreads, it also places certain requirement on lightweight arrhythmia detection algorithms. It is of great importance to implement an efficient arrhythmia detection algorithm with strong generalization performance. This work trains an arrhythmia detection model on the Georgia 12-lead ECG Challenge (G12EC) database and the China Physiological Signaling Challenge 2018 (CPSC2018) database using xResNet18 as the backbone network and momentum contrast learning as the framework, which allows contrast learning of positive samples and a large number of negative samples by introducing queue and momentum update encoder parameters to obtain a more comprehensive information representation. The model was pre-trained using the Georgia 12-lead ECG Challenge (G12EC) Database to obtain better characterization of initialization information and fine-tuned using the China Physiological Signal Challenge 2018 (CPSC2018) dataset. The experimental results showed that the model was effective with an AUC of 0.861, an Acc of 77.04% on the CPSC2018 database. © 2023 CinC.",-,10.22489/CinC.2023.164,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182323261&doi=10.22489%2fCinC.2023.164&partnerID=40&md5=2f2ce54adb379d4dbea24fc6ee6ad680,2021
Private Federated Submodel Learning via Private Set Union,"We consider the federated submodel learning (FSL) problem and propose an approach where clients are able to update the central model information theoretically privately. Our approach is based on private set union (PSU), which is further based on multi-message symmetric private information retrieval (MM-SPIR). The server has two non-colluding databases which keep the model in a replicated manner. With our scheme, the server does not get to learn anything further than the subset of submodels updated by the clients: the server does not get to know which client updated which submodel(s), or anything about the local client data. In comparison to the state-of-the-art private FSL schemes of Jia-Jafar and Vithana-Ulukus, our scheme does not require noisy storage of the model at the databases; and in comparison to the secure aggregation scheme of Zhao-Sun, our scheme does not require pre-distribution of client-side common randomness, instead, our scheme creates the required client-side common randomness via random symmetric private information retrieval (RSPIR) and one-time pads. Our system is initialized with a replicated storage of submodels and a sufficient amount of common randomness at the two databases on the server-side. The protocol starts with a common randomness generation (CRG) phase where the two databases establish common randomness at the client-side using RSPIR and one-time pads (this phase is called FSL-CRG). Next, the clients utilize the established client-side common randomness to have the server determine privately the union of indices of submodels to be updated collectively by the clients (this phase is called FSL-PSU). Then, the two databases broadcast the current versions of the submodels in the set union to clients. The clients update the submodels based on their local training data. Finally, the clients use a variation of FSL-PSU to write the updates back to the databases privately (this phase is called FSL-write). As the databases at the server do not communicate, as a novel approach, we utilize carefully chosen alive clients to route the required information between the two databases. Our proposed private FSL scheme is robust against client drop-outs, client late-arrivals, and database drop-outs.  © 1963-2012 IEEE.",2903-2921,10.1109/TIT.2023.3344717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181560076&doi=10.1109%2fTIT.2023.3344717&partnerID=40&md5=06ed17a376ae85f806e539ba40a4db67,2021
Privacy-Preserving WiFi Localization Based On Inner Product Encryption in a Cloud Environment,"Cloud-based indoor positioning services have advantages over non-cloud methods but also confront serious privacy concerns. Existing privacy-preserving schemes are designed for conventional two-entity localization models thus not applicable to the cloud-based indoor positioning services involving three entities. In addition, these methods incur high computational and communication overhead. To tackle these issues, we proposed a privacy-preserving indoor positioning scheme for WiFi localization based on Inner Product Encryption in a cloud environment. A bloom filter constructed with Locality Sensitive Hashing was designed to map WiFi fingerprints from Euclidean to inner product space with the distance relationships maintained for converting the location estimation to inner product calculations. Inner Product Encryption protects the user&#x2019;s fingerprint and database information held by the positioning service provider. Fingerprint similarity as determined by the inner product is decrypted on the cloud to retrieve the closest encrypted location coordinates for users. In addition, a retrieval structure based on Hierarchical Navigable Small World graph was designed to improve efficiency. Theoretical analysis and experimental results demonstrate that the scheme has low computational and communication overhead while ensuring security and not significantly degrading the localization accuracy. Moreover, the overhead does not increase significantly with database size thus this approach is highly scalable. IEEE",1-1,10.1109/JIOT.2024.3358349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183978186&doi=10.1109%2fJIOT.2024.3358349&partnerID=40&md5=b952db0617ee2297dc1c3d979802b7a0,2024
Explainable deep learning for image-driven fire calorimetry,"The rapid advancement of deep learning and computer vision has driven the intelligent evolution of fire detection, quantification and fighting, although most AI models remain opaque black boxes. This work applies explainable deep learning methods to quantify fire power by flame images and aims to elucidate the underlying mechanism of computer-vision fire calorimetry. The process begins with the utilization of a pre-trained fire segmentation model to create a flame image database in various formats: (1) original RGB, (2) background-free RGB, (3) background-free grey, and (4) background-free binary. This diverse database accounts for factors such as background, color, and brightness. Then, the synthetic database is employed to train and test the fire-calorimetry AI model. Results highlight the dominant role of determining flame area in fire calorimetry, while other factors displaying minimal influence. Enhancing the accuracy of flame segmentation significantly reduces error in computer-vision fire calorimetry to less than 20%. Finally, the study incorporates the Gradient-weighted Class Activation Mapping (Grad-CAM) method to visualize the pixel-level contribution to fire image identification. This research deepens the understanding of vision-based fire calorimetry and providing scientific support for future AI applications in fire monitoring, digital twin, and smart firefighting. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",1047-1062,10.1007/s10489-023-05231-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180700753&doi=10.1007%2fs10489-023-05231-x&partnerID=40&md5=222eb3c54e3e679c1c589760fbfc3c57,2024
An Integrated Transfer Learning Method for Power Generation Prediction of Run-Off Small Hydropower in Data-Scarce Areas,"Historical data scarce and varying patterns of new built run-off small hydropower (RSHP) limits precise power generation prediction. Unforeseen hydropower can induce uneconomic power grid operations. To address this issue, a novel transfer learning method enabling integration of public RSHP knowledge is proposed. First, a RSHP data matching algorithm is proposed to pre-filter similar source domain data and produce a RSHP database matching patterns of target RSHP. This algorithm allows us to improve performance of transfer learning model. Next, public prediction knowledge implicated in the RSHP database is learned towards a CNN-BiLSTM hybrid pre-trained network. Then, the pre-trained network is transferred to the target RSHP prediction models by hyper-parameter fine-tuning algorithm, which reduces divergence between the pre-trained network outputs and the target domain data. As a result, accurate new RSHP prediction models can be generated under the challenge of data lack. At the last, the RSHP prediction models are fed back to the fine-tuning algorithm such that generalizability of the models enables life-long self-renewal. The real-world case demonstrates the superiority of the proposed method in terms of accuracy and data utilization. The average prediction error of the proposed method is 16.27% lower than the best traditional alternative.  © 2010-2012 IEEE.",1030-1041,10.1109/TSG.2023.3276390,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160279155&doi=10.1109%2fTSG.2023.3276390&partnerID=40&md5=395e17cad40dbf1bdb0d51376b06921d,2020
Authentibility Pass: An Accessible Authentication Gateway for People with Reduced Abilities,"It is important that authentication mechanisms are inclusive to all users including those with reduced abilities. This paper describes our work on Authentibility Pass that allows users with physical and cognitive disabilities to communicate their authentication and accessibility requirements to organizations, including e-businesses, thus ensuring that authentication is accessible. In the initial development phase, a Business Model canvas was created for the Authentibility concept, where the key partners, activities, cost structure and revenue streams were established. This contributed to a value proposition canvas to highlight the customers' tasks, gains, and pains, as well as how Authentibility Pass will mitigate the pains. The second market validation phase is then described consisting of 30 minute interviews (N=9) with key stakeholders, such as higher education institutions, charities and financial institutions. From our findings, it was identified that people with reduced abilities need to repeatedly inform organizations or e-businesses of their requirements. The Authentibility Pass Proof of Concept was subsequently developed comprising of an Android Application, database, web interface and Application Programming Interface (API). User requirements are entered through the Application, including authentication checks for verification. The user's accessibility and authentication requirements are communicated using token-based authentication and stored in organization databases. The API is designed for organizations and e-businesses with existing database systems. The Business Model and Value Proposition canvases, market validation results, user interface designs and preliminary evaluation results are discussed, including suggestions for future work. Authentibility Pass will increase the awareness of organizations to customer requirements with disabilities, resulting in higher levels of satisfaction.  © 2023 IEEE.",155-162,10.1109/ICEBE59045.2023.00043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183022521&doi=10.1109%2fICEBE59045.2023.00043&partnerID=40&md5=fbb736b2fdbcd010f5590174de156de4,2023
FF-BERT: A BERT-based ensemble for automated classification of web-based text on flash flood events,"The web is a rich information repository that can be mined to uncover additional data about past flash flood (FF) events, currently missing from existing structured databases. However, this information originates from multiple sources (news articles, government records, and weather records among others) and may cover several topics. Furthermore, these topics may be disproportionately covered on the web. The large size and heterogenous nature of web information render manual review difficult. To address this challenge, we have developed a multi-label text classification model, FF-BERT. FF-BERT is designed to classify FF-related web paragraphs into one or more of seven categories: (1) Damage and Economic Impact (DI), (2) Fatalities, Injuries, and Rescue (FIR), (3) Hydrometeorology (HM), (4) Warning and Emergency (WE), (5) Response and Recovery (RR), (6) Public Health (PH), and (7) Mitigation (MG). To develop FF-BERT, we labeled 21,180 paragraphs from FF-related webpages and performed experiments with multiple model architectures based on the widely used language model Bidirectional Encoder Representation from Transformers (BERT). Our final model outperforms the baseline by 11.83%, as measured by the micro-F1 score. In addition, FF-BERT significantly improves the prediction of minority labels (RR-32.1%, PH-260.4%, and MG-138.6%). We demonstrate using real world examples that FF-BERT can be used to uncover new information about flash flood events. This information can be used to enhance existing databases, such as NOAA's Storm Events Database. © 2023 Elsevier Ltd",-,10.1016/j.aei.2023.102293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178656911&doi=10.1016%2fj.aei.2023.102293&partnerID=40&md5=b8703d8466c2f86cd08ad10ba45a45ab,2024
Fragment databases from screened ligands for drug discovery (FDSL-DD),"Fragment-based drug design (FBDD) is one major drug discovery method employed in computer-aided drug discovery. Due to its inherent limitations, this process experiences long processing times and limited success rates. Here we present a new Fragment Databases from Screened Ligands Drug Design method (FDSL-DD) that intelligently incorporates information about fragment characteristics into a fragment-based design approach to the drug development process. The initial step of the FDSL-DD is the creation of a fragment database from a library of docked, drug-like ligands for a specific target, which deviates from the traditional in silico FBDD strategy, incorporating structure-based design screening techniques to combine the advantages of both approaches. Three different protein targets have been tested in this study to demonstrate the potential of the created fragment library and FDSL-DD. Utilizing the FDSL-DD led to an increase in binding affinity for each protein target. The most substantial increase was exhibited by the ligand designed for TIPE2, with a 3.6 kcalmol-1 difference between the top ligand from the FDSL-DD and top ligand from the high throughput virtual screening (HTVS). Using drug-like ligands in the initial HTVS allows for a greater search of chemical space, with higher efficiency in fragments selection, less grid boxes, and potentially identifying more interactions. © 2023 Elsevier Inc.",-,10.1016/j.jmgm.2023.108669,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181692663&doi=10.1016%2fj.jmgm.2023.108669&partnerID=40&md5=5c01a0f13c58eecd5056d6725337ac88,2022
"Quantum Machine Learning: Foundation, New Techniques, and Opportunities for Database Research","In the last few years, the field of quantum computing has experienced remarkable progress. The prototypes of quantum computers already exist and have been made available to users through cloud services (e.g., IBM Q experience, Google quantum AI, or Xanadu quantum cloud). While fault-tolerant and large-scale quantum computers are not available yet (and may not be for a long time, if ever), the potential of this new technology is undeniable. Quantum algorithms have the proven ability to either outperform classical approaches for several tasks, or are impossible to be efficiently simulated by classical means under reasonable complexity-theoretic assumptions. Even imperfect current-day technology is speculated to exhibit computational advantages over classical systems. Recent research is using quantum computers to solve machine learning tasks. Meanwhile, the database community has already successfully applied various machine learning algorithms for data management tasks, so combining the fields seems to be a promising endeavour. However, quantum machine learning is a new research field for most database researchers. In this tutorial, we provide a fundamental introduction to quantum computing and quantum machine learning and show the potential benefits and applications for database research. In addition, we demonstrate how to apply quantum machine learning to the join order optimization problem in databases.  © 2023 ACM.",45-52,10.1145/3555041.3589404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162896901&doi=10.1145%2f3555041.3589404&partnerID=40&md5=8b9ee512808ba1315d83af1c0553702c,2023
Network Big Data Visualization Information System,"This article discusses a network Big data visualization information system based on a three-tier architecture, namely database system layer, data exchange layer, and operation layer. Among them, the database system layer is responsible for collecting, integrating, managing, maintaining, updating, etc., and can effectively protect, transmit, share, query, etc; The data exchange layer, on the other hand, is a web server that enables fast communication between multiple servers, making communication between servers more convenient and orderly. The WEB front-end is a complex platform that consists of underlying logical processing, information system transmission, communication, information storage, user permission settings, system configuration, WEB interface layout, visual display of information, programming of various functions, and program execution. Together, they form the core of the entire platform.  © 2023 IEEE.",553-557,10.1109/ICEDCS60513.2023.00105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182590263&doi=10.1109%2fICEDCS60513.2023.00105&partnerID=40&md5=ce2bd777d706cc7f2e595100b1abc4a2,2021
Contact-Free Atrial Fibrillation Screening with Attention Network,"Atrial Fibrillation (AF) screening from face videos has become popular with the trend of telemedicine and telehealth in recent years. In this study, the largest facial image database for camera-based AF detection is proposed. There are 657 participants from two clinical sites and each of them is recorded for about 10 minutes of video data, which can be further processed as over 10,000 segments around 30 seconds, where the duration setting is referred to the guideline of AF diagnosis. It is also worth noting that, 2,979 segments are segment-wise labeled, that is, every rhythm is independently labeled with AF or not. Besides, all labels are confirmed by the cardiologist manually. Various environments, talking, facial expressions, and head movements are involved in data collection, which meets the situations in practical usage. Specific to camera-based AF screening, a novel CNN-based architecture equipped with an attention mechanism is proposed. It is capable of fusing heartbeat consistency, heart rate variability derived from remote photoplethysmography, and motion features simultaneously to reliable outputs. With the proposed model, the performance of intra-database evaluation comes up to 96.62&#x0025; of sensitivity, 90.61&#x0025; of specificity, and 0.96 of AUC. Furthermore, to check the capability of adaptation of the proposed method thoroughly, the cross-database evaluation is also conducted, and the performance also reaches about 90&#x0025; on average with the AUCs being over 0.94 in both clinical sites. IEEE",1-12,10.1109/JBHI.2024.3368049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187006186&doi=10.1109%2fJBHI.2024.3368049&partnerID=40&md5=f86323d331e693b3339bbc27aca6f412,2023
Machine learning approach to predicting the macro-mechanical properties of rock from the meso-mechanical parameters,"The determination of fundamental rock mechanical properties, uniaxial compression strength (UCS) and elastic modulus (E), constitutes a pivotal facet of rock engineering design. However, deriving these properties directly from standard laboratory tests on rock core samples can be challenging, especially when dealing with deep high-stress rock formations and weak fractured strata. Thus, it is crucial to establish a cost-effective and practical approach for predicting the macro-mechanical properties of rocks in situ. In this study, a machine learning approach was proposed to predict UCS and E by upscaling meso-mechanical parameters at particle scale in low-porosity crystalline rocks. To expand the correlation database of rock meso-macro mechanical properties, the meso-mechanical parameters, including the fracture toughness, tensile strength of the rock crystal interface, and the elastic modulus of rock crystal, were accurately measured, using a newly designed mechanical apparatus and a nanoindentation device. The grain-based models implemented in the combined finite discrete element method (FDEM-GBM) were developed based on these experimental results, and their reliability was validated though standard tests. Subsequently, a database, including 225 groups of data, was established using the numerical method. Five machine learning algorithms were applied to develop prediction models for UCS and E through data training in the database. Excellent performance improvement was achieved through the application of the grid-search method. The results indicate the optimized kernel ridge regression (KRR) and gaussian process regression (GPR) models demonstrated excellent performance with relative average errors of 4.9% and 1.1% in predicting UCS and E, respectively. Finally, the predicted values of UCS and E were compared with the experimental results, validating the feasibility of the optimized KRR and GPR models. © 2023 Elsevier Ltd",-,10.1016/j.compgeo.2023.105933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182212335&doi=10.1016%2fj.compgeo.2023.105933&partnerID=40&md5=758a6384d48f7b03c9fb8cf2a58aa087,2024
Multi-user quantum private query using Bell states,"Quantum private query (QPQ) can protect both user and database privacy when a user retrieves an entry from a database. Most existing QPQ protocols only support a single user retrieving a database, but in reality, people collaborate with each other to complete a task. Thus, it is necessary for multiple users to simultaneously retrieve a database. So far, all proposed multi-user QPQ protocols are based on high-dimensional quantum states and do not consider external attackers. In this paper, we present a multi-user QPQ protocol with two-way authentication based on Bell state and entanglement swapping technology. Under the premise of ensuring the privacy of users and the database, this protocol enables multiple users to simultaneously retrieve entries from a database. The security analysis demonstrates that the protocol can ensure privacy and resist external attacks. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",-,10.1007/s11128-024-04288-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186142069&doi=10.1007%2fs11128-024-04288-y&partnerID=40&md5=f173bc31e66e97ec8802ce4627dc0cd9,2021
A KMP-based interactive learning approach for robot trajectory adaptation with obstacle avoidance,"Purpose: Imitation learning is a powerful tool for planning the trajectory of robotic end-effectors in Cartesian space. Present methods can adapt the trajectory to the obstacle; however, the solutions may not always satisfy users, whereas it is hard for a nonexpert user to teach the robot to avoid obstacles in time as he/she wishes through demonstrations. This paper aims to address the above problem by proposing an approach that combines human supervision with the kernelized movement primitives (KMP) model. Design/methodology/approach: This approach first extracts the reference database used to train KMP from demonstrations by using Gaussian mixture model and Gaussian mixture regression. Subsequently, KMP is used to modulate the trajectory of robotic end-effectors in real time based on feedback from its interaction with humans to avoid obstacles, which benefits from a novel reference database update strategy. The user can test different obstacle avoidance trajectories in the current task until a satisfactory solution is found. Findings: Experiments performed with the KUKA cobot for obstacle avoidance show that this approach can adapt the trajectories of the robotic end-effector to the user’s wishes in real time, including trajectories that the robot has already passed and has not yet passed. Simulation comparisons also show that it exhibits better performance than KMP with the original reference database update strategy. Originality/value: An interactive learning approach based on KMP is proposed and verified, which not only enables users to plan the trajectory of robotic end-effectors for obstacle avoidance more conveniently and efficiently but also provides an effective idea for accomplishing interactive learning tasks under constraints. © 2024, Emerald Publishing Limited.",326-339,10.1108/IR-11-2023-0284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182498055&doi=10.1108%2fIR-11-2023-0284&partnerID=40&md5=45fb4bdc24573ff7bff9509ba490a768,2021
Efficient Algorithms for Rank-Regret Minimization,"Multi-criteria decision-making usually requires finding a small representative set from the database. popular method, the regret minimization set (RMS) query, returns a size <inline-formula><tex-math notation=""LaTeX"">$r$</tex-math></inline-formula> subset <inline-formula><tex-math notation=""LaTeX"">$S$</tex-math></inline-formula> of the full dataset <inline-formula><tex-math notation=""LaTeX"">$D$</tex-math></inline-formula> that minimizes the regret-ratio (the difference between the scores of top-1 in <inline-formula><tex-math notation=""LaTeX"">$S$</tex-math></inline-formula> and top-1 in <inline-formula><tex-math notation=""LaTeX"">$D$</tex-math></inline-formula>, for any utility function). RMS is not shift invariant, causing inconsistency in results. Further, the regret-ratio is often a &#x201D;made up&#x201D; number and users may mistake its absolute value. Instead, users do understand the notion of rank. Therefore, in this paper, we consider finding a fixed-size set <inline-formula><tex-math notation=""LaTeX"">$S$</tex-math></inline-formula> to minimize the maximum rank-regret (the rank of top-1 of <inline-formula><tex-math notation=""LaTeX"">$S$</tex-math></inline-formula> in the sorted list of <inline-formula><tex-math notation=""LaTeX"">$D$</tex-math></inline-formula>) over all possible utility functions, called the rank-regret minimization (RRM) problem which is shift invariant. In 2D space, we design an exact algorithm 2DRRM for RRM. In HD space, we propose an approximate algorithm HDRRM with theoretical guarantees on rank-regret. It combines the ideas of space discretization and clustering. Extensive experiments verify the efficiency and effectiveness of our algorithms. In particular, HDRRM always has the best output quality in experiments. IEEE",1-14,10.1109/TKDE.2024.3363009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184796407&doi=10.1109%2fTKDE.2024.3363009&partnerID=40&md5=b0a50ea3bdc0b0d38ecc1ef6f1bcdf99,2022
Network characteristics adaption and hierarchical feature exploration for robust object recognition,"Recent advances in deep networks have achieved appealing performances on object recognition tasks, due to their robust feature learning abilities. Besides the generated deep features, other network characteristics, e.g., inter-layer weight matrix and their back-propagated derivatives, may behave complementarily in feature learning in terms of generalization and robustness performances. However, characteristics adaptivity to different databases is not well studied. Meanwhile, current algorithms are apt to explore the most salient features for better generalization performance, while the hierarchically-salient features that may be beneficial for network robustness are not fully explored. Thus, we propose an attention module to make network characteristics adaptive to different training tasks, which can be further combined with the dynamic dropout algorithm to suppress salient neurons to explore more SndMS (Second Most Salient) features for robust recognition. The proposed algorithm has two main merits. First, the complementarity of network characteristics is taken into account when conducting training on different databases; Second, with the exploration of more SndMS neurons for hierarchically-salient feature representation learning, the network robustness against adversarial perturbations or fine-grained differences can be enhanced. The extensive experiments on seven public databases show that the proposed attention-based dropout largely improves the network robustness, without compromising the generalization performance, compared with related variants and state-of-the-art (SOTA) algorithms. Algorithm codes are available at https://github.com/lingjivoo/ACAD. © 2023 Elsevier Ltd",-,10.1016/j.patcog.2023.110240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182249503&doi=10.1016%2fj.patcog.2023.110240&partnerID=40&md5=607721706f0e2dc16e762940921c6bd6,2020
A Brief Survey of Vector Databases,"The explosive growth of massive high-dimensional data requires capabilities for data processing, storing, and analyzing. This brings significant challenges to traditional databases due to the poor ability to handle high-dimensional data and its original design for stand-alone machines. Fortunately, vector databases have provided a practical solution for the management and analysis of high-dimensional data. Especially, they retrieve results related to the query efficiently after encoding various forms of data (e.g., text, image, and video) into vectors. The purpose of this paper is to offer insight into vector databases by presenting a brief survey. Firstly, the workflow of vector databases including indexing and querying, is detailed along with a specific case. Subsequently, we elaborate on the related methods applied in vector databases, which are the core techniques to enhance search efficiency and reduce computational overhead, particularly similarity search algorithms and similarity metrics. Further, we introduce widely used vector database products (e.g., Pinecone, Chroma, and Milvus) and compare them from multiple factors that should be taken into consideration. We also discuss potential avenues for future research in this domain. To conclude, this survey provides a comprehensive understanding of vector databases for retrieval from vast high-dimensional datasets. © 2023 IEEE.",364-371,10.1109/BigDIA60676.2023.10429609,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186661681&doi=10.1109%2fBigDIA60676.2023.10429609&partnerID=40&md5=efe29256c6abb952256336d66bc8d721,2023
Short-term paroxysmal atrial fibrillation detection with intra- and inter-patient paradigm based on R-R intervals,"Paroxysmal Atrial Fibrillation (PAF) may lead to the decline of atrial mechanism and hemodynamic disturbance, which is the major risk factor for stroke, myocardial infarction, heart failure and other diseases. Although a number of methods have been developed to detect PAF, they rely on long ECG signals. Furthermore, most studies are only for the intra-patient test, ignoring individual specificity. Therefore, this paper proposes an automatic detection algorithm based on the combination of RR interval time-domain and nonlinear features to detect short-term PAF. The proposed features reflect the underlying physiological phenomenon of highly irregular ventricular activity caused by atrial unstable conduction. These include changes in morphological features and intrinsic changes in the cardiac dynamic system, which can capture subtle changes in early short-term PAF episodes and attenuate specificity between individuals. Finally, the random forest algorithm was used to detect the MIT-BIH Atrial Fibrillation Database and MIT-BIH Arrhythmia Database with intra-patient and inter-patient paradigm. The results show that the two databases’ algorithm accuracy in different paradigms of intra-patient and inter-patient have achieved more than 99.50% and 95.50%, respectively. Therefore, the proposed algorithm has a potential application for clinical monitoring. © 2023",-,10.1016/j.bspc.2023.105750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178663360&doi=10.1016%2fj.bspc.2023.105750&partnerID=40&md5=bfa9a9e82ea0326d0d74f2703a2245ef,2020
3D meta-classification: A meta-learning approach for selecting 3D point-cloud classification algorithm,"Algorithm selection technology, aimed at choosing the most suitable algorithm for a given machine learning task, has achieved significant success in the domain of 2D vision. However, few studies have explored its application to the 3D point cloud domain. The rapid proliferation and development of 3D point cloud classification algorithms underscore the urgent need for exploration in selecting these algorithms. In this paper, we propose a novel meta-learning-based 3D classification approach, termed 3D meta-classification, to address this gap. The approach operates at both the base-level and meta-level phases. At the base level, candidate 3D classification algorithms constantly classify various 3D datasets, recording their classification performance as empirical knowledge. As for the meta-level phase, it specifically tailors the 3D meta-knowledge generator, 3D meta-feature extractor, and 3D meta-database constructor to establish the 3D meta-database, capturing the relationship between the 3D meta-features and empirical knowledge. Leveraging the 3D meta-database, the 3D classification meta-learner trains the meta-model and predicts suitable algorithms for new incoming 3D datasets. Extensive experiments are conducted to select the best-performing classification algorithm for specific 3D datasets from ModelNet40. The results demonstrate the effectiveness of the proposed 3D meta-classification model; the accuracies of one-from-two and one-from-three algorithm selection tasks reach over 98% and 80%, respectively. © 2024 Elsevier Inc.",-,10.1016/j.ins.2024.120272,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184148009&doi=10.1016%2fj.ins.2024.120272&partnerID=40&md5=8e96b8fff3c1d04e9e54efbb00112002,2024
A dynamic transfer network for cross-database atrial fibrillation detection,"Deep learning has been successfully applied to the automatic diagnosis of cardiovascular disease. However, the domain shift of electrocardiogram (ECG) signals are huge since the ECG signals are generated in different acquisition environments, such that the model trained on a specific dataset typically performs worse when directly applied to a new dataset. In this paper, a dynamic transfer network (DTN) is proposed and applied to the cross-database AF detection. We devise a more general two-phase domain adaptation framework. Firstly, in the pre-training phase, three convolutional neural networks are trained as pre-trained models using the ECG signals from the source domain. Secondly, in the domain adaptation phase, a dynamic adaptive module (DAM) is introduced to mitigate the impact of the distribution differences by adaptively learning the ECG features of source and target domains. Furthermore, the minimum class confusion (MCC) loss is used to enhance the class discriminability to achieve highly accurate AF detection on the target domain. We performed six transfer tasks on three public ECG databases: the MIT-BIH Atrial Fibrillation Database (AFDB), the 2017 PhysioNet/CinC Challenge Database (Phy2017), and the China Physiological Signal Challenge 2018 Database (CPSC2018). The DTN-AlexNet obtained 91.35% accuracy and 87.92% F1 score on the transfer task AFDB→Phy2017. The DTN-VGG11 obtained 93.38% accuracy and 89.50% F1 score on the transfer task AFDB→CPSC2018. The DTN-ResNet obtained 97.58% accuracy and 96.83% F1 score on the transfer task Phy2017→AFDB. The experimental results demonstrate that the proposed DTN reduces the impact of the distribution differences, and performs well on cross-database AF detection. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179140260&doi=10.1016%2fj.bspc.2023.105799&partnerID=40&md5=8ff2fc79ffe11788e89ea3f7090ed793,2024
Training Universal Deep-Learning Networks for Electromagnetic Medical Imaging Using a Large Database of Randomized Objects,"Deep learning has become a powerful tool for solving inverse problems in electromagnetic medical imaging. However, contemporary deep-learning-based approaches are susceptible to inaccuracies stemming from inadequate training datasets, primarily consisting of signals generated from simplified and homogeneous imaging scenarios. This paper introduces a novel methodology to construct an expansive and diverse database encompassing domains featuring randomly shaped structures with electrical properties representative of healthy and abnormal tissues. The core objective of this database is to enable the training of universal deep-learning techniques for permittivity profile reconstruction in complex electromagnetic medical imaging domains. The constructed database contains 25,000 unique objects created by superimposing from 6 to 24 randomly sized ellipses and polygons with varying electrical attributes. Introducing randomness in the database enhances training, allowing the neural network to achieve universality while reducing the risk of overfitting. The representative signals in the database are generated using an array of antennas that irradiate the imaging domain and capture scattered signals. A custom-designed U-net is trained by using those signals to generate the permittivity profile of the defined imaging domain. To assess the database and confirm the universality of the trained network, three distinct testing datasets with diverse objects are imaged using the designed U-net. Quantitative assessments of the generated images show promising results, with structural similarity scores consistently exceeding 0.84, normalized root mean square errors remaining below 14%, and peak signal-to-noise ratios exceeding 33 dB. These results demonstrate the practicality of the constructed database for training deep learning networks that have generalization capabilities in solving inverse problems in medical imaging without the need for additional physical assistant algorithms. © 2023 by the authors.",-,10.3390/s24010008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181849896&doi=10.3390%2fs24010008&partnerID=40&md5=e0b9ca2abedef7d67f7523ff5b3800ae,2024
Development of Employee Information Management System Based on B/S Mode and SQL Server Database,"With the rapid development of Big data and information technology, this paper designs an employee information management system based on B/S mode and SQL Server database. Proposed the B/S mode architecture design and the functional module design of the employee information management system, analyzed the association rules of the SQL Server database, and detailed the design and development process of the employee information management system database using SQL Server software, which can achieve various functions such as employee information collection, query, and update. © 2023 IEEE.",245-249,10.1109/CIPAE60493.2023.00053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182326697&doi=10.1109%2fCIPAE60493.2023.00053&partnerID=40&md5=91bd29c7134c3716540403d51566d981,2022
Revisiting the robustness of spatio-temporal modeling in video quality assessment,"Video quality assessment (VQA) is a non-trivial task since accurately predicting video quality is of great importance in real applications. Most VQA models focus on designing efficient spatio-temporal modeling modules for improving their performance on the closed-sets while ignoring the core problem, i.e., the robustness of spatio-temporal modeling on predicting video quality. In this paper, we present an empirical study on spatio-temporal modeling in VQA by disturbing temporal information. To this end, we elaborately construct two video quality databases named Motion-Free and Motion-Interrupted, respectively. Specifically, the videos in the former database are without implicit or explicit temporal information, while the temporal information of the videos in the latter database is perturbed artificially. Then, we conduct a comprehensive study on the constructed databases by testing the state-of-the-art VQA models, and explore the robustness of spatio-temporal modeling in VQA. We find that the spatio-temporal modeling modules are prone to overlook motion interruption, in other words, the VQA models cannot handle the global flickering problem in videos. Moreover, they become ineffective when the temporal information is completely lost. © 2023",-,10.1016/j.displa.2023.102585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179001327&doi=10.1016%2fj.displa.2023.102585&partnerID=40&md5=25eb46cd62ca7efc229940da9ddeb71b,2021
Network-based analysis between SARS-CoV-2 receptor ACE2 and common host factors in COVID-19 and asthma: Potential mechanistic insights,"ACE2 as a functional receptor for SARS coronavirus plays an important role in COVID-19 infection of the host. Thus, we aimed to explore interaction networks between ACE2 and common host factors in COVID-19/Asthma comorbidity. We identified 1,191 protein targets closely related to ACE2, and integrated the GEO database and multiple public databases to identify 305 host factors related to ACE2 between COVID-19/Asthma comorbidity. Further enrichment analyses showed that metabolic processes, Th1 and Th2 cell differentiation and PPAR signaling pathways had vital significance in the comorbidity of asthma and COIVD-19. HRAS, IFNG, CAT, CDH1, FASN, ACLY, CCL5, VCAM1, SCD and HMGCR were the key host factors related to ACE2. Tissue-specific enrichment and correlation analysis with ACE2 showed that SCD, CAT and FASN were highly expressed in the lung; and these 10 molecules were closely related to ACE2 and specifically expressed in multiple tissues. We also identified a series of drugs, such as estradiol, tetrachlorodibenzodioxin, resveratrol, cyclosporine, perfluorooctanoic acid and so on. Finally, the expression levels and diagnostic performance of HRAS and SCD showed statistical significance in external databases. This study explored the interaction networks of ACE2-related host factors in COVID-19 and asthma and identified several potential drugs for COVID-19/Asthma comorbidity. Although our research needs further verification, it still informs the mechanisms and treatment of COVID-19/Asthma comorbidity. © 2023",-,10.1016/j.bspc.2023.105502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173151217&doi=10.1016%2fj.bspc.2023.105502&partnerID=40&md5=c1ca3140123f72d264cc0fa8706a465f,2024
Impact of random oversampling and random undersampling on the performance of prediction models developed using observational health data,"Background: There is currently no consensus on the impact of class imbalance methods on the performance of clinical prediction models. We aimed to empirically investigate the impact of random oversampling and random undersampling, two commonly used class imbalance methods, on the internal and external validation performance of prediction models developed using observational health data. Methods: We developed and externally validated prediction models for various outcomes of interest within a target population of people with pharmaceutically treated depression across four large observational health databases. We used three different classifiers (lasso logistic regression, random forest, XGBoost) and varied the target imbalance ratio. We evaluated the impact on model performance in terms of discrimination and calibration. Discrimination was assessed using the area under the receiver operating characteristic curve (AUROC) and calibration was assessed using calibration plots. Results: We developed and externally validated a total of 1,566 prediction models. On internal and external validation, random oversampling and random undersampling generally did not result in higher AUROCs. Moreover, we found overestimated risks, although this miscalibration could largely be corrected by recalibrating the models towards the imbalance ratios in the original dataset. Conclusions: Overall, we found that random oversampling or random undersampling generally does not improve the internal and external validation performance of prediction models developed in large observational health databases. Based on our findings, we do not recommend applying random oversampling or random undersampling when developing prediction models in large observational health databases. © 2023, The Author(s).",-,10.1186/s40537-023-00857-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181239097&doi=10.1186%2fs40537-023-00857-7&partnerID=40&md5=0b61b1b1c8390604698e8d8bdae1024b,2021
"DyTIS: A Dynamic Dataset Targeted Index Structure Simultaneously Efficient for Search, Insert, and Scan","Many datasets in real life are complex and dynamic, that is, their key densities are varied over the whole key space and their key distributions change over time. It is challenging for an index structure to efficiently support all key operations for data management, in particular, search, insert, and scan, for such dynamic datasets. In this paper, we present DyTIS (Dynamic dataset Targeted Index Structure), an index that targets dynamic datasets. DyTIS, though based on the structure of Extendible hashing, leverages the CDF of the key distribution of a dataset, and learns and adjusts its structure as the dataset grows. The key novelty behind DyTIS is to group keys by the natural key order and maintain keys in sorted order in each bucket to support scan operations within a hash index. We also define what we refer to as a dynamic dataset and propose a means to quantify its dynamic characteristics. Our experimental results show that DyTIS provides higher performance than the state-of-the-art learned index for the dynamic datasets considered. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",800-816,10.1145/3552326.3587434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160202025&doi=10.1145%2f3552326.3587434&partnerID=40&md5=16e158afd8610c99aaae7675e4231663,2022
A Novel Two-Stage Data-mining Model Combining Gait Recognition and Temporal Sequence Mining,"In recent years, artificial intelligence applications have been on the rise. Many enterprises have embraced digital transformation and have established new business models based on artificial intelligence and the Internet of Things, such as the telerehabilitation industry. The companies may utilize sensors or cameras to collect user data, and data mining is applied to discover insights for doctors’ aids. This paper establishes a novel two-stage data mining model combining gait recognition and sequential pattern mining. In the first stage, a particular computer vision application, gait recognition, identifies possible diseases using the subject’s walking postures. The gaits in a video can be converted to a temporal sequence according to user-defined events. For example, (normal gait, Parkinsonian gait, normal gait) is a temporal sequence in which the identified gaits are arranged by temporal orders in the sequence. In the second stage, after collecting a dataset of temporal sequences, the frequent patterns are discovered by sequential pattern mining. Our preliminary experiment collected 30 samples from the real world and demonstrated the model’s feasibility. © 2023 Copyright held by the owner/author(s).",17-22,10.1145/3639390.3639393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185534054&doi=10.1145%2f3639390.3639393&partnerID=40&md5=9f2a320ce18ffed32c49ae273e4810c2,2023
Human-in-the-loop Machine Translation with Large Language Model,"The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting. We evaluate the proposed pipeline using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance compared to direct translation instructions. Additionally, we discuss the experimental results from the following perspectives: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource scenarios; 3) the observed differences across selected domains; 4) the quantitative analysis of sentence-level and word-level statistics; and 5) the qualitative analysis of representative translation cases. © 2023 The authors. This article is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0)",88-98,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185226875&partnerID=40&md5=66b107ba9f2c9cff92146b47222bda87,2023
LSE: Efficient Symmetric Searchable Encryption based on Labeled PSI,"Searchable encryption (SE) allows a data owner to outsource encrypted documents to an untrusted cloud server while preserving privacy and achieving secure data sharing. However, most existing SE schemes have a trade-off between security and efficiency. Moreover, these SE schemes leak the server&#x0027;s partial database or search information to perform better. Recent attacks show that such leakages can be used to recover the content of queried keywords or partial database information. To solve this problem and ensure efficiency, this paper proposes labeled searchable encryption (LSE), an efficient searchable encryption scheme based on the labeled private set intersection. We also give formal proofs to prove the security of the proposed labeled PSI protocol and searchable encryption scheme. Finally, we do experiments to compare the performance with some state-of-the-art works, and the experimental results show that the LSE outperforms in terms of total size and generation time of the encrypted database as well as the total search time at client side. IEEE",1-12,10.1109/TSC.2024.3356728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183946084&doi=10.1109%2fTSC.2024.3356728&partnerID=40&md5=2d657501d8bb3a360b13fd06eaa63fda,2023
A patch-based real-time six degrees of freedom object pose refinement method for robotic manipulation,"A fundamental vision technique for industrial robots involves the six degrees of freedom pose estimation of target objects from a single image. However, the direct estimation of the six degrees of freedom object pose solely from a single image is subject to limited accuracy. Various refinement approaches have been proposed to improve the accuracy by utilizing rendered images from a 3D model. Nevertheless, balancing speed and accuracy in an industrial setting remains a challenge for these methods. In this study, we propose a novel six degrees of freedom pose refinement approach centered around matching real image patches. Unlike previous approaches, our method does not rely on a 3D model, resulting in increased speed and accuracy. In the offline phase, we construct an offline database using image patches obtained from real images. During the inference phase, our method initially identifies the image patch within the offline database that is closest to the initial pose. Subsequently, we refine the six degrees of freedom pose by matching the corresponding image patches from the offline database. Experimental results indicate that our six degrees of freedom pose refinement method achieves real-time capability with a frame rate of 71 Frames Per Second (FPS), along with high precision. When the threshold is set to 0.5% of the object diameter, the average distance of dots score on the test data surpasses 70%. Moreover, experiments involving gripping and assembling tasks on an industrial robot demonstrate the ability of our method to autonomously select appropriate grasping angles and positions in real time. It further generates suitable motion paths, ultimately ensuring production efficiency. © The Author(s) 2024.",-,10.1177/17298806241229270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186756452&doi=10.1177%2f17298806241229270&partnerID=40&md5=579f243ac1eafbc68f80fa26650c8f07,2021
Head-Pose Estimation Based on Lateral Canthus Localizations in 2-D Images,"Head-pose estimation plays an important role in computer vision. The head-pose estimation aims to determine the orientation of a human head by representing the yaw, pitch, and roll angles. Implementations can be achieved by different techniques depending on the type of input and training data. This article presents a simple three-dimensional (3-D) face model for estimating head poses. The personalized 3-D face model is constructed by 2-D face photographs. A frontal face photograph determines the plane coordinates of facial features. By knowing the yaw angles in the other averted face photograph, the depth coordinates can be determined. The yaw angle of the averted face is evaluated by the canthus positions. Once the 3-D face model is constructed, we can find the matching angles for a target head pose in a query 2-D photograph. The personalized 3-D face model rotates itself about the <italic>x</italic>-, <italic>y</italic>-, and <italic>z</italic>-axes and then projects its facial features onto plane coordinates. If the rotation angles are correct, the disparities between the 2-D facial features and those in the query face photograph are supposed to be at their minimum. The personalized 3-D face model is validated with the University of South Florida human-identification database. The performance of the proposed head-pose estimation is evaluated on the Biwi Kinect head-pose database and Pointing&#x2019;04 head-pose image database. The results show that the proposed method outperforms state-of-the-art technologies on both benchmark databases. IEEE",1-12,10.1109/THMS.2024.3351138,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183977119&doi=10.1109%2fTHMS.2024.3351138&partnerID=40&md5=98c9919517dc6343d92ed208cb538625,2021
Advancing Production Operation Safety with Virtual Reality Solutions and AI-Driven Computer Vision,"Artificial intelligence computer vision has always attracted much attention in the global academic community, and their method of analyzing security issues is mainly through simulation experiments. This article will conduct research based on human-computer interaction technology image processing and other related theories. First, this article introduces the principles of production operation cockpit. Then, this article elaborates on the computer vision algorithm and its mathematical derivation method and illustrates the feasibility and practicability of the research through examples. Finally, this article proposes an instrument panel safety design strategy based on the results of artificial intelligence computer vision research and conducts a safe driving simulation experiment. The experimental results show: (1) Security model testing: In terms of stability function testing, the wall display module is 0.857, the management prediction core module is 0.895, the operation management service database is 0.883, and the enterprise resource planning module is 0.837; in terms of throughput, the wall display module is 7478849, the management prediction core module is 7889587, the operation management service database is 895890, and the enterprise resource planning module is 742689; in terms of the number of concurrent users, these four modules are all 5,000; in terms of recovery failure time, the wall display module is 4 seconds, the management prediction core module is 3 seconds, the running management service database is 8 seconds, and the enterprise resource planning module is 4 seconds; the security performance of its model wall display module is 94%, the security performance of the management prediction core module is 97%, the security performance of the operation management service database is 99%, and the enterprise resource planning module has a security performance of 96%; in terms of page loading speed test, its wall display module is 4m/s, management prediction core module is 5m/s, operation management service database is 8m/s, and enterprise resource planning module is 6m/s. (2) In terms of computer vision algorithm performance: the accuracy of the computer vision algorithm is between 0.907-0.996, the recall rate is between 0.768-0.897, and the number of frames is 122HZ and 144HZ; the regression error range of the computer vision algorithm is between 0.047-0.79; the processor utilization of the algorithm is between 2% and 5%. © 2024 U-turn Press LLC.",132-143,10.14733/cadaps.2024.S17.132-143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331877&doi=10.14733%2fcadaps.2024.S17.132-143&partnerID=40&md5=1af03c767b7ae7c801aebe76567c997e,2020
Investigation on the abilities of different artificial intelligence methods to predict the aerodynamic coefficients,"The Computational Fluid Dynamics (CFD) simulations at different fidelity levels are a common tool for predicting the aerodynamic performance in many engineering applications such as; automotive and aerospace to cite a few. However, the associated computational costs with these simulations are not trivial, and using the CFD simulations might easily become unpractical specially at the early design stages. The number of the required CFD simulations could be reduced immensely by using Artificial Intelligence (AI) methods which can predict, for example, the aerodynamic coefficients based on previous and limited CFD simulations. Having discussed the AI potential to minimize the CFD usage, the objective of this work is to investigate and compare the ability of different AI methods to predict the aerodynamic coefficients using the same CFD database from the literature. In this study, three different AI methods, namely; machine learning (e.g. Extreme Gradient Boosting (XGB), CatBoost, Bagging, Light Gradient Boosting Machine (Light-GBM), Random Forest (RFR), Gradient Boosting), deep learning (e.g. One Dimensional Convolutional Neural Network (Conv1D)) and surrogate models (e.g. Gaussian Process Regression (GPR)) have been utilized to estimate a sample CFD database (e.g. NACA0012, RAE2822) from the existing literature, and the prediction performance of these methods have been compared. The results show that, the aerodynamic coefficients predicted using CatBoost, XGB and Bagging are in good agreement with the reference CFD results. Furthermore, the proposed Conv1D method with LeakyReLU activation function showed a promising results in the development of aerodynamic models which can be used in the early stages of the air vehicles design. The obtained results from this work indicate that the CatBoost and the XGB models required much smaller training time compared to the Conv1D method, moreover, the proposed regression trees; CatBoost, Bagging and XGB can reduce the required number of CFD simulations significantly. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.121324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170423674&doi=10.1016%2fj.eswa.2023.121324&partnerID=40&md5=8f6b15079abe9757f1e9a484f9fa3f04,2022
ECG Biometrics Based on Attention Enhanced Domain Adaptive Feature Fusion Network,"In recent years, there has been an increasing focus on privacy and security among individuals. Biometric systems are highly regarded for their strong resistance to counterfeiting. Among various biometric features, ECG signals are difficult to falsify and are less attack-prone. However, as the time interval between signal acquisitions increases, the dissimilarities between individual ECG signals become more pronounced, making it difficult for many studies to achieve satisfactory recognition results in multi-session recognition. Furthermore, some studies encountered challenges in extracting crucial features from the ECG signal, which posed difficulties for identification experiments. To address the above challenge, this study proposes a novel attention-enhanced domain adaptive feature fusion network. Firstly, the network employs a multi-branch architecture to extract essential features from various dimensions of the ECG signal. Secondly, it incorporates the proposed weight fusion adaptive attention mechanism to further emphasize the features of heartbeats that contribute to recognition. Additionally, domain adaptive technology is employed to mitigate differences in feature distribution of ECG signals across sessions, thereby enhancing the model's generalization capability. Finally, four well-known databases, the ECG-ID database, PTB database, CYBHi database, and Heartprint database, were utilized to evaluate the performance of the model. These databases predominantly comprise individuals with multiple records, fulfilling the prerequisites for multi-session recognition. The model achieved recognition results of 96.31%, 73.79%, 77.80%, and 54.78% on these four databases, respectively, surpassing related studies in multi-session recognition scenarios.  © 2023 The Authors.",1291-1307,10.1109/ACCESS.2023.3346997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181569402&doi=10.1109%2fACCESS.2023.3346997&partnerID=40&md5=7eac915166b0bdfe53b70a9a0451b405,2020
Nonlinear dynamical system iteration applied in video face feature extraction and recognition,"Video recognition is the fundamental way human eyes and brains perceive the world. The human brain's real recognition, thinking, and memory mechanism has nonlinear dynamical system models, and the interaction between neurons exists in chaotic iterative patterns characterized by nonlinear dynamics. This paper proposes a new method for extracting video face features based on a nonlinear dynamical system, which is constructed by using video images as ternary functions and Discrete Cosine Transform (DCT) basis functions as auxiliary functions. Then, chaotic iterations are performed on the system, and the iteration trajectories are used as video features. Finally, the Pearson correlation coefficient between features is calculated to determine whether the faces in different videos are similar to complete the recognition work. Regarding database preprocessing, we propose Adaptive Threshold Region Detection (ATRD) algorithm for video region segmentation to optimize the database and improve recognition performance. Overall, this paper's method is simple, flexible, and has high recognition accuracy, with good recognition results on the YouTube and VidTIMIT databases. Of course, as a new method, there is still room for improvement and refinement. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",397-412,10.1007/s12530-023-09562-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181524863&doi=10.1007%2fs12530-023-09562-5&partnerID=40&md5=152afd174fa20bcf82508036181026b5,2021
Facial Expression Recognition Based on Improved VGG-face Model and Transfer Learning,"The problem of facial expression recognition based on VGG-face transfer learning is studied. In order to solve the problem of incomplete extraction of subtle features in the current facial expression recognition model, a transfer learning facial expression recognition algorithm based on improved VGG-face model is proposed. Compared with ResNet or U-Net, its structure is relatively simple, easy to understand and implement, and has been widely used in face related tasks. By improving the output module to enhance the VGG-face model, and removing the full connection layer, the final convolution layer uses the global average pooled output features, which effectively improves the feature extraction performance of the model. The extracted deep facial expression features can effectively solve the problem of small differences between the class samples in the facial expression database, and avoid the phenomenon of over-fitting. Compared with the shallow feature analysis, the proposed method has achieved significant accuracy on four databases, and the recognition accuracy on CK + and Enterface '05 database is the best, reaching 94. 88% and 91% respectively. The results show that the method has a good effect of emotion recognition and classification.  © 2023 ACM.",-,10.1145/3627341.3630376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181399613&doi=10.1145%2f3627341.3630376&partnerID=40&md5=fd6060804ea46d5ce842881cbec9604e,2021
An Effective Framework for Enhancing Query Answering in a Heterogeneous Data Lake,"There has been a growing interest in cross-source searching to gain rich knowledge in recent years. A data lake collects massive raw and heterogeneous data with different data schemas and query interfaces. Many real-life applications require query answering over the heterogeneous data lake, such as e-commerce, bioinformatics and healthcare. In this paper, we propose LakeAns that semantically integrates heterogeneous data schemas of the lake to enhance the semantics of query answers. To this end, we propose a novel framework to efficiently and effectively perform the cross-source searching. The framework exploits a reinforcement learning method to semantically integrate the data schemas and further create a global relational schema for the heterogeneous data. It then performs a query answering algorithm based on the global schema to find answers across multiple data sources. We conduct extensive experimental evaluations using real-life data to verify that our approach outperforms existing solutions in terms of effectiveness and efficiency. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",770-780,10.1145/3539618.3591637,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168660874&doi=10.1145%2f3539618.3591637&partnerID=40&md5=b16a1c0a9ee262f466a07a7e6d072422,2022
A New Database of Houma Alliance Book Ancient Handwritten Characters and Its Baseline Algorithm,"The Houma Alliance Book is one of the national treasures of the Museum in Shanxi Museum Town in China. It has great historical significance in researching ancient history. To date, the research on the Houma Alliance Book has been staying in the identification of paper documents, which is inefficient to identify and difficult to display, study and publicize. Therefore, the digitization of the recognized ancient characters of Houma League can effectively improve the efficiency of recognizing ancient characters and provide more reliable technical support and text data. This paper proposes a new database of Houma Alliance Book ancient handwritten characters and a multi-modal fusion method to recognize ancient handwritten characters. In the database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten characters are collected from the original book collection and by human imitative writing. Furthermore, the decision-level classifier fusion strategy is applied to fuse three well-known deep neural network architectures for ancient handwritten character recognition. Experiments are performed on our new database. The experimental results first provide the baseline result of the new database to the research community and then demonstrate the efficiency of our proposed method.  © 2023 ACM.",40-48,10.1145/3613917.3613923,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173839810&doi=10.1145%2f3613917.3613923&partnerID=40&md5=ee2b349231f3c3427bad8b6495365b7b,2023
Desis: Efficient Window Aggregation in Decentralized Networks,"Stream processing is widely applied in industry as well as in research to process unbounded data streams. In many use cases, specific data streams are processed by multiple continuous queries. Current systems group events of an unbounded data stream into bounded windows to produce results of individual queries in a timely fashion. For multiple concurrent queries, multiple concurrent and usually overlapping windows are generated. To reduce redundant computations and share partial results, state-of-the-art solutions divide windows into slices and then share the results of those slices. However, this is only applicable for queries with the same aggregation function and window measure, as in the case of overlaps for sliding windows. For multiple queries on the same stream with different aggregation functions and window measures, partial results cannot be shared. Furthermore, data streams are produced from devices that are distributed in large decentralized networks. Current systems cannot process queries on decentralized data streams efficiently. All queries in a decentralized network are either computed centrally or processed individually without exploiting partial results across queries. We present Desis, a stream processing system that can efficiently process multiple stream aggregation queries. We propose an aggregation engine that can share partial results between multiple queries with different window types, measures, and aggregation functions. In decentralized networks, Desis moves computation to data sources and shares overlapping computation as early as possible between queries. Desis outperforms existing solutions by orders of magnitude in throughput when processing multiple queries and can scale to millions of queries. In a decentralized setup, Desis can save up to 99% of network traffic and scale performance linearly. © 2023 Copyright held by the owner/author(s)",618-631,10.48786/edbt.2023.52,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165120930&doi=10.48786%2fedbt.2023.52&partnerID=40&md5=a850f75dd6f44e51bf9bcc512fac44b4,2023
Site adaptation with machine learning for a Northern Europe gridded global solar irradiance product,"Gridded global horizontal irradiance (GHI) databases are fundamental for analysing solar energy applications' technical and economic aspects, particularly photovoltaic applications. Today, there exist numerous gridded GHI databases whose quality has been thoroughly validated against ground-based irradiance measurements. Nonetheless, databases that generate data at latitudes above 65˚ are few, and those available gridded irradiance products, which are either reanalysis or based on polar orbiters, such as ERA5, COSMO-REA6, or CM SAF CLARA-A2, generally have lower quality or a coarser time resolution than those gridded irradiance products based on geostationary satellites. Amongst the high-latitude gridded GHI databases, the STRÅNG model developed by the Swedish Meteorological and Hydrological Institute (SMHI) is likely the most accurate one, providing data across Sweden. To further enhance the product quality, the calibration technique called ""site adaptation"" is herein used to improve the STRÅNG dataset, which seeks to adjust a long period of low-quality gridded irradiance estimates based on a short period of high-quality irradiance measurements. This study introduces a novel approach for site adaptation of solar irradiance based on machine learning techniques, which differs from the conventional statistical methods used in previous studies. Seven machine-learning algorithms have been analysed and compared with conventional statistical approaches to identify Sweden's most accurate algorithms for site adaptation. Solar irradiance data gathered from three weather stations of SMHI is used for training and validation. The results show that machine learning can substantially improve the STRÅNG model's accuracy. However, due to the spatiotemporal heterogeneity in model performance, no universal machine learning model can be identified, which suggests that site adaptation is a location-dependant procedure. © 2023 The Author(s)",-,10.1016/j.egyai.2023.100331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180603961&doi=10.1016%2fj.egyai.2023.100331&partnerID=40&md5=b6bcb1f54afe2540efdefd53c1018571,2022
Census of abandoned built heritage assets: The importance of defining shared methodologies and ontologies,"The paper shows the studies carried out within the framework of two research agreements for the census of abandoned enlisted assets owned by public bodies in the Provinces of Piacenza, Parma and Reggio Emilia, stipulated with the respective Heritage Departments. The purpose of this research is to increase the cognitive framework of cultural assets in a state of abandonment, and then to transfer the results to the dedicated WebGIS platform, with the aim of identifying the ones which are most in need of a conservation and reuse intervention, in order to return them back to their community. Afterwards, as a result of the cataloguing carried out, it was possible to make some critical considerations and statistical reworkings of the collected data. The data analysis, performed through the design of a GIS database, finds out a higher frequency of abandonment for some specific building types and with recurring locations. The research also highlights, once again, the importance of defining common ontologies, which are essential for statistical data processing and interoperability between different existing databases. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved.",1665-1670,10.5194/isprs-archives-XLVIII-M-2-2023-1665-2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164721478&doi=10.5194%2fisprs-archives-XLVIII-M-2-2023-1665-2023&partnerID=40&md5=a46ad502fad64f95557d331a8c5f40bf,2024
MAPPING COVID-19 EPIDEMIC DATA USING FOSS,"The recognition of spatial and temporal patterns in the distribution of a pandemic plays a pivotal role in guiding policy approaches to its management, containment and elimination. For this purpose, a database has been built for the COVID-19 pandemic in the Trentino Province, in the eastern Italian alps, near the border between Italy and Austria. The database management system and the WebGIS mapping these data is based on Free and Open Source Software. The Data Base Management System (DBMS) runs on MySQL, available under the GNU General Public License, storing and processing geographic data. A custom procedure has been created to update the dataset, with the capability to import data from suitably formatted spreadsheets by an authenticated administrator. To ensure flexibility and responsiveness on desktop and mobile devices, the WebGIS has been created with a client-side approach, using the Leaflet and Bootstrap JavaScript language libraries, available with Open Source Licenses. These libraries, with additional custom scripts, create the user interface and render geographic data into maps. The exchange of data between the DBMS server and the client is performed using geojson tables. To protect the privacy of the patients, WebGIS users cannot access the source data even though maps and graphs can be downloaded as pictures. Geo-statistical analysis aimed at the detection of spatial and temporal patters is underway.  © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved.",261-267,10.5194/isprs-archives-XLVIII-4-W7-2023-261-2023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164573047&doi=10.5194%2fisprs-archives-XLVIII-4-W7-2023-261-2023&partnerID=40&md5=ae6ca83df3b109b2ba3ccbc64802a365,2023
NF-Log: Revisiting Log Writes in Relational Database for Efficient Persistent Memory Utilization,"Non-volatile memory (NVM) is a promising storage technology that combines not only high performance and byte-addressability (like DRAM) but also durability (like SSD). However, as existing relational database management systems (RDBMS) are originally designed based on the assumption that all the data and log are stored on high latency block based devices, they are not able to take full advantage of this new technology yet. Consequently, write operations will under-utilize the device(NVM) and eventually downgrade the performance. In this work, we analyzed the redo-log mechanism in InnoDB and after discovering its severe impact on the overall performance, we decided to perform optimizations that will match with NVM features. We started by re-designing the log write path within InnoDB. After that, we worked on improving the log data access locality and finally, we optimized the write ahead mechanism. NF-log (Non Flushing Log) utilizes the NVM byte addressability and its persistence feature to reduce the write size by 30% and boost up the performance to up to 38% for sysbench write-intensive workloads and up to 16% for TPC-C.  © 2023 ACM.",305-312,10.1145/3555776.3577733,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162880365&doi=10.1145%2f3555776.3577733&partnerID=40&md5=03f9a228860f20d0185dca62ffce85b9,2020
Finding Favourite Tuples on Data Streams with Provably Few Comparisons,"One of the most fundamental tasks in data science is to assist a user with unknown preferences in finding high-utility tuples within a large database. To accurately elicit the unknown user preferences, a widely-adopted way is by asking the user to compare pairs of tuples. In this paper, we study the problem of identifying one or more high-utility tuples by adaptively receiving user input on a minimum number of pairwise comparisons. We devise a single-pass streaming algorithm, which processes each tuple in the stream at most once, while ensuring that the memory size and the number of requested comparisons are in the worst case logarithmic in n, where n is the number of all tuples. An important variant of the problem, which can help to reduce human error in comparisons, is to allow users to declare ties when confronted with pairs of tuples of nearly equal utility. We show that the theoretical guarantees of our method can be maintained for this important problem variant. In addition, we show how to enhance existing pruning techniques in the literature by leveraging powerful tools from mathematical programming. Finally, we systematically evaluate all proposed algorithms over both synthetic and real-life datasets, examine their scalability, and demonstrate their superior performance over existing methods.  © 2023 ACM.",3229-3238,10.1145/3580305.3599352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171326476&doi=10.1145%2f3580305.3599352&partnerID=40&md5=392e87067250d567602f37fb8a67dfe4,2021
Quality Assessment for DIBR-synthesized Views based on Wavelet Transform and Gradient Magnitude Similarity,"To drive upgrades of Depth-Image-Based Rendering (DIBR) algorithms, depth image refinement, etc., quality assessment models for DIBR-synthesized images in 3D video systems are developed. However, most of these models could not effectively evaluate distortion due to irregular stretching (e.g., crumbling), which is more complex and common than black holes and regular stretching (e.g., horizontal stretching) in synthesized images. To make an attempt at this issue, a new quality assessment method is proposed for DIBR views. First, feature point matching and affine transformation are adopted to remove and compensate for the global object shift between reference and synthesized view images. Second, multi-scale discrete wavelet transform is utilized to extract multi-scale structure distortion; gradient magnitude similarity is further integrated to highlight the distortion features; morphological open operation and median filtering are adopted to exclude perceptually unimportant features. Third, scores are obtained by standard deviation pooling on distortion feature maps for each wavelet scale and sub-band. Experimental results demonstrate that our proposed model outperforms the state-of-the-art handcrafted feature-based DIBR-synthesized image quality assessment models on IETR database, and performs the best on average on IETR and IRCCyN/IVC databases. The source code will be available at <uri>https://github.com/House-yuyu/DIBR_IQA</uri>. IEEE",1-14,10.1109/TMM.2024.3356029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182934951&doi=10.1109%2fTMM.2024.3356029&partnerID=40&md5=6409a4e3d4a72ed3981e5343f8df2943,2021
Viper: A Fast Snapshot Isolation Checker,"Snapshot isolation (SI) is supported by most commercial databases and is widely used by applications. However, checking SI today—given a set of transactions, checking if they obey SI—is either slow or gives up soundness. We present viper, an SI checker that is sound, complete, and fast. Viper checks black-box databases and hence is transparent to both users and databases. To be fast, viper introduces BC-polygraphs, a new representation of transaction dependencies. A BC-polygraph is acyclic iff transactions are SI, a theorem that we prove. Viper also introduces heuristic pruning, an optimization to accelerate checking SI by leveraging common knowledge of real-world database implementations. Besides vanilla SI, viper supports major SI variants including Strong SI, Generalized SI, and Strong Session SI. Our experiments show that given the same time budget, viper improves over baselines by 15× in the workload sizes being checked. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",654-671,10.1145/3552326.3567492,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160209685&doi=10.1145%2f3552326.3567492&partnerID=40&md5=4c561882aa8aaf53741848a8755426ac,2022
Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series,"Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module for representation learning. Additionally, we stack multiple warping and attention modules to learn at different scales, producing multi-scale representations that balance coarse-grained and fine-grained signals for downstream tasks. We conduct extensive experiments on widely used datasets and a new large-scale benchmark built from clinical databases. The results demonstrate the superiority of Warpformer over existing state-of-the-art approaches.  © 2023 ACM.",3273-3285,10.1145/3580305.3599543,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170502489&doi=10.1145%2f3580305.3599543&partnerID=40&md5=622347c217cb5655d95270404c6a8dd9,2021
Time-tired compaction: An elastic compaction scheme for LSM-tree based time-series database,"Time-series DBMSs based on the LSM-tree have been widely applied in numerous scenarios ranging from daily life to industrial production. Compared to the traditional key–value data, the time-series data workload has significant features of writing and querying in chronological order. While simultaneously, such features bring new challenges to efficient queries, especially data compaction. Namely, an effective time-series data compaction algorithm is crucial for efficient storage and query of massive time-series data. However, the current data compaction method based on traditional LSM-tree cannot solve the problems such as high delay and inaccurate range of time sequence data query. Therefore, we propose a novel compaction algorithm, Time-Tiered Compaction, to customize for time-series scenarios. Time-Tiered Compaction leverages the characteristics of time-series workloads to estimate query loads to select optimum SSTables for merging during every compaction process, reducing unnecessary expenses. Time-Tiered Compaction is implemented on Apache IoTDB to evaluate algorithm performance using TPC-xIoT. Results show that the presented Time-Tiered Compaction significantly reduces (30%) the latency of the range queries with only a slight increase (5%) in point queries, compared with traditional strategies. © 2023 Elsevier Ltd",-,10.1016/j.aei.2023.102224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177205717&doi=10.1016%2fj.aei.2023.102224&partnerID=40&md5=be97d7fd46c21d8e04ba8016608cd9e6,2024
Distributed Logical Timestamp Allocation for DBMS Concurrency Control on Many-core Machines,"We investigate the timestamp allocation scheme in classical concurrency controls of the database management systems (DBMS) on many-core machines. Then we discuss a distributed logical timestamp allocation scheme with uniqueness and fairness to improve the performance of DBMS concurrency control algorithms on many-core machines. Further, the proposed logical timestamp generator is free of bottlenecks such as accessing the system clock counter, calling for atomic add operation, and synchronization. Finally, we experiment with an optimistic concurrency control algorithm based on the proposed and other allocation schemes. The results show that the performance of an optimistic concurrency control algorithm based on the proposed timestamp allocation outperforms one based on other allocations. Furthermore, it has better linear scalability under heavy loads.  © 2023 Owner/Author.",313-314,10.1145/3588195.3595942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169585187&doi=10.1145%2f3588195.3595942&partnerID=40&md5=6f68feb724368212530f0fa06880cdf7,2021
A prefetching indexing scheme for in-memory database systems,"In-memory databases (IMDBs) store all working data in the main memory, making memory access the dominant factor in system performance. Moreover, for modern multi-version systems, the extended version chain makes the access pattern more complex, putting extra pressure on indexing. Our micro-architectural profiling results of existing IMDB indexing schemes show that over half of the execution time goes to memory stalls caused by pointer chasing operations. This paper proposes a prefetching indexing scheme for modern in-memory database systems. This scheme achieves high performance in the presence of serial accesses in pointer chasing. The essential idea is to use a path prefetcher and a jump pointer prefetcher to hide cache miss latencies induced by indexing searches on versioned tuples. Specifically, this scheme works by associating a read counter with each block and updating the counters in the search keys’ access paths. To generate the optimal paths for future prefetches, we present how to continuously evolve the frequent access paths by analyzing those access patterns. Also, we create a jump pointer for each tuple in the leaf nodes to prefetch the head of its version chain. We present a jump pointer queue to accelerate linear version traversal. We achieve the high update performance because our improved search speed more than offsets any increase in prefetching overhead. Evaluations show that the prefetching indexing scheme outperforms the state-of-the-art indexing scheme by up to 70%. © 2024 The Author(s)",179-190,10.1016/j.future.2024.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187802514&doi=10.1016%2fj.future.2024.03.012&partnerID=40&md5=90c17a744cc1419a72d1d9fef60c1124,2022
Access Control for Database Applications: Beyond Policy Enforcement,"There have been many recent advances in enforcing finegrained access control for database-backed applications. However, operators face significant challenges both before and after an enforcement mechanism has been deployed. We identify three such challenges beyond enforcement and discuss possible solutions.  © 2023 Owner/Author(s).",223-230,10.1145/3593856.3595905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166233309&doi=10.1145%2f3593856.3595905&partnerID=40&md5=c71c190a5d798dace9c66410e6048498,2021
RESEARCH ON THE APPLICATION OF SPEECH DATABASE BASED ON EMOTIONAL FEATURE EXTRACTION IN INTERNATIONAL CHINESE EDUCATION AND TEACHING,"The advanced analysis of the relationship between acoustic and emotional characteristics of speech signals can effectively improve the interactivity and intelligence of computers. Given the current status of speech recognition and the problems encountered in international Chinese education, the study proposes to extract emotional characteristics to achieve speech construction of the database. Based on considering the emotional characteristics of speech, a hybrid algorithm based on spectral sequence context features is proposed. The DBN-BP algorithm is used to process emotional data of different dimensions, and a speech database is constructed. After testing and analyzing the algorithm model, it is found that the dynamic recognition accuracy of the DBN-BP model fused with emotional features is over 90%, and the negative emotion recognition rates in the three databases are all above 60%. At the same time, the accuracy rate of the model in the algorithm comparison experiment remains above 85%, the data information extraction is relatively complete, and the average test time of less than 1s is less than 3%. The speech database based on multi-emotional feature extraction can effectively provide a new reference for the improvement of the quality of Chinese international education and the improvement of the speech recognition system. © 2024 SCPE.",299-311,10.12694/scpe.v25i1.2296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187166521&doi=10.12694%2fscpe.v25i1.2296&partnerID=40&md5=fcc9cb0c35780fb11278633b6ae0a029,2020
Complications Associated With Implantable Spinal Cord Stimulator: A Study of FDA Databases From Regulatory Perspective,"An adverse event (AE) is any unexpected outcome associated with the use of a medical product in a patient. The tracking and recording of AE are important for reevaluation of safety and effectiveness of medical devices. The response to AE may include recall, which attempts to address the reported problem by repairing, re-labeling, etc. Spinal cord stimulator system (SCS) is an implantable device that sends low levels of electricity directly into the spinal cord to relieve pain, its indications include back pain, postsurgical pain, injuries to spinal cord, etc. As a type of active implantable device, SCS represent relatively high rate of AE. In this article, we analyzed 15 694 AE reports of SCS extracted from the Manufacturer and User Facility Device Experience (MAUDE) database of the United States Food and Drug Administration (FDA) between 1 January 2023 and 30 June 2023. In addition, we analyzed 65 recall reports of SCS extracted from the Recalls of Medical Devices database of FDA from 2005 to 2023 as supplement. The text data in AE dataset has been preprocessed using the natural language toolkit (NLTK), a natural language tool kit in Python. To predict the severity of AE, four classifiers as well as four vectorization methods were applied. The experiment achieved the highest accuracy of 90.6% with eXtreme Gradient Boosting Classifier (XGBC) and unigram. This study proposed an automatic approach of labeling the severity of AE data and conclude that SCS AE often occur when recharging the battery of SCS and when transmitting data via wireless communication route. IEEE",1-11,10.1109/TCSS.2024.3374642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189182548&doi=10.1109%2fTCSS.2024.3374642&partnerID=40&md5=c474a7cc09fb282d2c2c5d1ce620865d,2021
Split-Conv: A Resource-efficient Compression Method for Image Quality Assessment Models,"Blind Image Quality Assessment (BIQA) models based on deep neural networks (DNNs) have achieved state-of-The-Art performance recently. However, the heavyweight architecture makes them hard to deploy on resource-constrained devices. Filter pruning is one of the most effective ways to compress the DNN model. However, most pruning methods need complete retraining after pruning which is too resource-consuming for IQA models with complex training processes. In this paper, we propose a resource-efficient structural pruning method for IQA models called Split-Conv, where the model only needs to be retrained on small IQA databases. Specifically, we split convolutional layers into sub-convolutional kernels and decorators, which can measure the importance of convolutional channels more precisely, thus reducing the requirement for retraining conditions. The experiments on several IQA models demonstrate the effectiveness of our IQA model pruning approach. For instance, DBCNN, StairIQA, and HyperIQA's performances can be kept at the baseline level when 50% parameters are pruned with the model being retrained on the authentically distort IQA database containing only 586 images, and the performances of StairIQA and HyperIQA on KonIQ-10k database are still acceptable after 80% of the parameters have been pruned. Besides, due to Split-Conv's capability to identify the important, useless, and harmful filters, the performances of some BIQA models can even be boosted after model compression with a proper pruning ratio. © 2023 IEEE.",-,10.1109/VCIP59821.2023.10402756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184849635&doi=10.1109%2fVCIP59821.2023.10402756&partnerID=40&md5=fce7c021079c0794fcaadbe4c4798bbb,2022
Machine learning based real-time prediction of freeway crash risk using crowdsourced probe vehicle data,"Real-time prediction of crash risk can support traffic incident management by generating critical information for practitioners to allocate resources for responding to anticipated traffic crashes proactively. Unlike previous studies using archived traffic data covering a limited highway environment such as a segment or corridor, this study uses a statewide live traffic database from HERE to develop real-time traffic crash prediction models. This database provides crowdsourced probe vehicle data that are high-resolution real-time traffic speed for the entire freeway network (nearly 2,000 miles) in Alabama. This study aims to use machine learning models to predict crash risk on freeways according to pre-crash traffic dynamics (e.g., mean speed, speed reduction) along with static freeway attributes. Traffic speed characteristics were extracted from the HERE database for both pre-crash and crash-free traffic conditions. Random Forest (RF), Support Vector Machine (SVM) and Extreme Gradient Boosting (XGBoost) were developed and compared. Separate models were estimated for three major crash types: single-vehicle, rear-end, and sideswipe crashes. The model prediction accuracy indicated that the RF models outperform other models. Models for rear-end crashes are found to have greater accuracy than other models, which implies that rear-end crashes have a significant relationship with pre-crash traffic dynamics and are more predictable. The traffic speed factors that are ranked high in terms of feature importance are the speed variance and speed reduction prior to crashes. According to partial dependence plots, the rear-end crash risk is positively related to the speed variance and speed reductions. More results are discussed in the paper. © 2022 Taylor & Francis Group, LLC.",84-102,10.1080/15472450.2022.2106564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135609808&doi=10.1080%2f15472450.2022.2106564&partnerID=40&md5=5a5a30757141421e4a2e93b8901379e8,2024
Sequence signal reconstruction based multi-task deep learning for sleep staging on single-channel EEG,"The temporal context information between sleep stage sequence contains sleep transition rules, which is important for improving sleep staging performance. Existing multi-task learning methods reconstruct EEG signals from one sleep stage, ignoring the importance of sequential temporal context in capturing long-term dependencies to enhance representation learning. To address these issues, we propose a multi-task deep learning model to jointly reconstruct sequence signal and segment time series. The model enhances the ability of time series segmentation task to capture sequential temporal context and improves the performance of single-channel EEG by optimizing the common encoder for sequence signal reconstruction task. In addition, we design a one-dimensional channel attention module to enhance the feature representation extracted for sleep sequence signal. The experimental results on four datasets show that the multi-task deep learning model can improve the generalization using sequence signal reconstruction. Compared with other state-of-the-art methods, the method proposed in this study obtained competitive performance in terms of metrics such as accuracy, which is 85.6% on the 2013 version of Sleep-EDF Database Expanded, 83.4% on the 2018 version of Sleep-EDF Database Expanded, 85.6% on Sleep Heart Health Study, and 77.4% on CAP Sleep Database. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105615,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175012515&doi=10.1016%2fj.bspc.2023.105615&partnerID=40&md5=dc45b348b44fc3266d03b2df550b1de3,2020
SCInter: A comprehensive single-cell transcriptome integration database for human and mouse,"Single-cell RNA sequencing (scRNA-seq), which profiles gene expression at the cellular level, has effectively explored cell heterogeneity and reconstructed developmental trajectories. With the increasing research on diseases and biological processes, scRNA-seq datasets are accumulating rapidly, highlighting the urgent need for collecting and processing these data to support comprehensive and effective annotation and analysis. Here, we have developed a comprehensive Single-Cell transcriptome integration database for human and mouse (SCInter, https://bio.liclab.net/SCInter/index.php), which aims to provide a manually curated database that supports the provision of gene expression profiles across various cell types at the sample level. The current version of SCInter includes 115 integrated datasets and 1016 samples, covering nearly 150 tissues/cell lines. It contains 8016,646 cell markers in 457 identified cell types. SCInter enabled comprehensive analysis of cataloged single-cell data encompassing quality control (QC), clustering, cell markers, multi-method cell type automatic annotation, predicting cell differentiation trajectories and so on. At the same time, SCInter provided a user-friendly interface to query, browse, analyze and visualize each integrated dataset and single cell sample, along with comprehensive QC reports and processing results. It will facilitate the identification of cell type in different cell subpopulations and explore developmental trajectories, enhancing the study of cell heterogeneity in the fields of immunology and oncology. © 2023 The Authors",77-86,10.1016/j.csbj.2023.11.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178606041&doi=10.1016%2fj.csbj.2023.11.024&partnerID=40&md5=b63dabdc8c5d630c176c973436a06a4f,2024
VPCFormer: A transformer-based multi-view finger vein recognition model and a new benchmark,"In the past decade, finger vein authentication garners significant interest. However, most existing databases and algorithms predominantly focused on single-view finger vein recognition. The current projection of vein patterns actually maps a 3D network topology into a 2D plane, which inevitably leads to 3D feature loss and topological ambiguity in 2D images. Additionally, single-view based methods are sensitive to finger rotation and translation in practical applications. So far, there are currently few dedicated studies and public databases on multi-view finger vein recognition. To address these issues, we first establish a benchmark for future research by constructing the multi-view finger vein database, named Tsinghua Multi-View Finger Vein-3 Views (THUMVFV-3V) Database, which is collected over two sessions. THUMVFV-3V provides three types of Regions of Interest (ROIs) and includes unified preprocessing operations, catering to the majority of existing methods. Furthermore, we propose a novel Transformer-based model named Vein Pattern Constrained Transformer (VPCFormer) for multi-view finger vein recognition, primarily composed of multiple Vein Pattern Constrained Encoders (VPC-Encoders) and Neighborhood-Perspective Modules (NPMs). Specifically, the VPC-Encoder incorporates a novel Vein Pattern Attention Module (VPAM) and an Integrative Feed-Forward Network (IFFN). Motivated by the fact that the strong correlations veins exhibit across different views, we devise the VPAM. Assisted by a vein mask, VPAM is meticulously designed to exclusively extract intra- and inter-view dependencies between vein patterns. Further, we propose IFFN to efficiently aggregate the preceding attention and contextual information of VPAM. In addition, the NPM is utilized to capture the correlations within a single view, enhancing the final multi-view finger vein representation. Extensive experiments demonstrate the superiority of our VPCFormer. The THUMVFV-3V database is available at https://github.com/Pengyang233/THUMVFV-3V-Database. © 2023",-,10.1016/j.patcog.2023.110170,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179008367&doi=10.1016%2fj.patcog.2023.110170&partnerID=40&md5=2f90999017f9fb1df0d2c0d300534acd,2024
Identification the immune related marker genes and transcription-factor network in ruptured cerebral aneurysms using bioinformatics analysis and machine-learning strategies,"Background: As a high level of mortality and morbidity disease, ruptured cerebral aneurysms (RCA) is the most common cause of intracranial hemorrhage. Methods: The expression profiles of GSE13353, GSE26969 were downloaded from the Gene Expression Omnibus (GEO) database to obtain differentially expressed genes (DEGs) between RCA and unruptured cerebral aneurysms (URCA). The Least Absolute Shrinkage Selection Operator (LASSO) and the SVM-RFE (SVM-RFE) analysis were used to screen the potential diagnostic genes for RCA, which were further tested in the validation cohort (GSE54083). The TRRUST database was used to predict transcription factors (TF) and the JASPAR database was used to validate the marker genes and STAT1 binding sites. Finally, the CMAP database was used to predict RCA-associated drugs for clinical transformation. Results: A total of 120 DEGs were identified. The functional enrichment analysis revealed that NF-kappa B signaling pathway, tuberculosis, rheumatoid arthritis were enriched in the RCA group. NANS, NTRK3, OLFML2A were identified as diagnostic biomarkers of RCA. Transcription of all three marker genes is regulated by the predicted transcription factor STAT1, the CMAP database was eventually used to screen 10 most promising drugs for the treatment of RCA. Conclusion: NANS, NTRK3, OLFML2A may become new candidate biomarkers of RCA, moreover, STAT1 plays an important role in the development of RCA. The mentioned immune cells may play a key role in the development and progression of RCA. MEK and MAP kinase inhibitors are the most likely drug mechanisms for RCA therapy. © 2023 Elsevier Ltd",-,10.1016/j.bspc.2023.105611,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183902070&doi=10.1016%2fj.bspc.2023.105611&partnerID=40&md5=5283fe7ca877d605eacc84980b766358,2021
Tuning Database Parameters Using Query Perception and Evolutionary Reinforcement Learning,"Database systems serves as powerful tools for managing big data. Within Database Management System (DBMS), hundreds of parameters impact the performance of database system. Reasonable configuration and optimization of these parameters is a proven way to increase system throughput and reduce query latency. Machine learning methods such as reinforcement learning and Gaussian processes have been employed for tuning database parameters. However, these approaches suffer from dependence on training benchmarks, challenges of regression analysis in high-dimensional spaces, extended training times and difficulties in finding optimal solutions. To tackle these issues, this paper proposes a database tuning model with query perception and evolutionary reinforcement learning. The model encodes database query statements, combines them with DBMS parameters and predicts their latency by a neural network. The query encoding and perceived latency are incorporated as part of environment information for an evolutionary reinforcement scheme in the model, this scheme aims to minimize query latency by on-line tuning the parameters related to memory, logs, evaluation plan, connections number and storage system. Query perception mitigates model's dependence on the benchmarks as model training samples, providing parameters suggestions to improve the performance of databases in workload changing environments. Integrating evolutionary algorithms with reinforcement learning not only speeds up reinforcement learning based parameter tuning but also provides better parameter configuration by avoiding local convergence during learning procedures. This model has been validated on openGauss database system using several benchmarks. Compared to the default parameter configuration in openGauss, the optimized parameters suggested by the model significantly reduce the latency of workloads, with a maximum reduction of up to 55%. Additionally, the model also outperforms other referenced tuning schemes in query latency and model training time. It is demonstrated that leveraging machine learning to perceive workloads and adaptively adjust DBMS parameters is an effective approach for reducing query latency and increasing system throughput, enabling database systems to autonomously improve system performance in workload changing scenarios. This methodology allows database systems to play more significant roles in managing big data across diverse applications.  © 2024 ACM.",52-59,10.1145/3640824.3640869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188248367&doi=10.1145%2f3640824.3640869&partnerID=40&md5=fdb27d88e89dcbf5cdb1d5dda32d1809,2024
Research on Computer Intelligent Whole Process Construction Information System in Large International Hub Airport Project,"In order to meet the information security performance requirements of large hub airports in the era of big data, the platform adopts advanced big data architecture. The Logistic mapping technology is used to digitize the database information of the international hub airport, so that the information can be folded and transformed repeatedly in the region, and the hash encryption algorithm is used to encrypt and decrypt the database information, so as to realize the security management and storage of the database information. The intrusion system provides all-weather and all-day real-time monitoring and protection of the perimeter through video surveillance and sensor network system. The system timely obtains and records the image information in the case of alarms. At the same time, without losing data packets, the peak value of single type log entry can reach 15,000 pieces per second. The packet loss rate of the system database is also low, and good information security management effect is achieved. © 2023 IEEE.",391-395,10.1109/CIPAE60493.2023.00081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182335779&doi=10.1109%2fCIPAE60493.2023.00081&partnerID=40&md5=841518205b0e167c6d82314452c5e096,2022
ncRS: A resource of non-coding RNAs in sepsis,"Sepsis, a life-threatening condition triggered by the body's response to infection, presents a significant global healthcare challenge characterized by disarrayed host responses, widespread inflammation, organ impairment, and heightened mortality rates. This study introduces the ncRS database (http://www.ncrdb.cn), a meticulously curated repository housing 1144 experimentally validated non-coding RNAs (ncRNAs) intricately linked with sepsis. ncRS offers comprehensive RNA data, exhaustive experimental insights, and integrated annotations from diverse databases. This resource empowers researchers and clinicians to decipher ncRNAs' roles in sepsis pathogenesis, potentially identifying vital biomarkers for early diagnosis and prognosis, thus facilitating personalized treatments. © 2024 Elsevier Ltd",-,10.1016/j.compbiomed.2024.108256,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187715737&doi=10.1016%2fj.compbiomed.2024.108256&partnerID=40&md5=591fe562d0ed5175eddaccc48b99cc8b,2021
DyVer: Dynamic Version Handling for Array Databases,"Array databases are important data management systems for scientific applications. In array databases, version handling is an important problem due to the no-overwrite feature of scientific data. Existing studies for optimizing data versioning in array databases are relatively simple, which either focus on minimizing storage sizes or improving simple version chains. In this paper, we focus on two challenges: (1) how to balance the tradeoff between storage size and query time for numerous version data, which may have derivative relationships with each other; (2) how to dynamically maintain this balance with continuously added new versions. To address the above challenges, this paper presents DyVer, a versioning framework for SciDB which is one of the most well-known array databases. DyVer includes two techniques, including an efficient storage layout optimizer to quickly reduce data query time under storage capacity constraint and a version segment technique to cope with dynamic version additions. We evaluate DyVer using real-world scientific datasets. Results show that DyVer can achieve up to 95% improvement on the average query time compared to state-of-the-art data versioning techniques under the same storage capacity constraint.  © 2023 ACM.",144-154,10.1145/3577193.3593734,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168420980&doi=10.1145%2f3577193.3593734&partnerID=40&md5=67e223cfef49761301ec1ca658c2a988,2023
A ResNet-BiLSTM Multi-lead ECG Classification Method Embedded with Attention Mechanism,"Computer-assisted electrocardiogram analysis has important clinical significance for the prevention and treatment of cardiovascular diseases. A multi-lead electrocardiogram (ECG) classification method based on the residual network and bidirectional long short-term memory neural network was proposed. In order to extract more effective ECG features, the Squeeze-and-Excitation (SE) attention mechanism was embedded into the depth model. Finally, the effectiveness of the proposed method was verified on the Chinese cardiovascular disease database (CCDD) and the internationally recognized MIT-BIH-AR database. The accuracy, sensitivity and specificity of normal and abnormal heartbeats classification on the MIT-BIH-AR database that contains 48 recordings were 99.52%, 99.46% and 99.54%, respectively. The accuracy, sensitivity and specificity of the classification of normal and abnormal ECG recordings on the CCDD that contains more than 150000 recordings were 84.44%, 79.27% and 88.45%, respectively. The overall experimental results show that the classification performance of the proposed method is good on both small-scale and large-scale data sets.  © 2023 ACM.",99-106,10.1145/3606843.3606859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174298089&doi=10.1145%2f3606843.3606859&partnerID=40&md5=01972a4bae2032ebe794baa513e06a34,2024
Weighted Statistically Significant Pattern Mining,"Pattern discovery (aka pattern mining) is a fundamental task in the field of data science. Statistically significant pattern mining (SSPM) is the task of finding useful patterns that statistically occur more often from databases for one class than for another. The existing SSPM task does not consider the weight of each item. While in the real world, the significant level of different items/objects is various. Therefore, in this paper, we introduce the Weighted Statistically Significant Patterns Mining (WSSPM) problem and propose a novel WSSpm algorithm to successfully solve it. We present a new framework that effectively mines weighted statistically significant patterns by combining the weighted upper-bound model and the multiple hypotheses test. We also propose a new weighted support threshold that can satisfy the demand of WSSPM and prove its correctness and completeness. Besides, our weighted support threshold and modified weighted upper-bound can effectively shrink the mining range. Finally, experimental results on several real datasets show that the WSSpm algorithm performs well in terms of execution time and memory storage.  © 2023 ACM.",1276-1285,10.1145/3543873.3587586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159563762&doi=10.1145%2f3543873.3587586&partnerID=40&md5=e00236eb26fc157fd4ae0dbf8bf86a9f,2023
DB-GPT: Large Language Model Meets Database,"Large language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolutionize data management by serving as the ""brain"" of next-generation database systems. However, there are several challenges that utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demonstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical database characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to fine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict constraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific LLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database framework (DB-GPT), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design and pre-training. Preliminary experiments show that DB-GPT achieves relatively good performance in database tasks like query rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT. © 2024, The Author(s).",-,10.1007/s41019-023-00235-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182707404&doi=10.1007%2fs41019-023-00235-6&partnerID=40&md5=86cb7fb1e5253fe20581a4d12d6a3f53,2022
EARNet: Error-Aware Reconstruction Network for no-reference image quality assessment,"Deep learning-based no-reference image quality assessment (NRIQA) methods have demonstrated advanced performance. In this paper, a deep learning-based NRIQA method with strong error-aware and content-aware capabilities is proposed, which consists of Error-Aware Reconstruction Network (EARNet) module, Content Feature Extraction Network (CFENet) module, and Subjective Quality Regression Network (SQRNet) module. We first build a database to pre-train EARNet to obtain the ability to extract error features. For content features, CFENet pre-trained on large-scale image classification tasks is adopted to extract. The pre-trained EARNet and CFENet are serially connected with SQRNet so that the features received by SQRNet are both error-aware and content-aware. Extensive experimental results show that the proposed method achieves state-of-the-art performance on many well-known IQA databases. The robustness of the proposed method is verified on the large-scale Waterloo Exploration Database (WED), and its superiority is demonstrated by the group maximum differentiated (gMAD) competition game. Furthermore, we also verify that the proposed EARNet is highly extensible, which can further improve the performance of the existing deep learning-based NRIQA method. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.122050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173878774&doi=10.1016%2fj.eswa.2023.122050&partnerID=40&md5=bef353744ba862e3520073d7aceb39cd,2020
RAGN-L: A stacked ensemble learning technique for classification of Fire-Resistant columns,"One of the main challenges in using reinforced concrete materials in structures is to comprehend their fire resistance. The assessment of fire resistance can be performed in a laboratory environment using fire. However, such tests are time-consuming and expensive, and they may not provide a complete assessment of all relevant properties of a particular tested specimen. To that end, the implementation of machine learning (ML) in the investigation of fire-resistant structural performance would be beneficial, as it would also contribute to the reduction of time and cost problems related to traditional techniques. Here, this research proposes a novel ensemble ML approach to classify columns according to their fire resistance characteristics, supporting the application of ML techniques by fire engineers and scientists. The proposed model, named RAGN-L, combines Random Forest, Adaptive Boosting, and Gradient Naive Bayes, and is stacked using the Logistic Regression approach. RAGN-L is evaluated on real-world databases of reinforced concrete columns and concrete-filled steel tube columns, as well as a synthetic database generated by the TVAE deep learning model. The performance of the proposed solution is compared with ten different ML classifiers based on common statistical metrics, accuracy, precision, recall, and f1-score, and validated using the k-fold cross-validation approach. The developed algorithm outperforms ten different classifiers in all databases, with classification accuracies of 86.6%, and 99.6% for the real-world and synthetic databases of reinforced concrete columns, respectively, and 88.1% for the real-world database of concrete-filled steel tube columns. © 2023 Elsevier Ltd",-,10.1016/j.eswa.2023.122491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177567024&doi=10.1016%2fj.eswa.2023.122491&partnerID=40&md5=f2f37d74c5b00101f02a03f365919918,2022
In-depth investigation of speech emotion recognition studies from past to present –The importance of emotion recognition from speech signal for AI–,"In the super smart society (Society 5.0), new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-machine or human-computer interaction and collaboration. Speech signal contains much information about the speaker, such as age, sex, ethnicity, health condition, emotion, and thoughts. The field of study which analyzes the mood of the person from the speech is called speech emotion recognition (SER). Classifying the emotions from the speech data is a complicated problem for artificial intelligence, and its sub-discipline, machine learning. Because it is hard to analyze the speech signal which contains various frequencies and characteristics. Speech data are digitized with signal processing methods and speech features are obtained. These features vary depending on the emotions such as sadness, fear, anger, happiness, boredom, confusion, etc. Even though different methods have been developed for determining the audio properties and emotion recognition, the success rate varies depending on the languages, cultures, emotions, and data sets. In speech emotion recognition, there is a need for new methods which can be applied in data sets with different sizes, which will increase classification success, in which best properties can be obtained, and which are affordable. The success rates are affected by many factors such as the methods used, lack of speech emotion datasets, the homogeneity of the database, the difficulty of the language (linguistic differences), the noise in audio data and the length of the audio data. Within the scope of this study, studies on emotion recognition from speech signals from past to present have been analyzed in detail. In this study, classification studies based on a discrete emotion model using speech data belonging to the Berlin emotional database (EMO-DB), Italian emotional speech database (EMOVO), The Surrey audio-visual expressed emotion database (SAVEE), Ryerson Audio-Visual Database of Emotional Speech and Song Database (RAVDESS), which are mostly independent of the speaker and content, are examined. The results of both classical classifiers and deep learning methods are compared. Deep learning results are more successful, but classical classification is more important in determining the defining features of speech, song or voice. So It develops feature extraction stage. This study will be able to contribute to the literature and help the researchers in the SER field. © 2024",-,10.1016/j.iswa.2024.200351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187954118&doi=10.1016%2fj.iswa.2024.200351&partnerID=40&md5=5b5f5a811eecf308bf7806c104206755,2022
